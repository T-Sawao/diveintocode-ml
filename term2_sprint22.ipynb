{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "term2_sprint22.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyON/wpA5tMi4JT4CgBvy+no",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/T-Sawao/diveintocode-ml3/blob/main/term2_sprint22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qm73-fe-PgBR"
      },
      "source": [
        "# Sprint2 深層学習スクラッチ リカレントニューラルネットワーク"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5IH3-sdP3bi"
      },
      "source": [
        "## 1.このSprintについて\n",
        "\n",
        "### Sprintの目的\n",
        "スクラッチを通してリカレントニューラルネットワークの基礎を理解する\n",
        "\n",
        "### どのように学ぶか\n",
        "スクラッチでリカレントニューラルネットワークの実装を行います。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dM4JYPCoP3Qs"
      },
      "source": [
        "### 【問題1】SimpleRNNのフォワードプロパゲーション実装\n",
        "SimpleRNNのクラスSimpleRNNを作成してください。基本構造はFCクラスと同じになります。\n",
        "フォワードプロパゲーションの数式は以下のようになります。ndarrayのshapeがどうなるかを併記しています。\n",
        "バッチサイズをbatch_size、入力の特徴量数をn_features、RNNのノード数をn_nodesとして表記します。活性化関数はtanhとして進めますが、これまでのニューラルネットワーク同様にReLUなどに置き換えられます。\n",
        "\n",
        "$$a_t = x_t \\cdot W_x + h_{t-1} \\cdot W_h + B$$$$h_t = tanh(a_t)$$\n",
        "$a_t$ : 時刻tの活性化関数を通す前の状態 (batch_size, n_nodes)\n",
        "\n",
        "$h_t$ : 時刻tの状態・出力 (batch_size, n_nodes)\n",
        "\n",
        "$x_t$ : 時刻tの入力 (batch_size, n_features)\n",
        "\n",
        "$W_x$ : 入力に対する重み (n_features, n_nodes)\n",
        "\n",
        "$h_{t−1}$ : 時刻t-1の状態（前の時刻から伝わる順伝播） (batch_size, n_nodes)\n",
        "\n",
        "$W_h$ : 状態に対する重み。 (n_nodes, n_nodes)\n",
        "\n",
        "$B$ : バイアス項 (n_nodes,)\n",
        "\n",
        "初期状態 $h_0$ は全て0とすることが多いですが、任意の値を与えることも可能です。\n",
        "\n",
        "上記の処理を系列数n_sequences回繰り返すことになります。RNN全体への入力 $x$ は(batch_size, n_sequences, n_features)のような配列で渡されることになり、そこから各時刻の配列を取り出していきます。\n",
        "\n",
        "分類問題であれば、それぞれの時刻のhに対して全結合層とソフトマックス関数（またはシグモイド関数）を使用します。タスクによっては最後の時刻のhだけを使用することもあります。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWozJETWqVUd"
      },
      "source": [
        "### 1.1.1（解答）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tl-QSH3TqU2g"
      },
      "source": [
        "import numpy as np\n",
        "class SimpleRNN:\n",
        "    \"\"\"\n",
        "    シンプルなRNN\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    n_nodes : int\n",
        "      ノード数\n",
        "    n_features : int\n",
        "      特徴量数\n",
        "    initializer : インスタンス\n",
        "        初期化方法のインスタンス\n",
        "    optimizer : インスタンス\n",
        "        最適化手法のインスタンス\n",
        "    \"\"\"\n",
        "    def __init__(self, n_nodes, n_features, initializer, optimizer):\n",
        "        self.optimizer = optimizer #最適化手法\n",
        "        \n",
        "        # 初期化\n",
        "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
        "        self.Wx = initializer.W(n_features, n_nodes)\n",
        "        self.Wh = initializer.W(n_nodes, n_nodes)\n",
        "        self.B = initializer(1,)\n",
        "\n",
        "        self.h = None\n",
        "        \n",
        "        \n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        フォワードプロバケーション\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 次の形のndarray, shape (batch_size, n_sequences, n_features)\n",
        "            入力\n",
        "            \n",
        "        Returns\n",
        "        ----------\n",
        "        self.h : 次の形のndarray, shape (batch_size, n_nodes)\n",
        "            出力\n",
        "        \"\"\"\n",
        "        #サイズの取得\n",
        "        batch_size = X.shape[0] #バッチサイズ\n",
        "        n_sequences = X.shape[1] #シーケンス数\n",
        "        \n",
        "        #hの初期値\n",
        "        self.h = np.zeros([batch_size, n_nodes])\n",
        "\n",
        "        #hを算出\n",
        "        for i in range(n_sequences):\n",
        "            X_t = np.dot(X[:, i, :], self.Wx) + np.dot(self.h, self.Wh) + self.B\n",
        "            self.h = np.tanh(X_t)\n",
        "        \n",
        "        return self.h"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wz4d5PmWRe9L"
      },
      "source": [
        "### 【問題2】小さな配列でのフォワードプロパゲーションの実験\n",
        "小さな配列でフォワードプロパゲーションを考えてみます。\n",
        "\n",
        "\n",
        "入力x、初期状態h、重みw_xとw_h、バイアスbを次のようにします。\n",
        "\n",
        "\n",
        "ここで配列xの軸はバッチサイズ、系列数、特徴量数の順番です。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJ8ikkJtPQCK"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "x = np.array([[[1, 2], [2, 3], [3, 4]]])/100 # (batch_size, n_sequences, n_features)\n",
        "w_x = np.array([[1, 3, 5, 7], [3, 5, 7, 8]])/100 # (n_features, n_nodes)\n",
        "w_h = np.array([[1, 3, 5, 7], [2, 4, 6, 8], [3, 5, 7, 8], [4, 6, 8, 10]])/100 # (n_nodes, n_nodes)\n",
        "batch_size = x.shape[0] # 1\n",
        "n_sequences = x.shape[1] # 3\n",
        "n_features = x.shape[2] # 2\n",
        "n_nodes = w_x.shape[1] # 4\n",
        "h = np.zeros((batch_size, n_nodes)) # (batch_size, n_nodes)\n",
        "b = np.array([1, 1, 1, 1]) # (n_nodes,)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VE5ZxSYRsMU"
      },
      "source": [
        "フォワードプロパゲーションの出力が次のようになることを作成したコードで確認してください。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8nJyLqxRjKk"
      },
      "source": [
        "h = np.array([[0.79494228, 0.81839002, 0.83939649, 0.85584174]]) # (batch_size, n_nodes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eorN-L01GdYi"
      },
      "source": [
        "class SimpleRNN1:\n",
        "    \"\"\"\n",
        "    シンプルなRNN\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    n_nodes : int\n",
        "      ノード数\n",
        "    n_features : int\n",
        "      特徴量数\n",
        "    initializer : インスタンス\n",
        "        初期化方法のインスタンス\n",
        "    optimizer : インスタンス\n",
        "        最適化手法のインスタンス\n",
        "    \"\"\"\n",
        "    def __init__(self, n_nodes, n_features, initializer, optimizer):\n",
        "        self.optimizer = optimizer #最適化手法\n",
        "        \n",
        "        # 初期化\n",
        "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
        "        self.Wx = np.array([[1, 3, 5, 7], [3, 5, 7, 8]])/100 # (n_features, n_nodes)\n",
        "        self.Wh = np.array([[1, 3, 5, 7], [2, 4, 6, 8], [3, 5, 7, 8], [4, 6, 8, 10]])/100 # (n_nodes, n_nodes)\n",
        "        self.B = np.array([1, 1, 1, 1]) # (n_nodes,)\n",
        "\n",
        "        self.h = None\n",
        "        \n",
        "        \n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        フォワードプロバケーション\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 次の形のndarray, shape (batch_size, n_sequences, n_features)\n",
        "            入力\n",
        "            \n",
        "        Returns\n",
        "        ----------\n",
        "        self.h : 次の形のndarray, shape (batch_size, n_nodes)\n",
        "            出力\n",
        "        \"\"\"\n",
        "        #サイズの取得\n",
        "        batch_size = X.shape[0] #バッチサイズ\n",
        "        n_sequences = X.shape[1] #シーケンス数\n",
        "        \n",
        "        #hの初期値\n",
        "        self.h = np.zeros([batch_size, n_nodes])\n",
        "\n",
        "        #hを算出\n",
        "        for i in range(n_sequences):\n",
        "            X_t = np.dot(X[:, i, :], self.Wx) + np.dot(self.h, self.Wh) + self.B\n",
        "            self.h = np.tanh(X_t)\n",
        "        \n",
        "        return self.h\n",
        "\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-B_y4pbHJI2"
      },
      "source": [
        "RNN = SimpleRNN1(n_nodes, _, _, _)\n",
        "RNN.forward(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0MNNFduR125"
      },
      "source": [
        "### 【問題3】（アドバンス課題）バックプロパゲーションの実装\n",
        "バックプロパゲーションを実装してください。\n",
        "\n",
        "RNNの内部は全結合層を組み合わせた形になっているので、更新式は全結合層などと同様です。\n",
        "\n",
        "$$\n",
        "W_x^{\\prime} = W_x - \\alpha \\frac{\\partial L}{\\partial W_x} \\\\\n",
        "W_h^{\\prime} = W_h - \\alpha \\frac{\\partial L}{\\partial W_h} \\\\\n",
        "B^{\\prime} = B - \\alpha \\frac{\\partial L}{\\partial B}\n",
        "$$\n",
        "$\\alpha$ : 学習率\n",
        "\n",
        "$\\frac{\\partial L}{\\partial W_x}$ : $W_x$ に関する損失 $L$ の勾配\n",
        "\n",
        "$\\frac{\\partial L}{\\partial W_h}$ : $W_h$ に関する損失 $L$ の勾配\n",
        "\n",
        "$\\frac{\\partial L}{\\partial B}$ : $B$ に関する損失 $L$ の勾配\n",
        "\n",
        "勾配を求めるためのバックプロパゲーションの数式が以下です。\n",
        "\n",
        "$\\frac{\\partial h_t}{\\partial a_t} = \\frac{\\partial L}{\\partial h_t} ×(1-tanh^2(a_t))$\n",
        "\n",
        "$\\frac{\\partial L}{\\partial B} = \\frac{\\partial h_t}{\\partial a_t}$\n",
        "\n",
        "$\\frac{\\partial L}{\\partial W_x} = x_{t}^{T}\\cdot \\frac{\\partial h_t}{\\partial a_t}$\n",
        "\n",
        "$\\frac{\\partial L}{\\partial W_h} = h_{t-1}^{T}\\cdot \\frac{\\partial h_t}{\\partial a_t}$\n",
        "\n",
        "＊$\\frac{\\partial L}{\\partial h_t}$ は前の時刻からの状態の誤差と出力の誤差の合計です。hは順伝播時に出力と次の層に伝わる状態双方に使われているからです。\n",
        "\n",
        "前の時刻や層に流す誤差の数式は以下です。\n",
        "\n",
        "$\\frac{\\partial L}{\\partial h_{t-1}} = \\frac{\\partial h_t}{\\partial a_t}\\cdot W_{h}^{T}$\n",
        "\n",
        "$\\frac{\\partial L}{\\partial x_{t}} = \\frac{\\partial h_t}{\\partial a_t}\\cdot W_{x}^{T}$"
      ]
    }
  ]
}