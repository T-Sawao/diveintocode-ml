{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "term2_sprint11.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOR9NKG3hj1HCdT/rDzLFZA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/T-Sawao/diveintocode-ml/blob/master/term2_sprint11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HczNh-9AOdE"
      },
      "source": [
        "## 2.1次元の畳み込みニューラルネットワークスクラッチ\n",
        "\n",
        "畳み込みニューラルネットワーク（CNN） のクラスをスクラッチで作成していきます。NumPyなど最低限のライブラリのみを使いアルゴリズムを実装していきます。\n",
        "\n",
        "\n",
        "このSprintでは1次元の 畳み込み層 を作成し、畳み込みの基礎を理解することを目指します。次のSprintでは2次元畳み込み層とプーリング層を作成することで、一般的に画像に対して利用されるCNNを完成させます。\n",
        "\n",
        "\n",
        "クラスの名前はScratch1dCNNClassifierとしてください。クラスの構造などは前のSprintで作成したScratchDeepNeuralNetrowkClassifierを参考にしてください。\n",
        "\n",
        "\n",
        "**1次元畳み込み層とは**  \n",
        "CNNでは画像に対しての2次元畳み込み層が定番ですが、ここでは理解しやすくするためにまずは1次元畳み込み層を実装します。1次元畳み込みは実用上は自然言語や波形データなどの 系列データ で使われることが多いです。\n",
        "\n",
        "\n",
        "畳み込みは任意の次元に対して考えることができ、立体データに対しての3次元畳み込みまではフレームワークで一般的に用意されています。\n",
        "\n",
        "\n",
        "**データセットの用意** \n",
        "検証には引き続きMNISTデータセットを使用します。1次元畳み込みでは全結合のニューラルネットワークと同様に平滑化されたものを入力します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfEovpOnkz6h"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as setattr\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sGj38cePWNL"
      },
      "source": [
        "x = np.array([1,2,3,4])\n",
        "y = np.array([45, 70])\n",
        "w = np.array([3, 5, 7])\n",
        "b = np.array([1])\n",
        "\n",
        "F = w.shape[0]\n",
        "P = 0\n",
        "S = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6BUjrV6kkjL"
      },
      "source": [
        "### 1.1.1（解答）フォワードプロパゲーション"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kktxKUTOfwH9"
      },
      "source": [
        "y_hat = np.zeros(F-1)\n",
        "for i in range(F-1):\n",
        "  count = 0\n",
        "  for s in range(w.shape[0]):\n",
        "    count += x[i+s] * w[s]\n",
        "  y_hat[i] = count + b\n",
        "y_hat\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HPePtapk2fA"
      },
      "source": [
        "1.1.2（別解)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9kgqMOgaQHb"
      },
      "source": [
        "y_hat1 = np.zeros(F-1)\n",
        "for m in range(F-1):\n",
        "  y_hat1[m] = np.sum((x[m:m+F])*w)+b\n",
        "print(y_hat1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-sI4k_qlINK"
      },
      "source": [
        "### 1.2.1（解答）更新式"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sfI2EiZtQuJ"
      },
      "source": [
        "loss = y - y_hat\n",
        "loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F08pX1fQsTxN"
      },
      "source": [
        "### 1.3.1（解答）バックプロパゲーション"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbCVaqBqQ7xT"
      },
      "source": [
        "db = np.sum(loss)\n",
        "db"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPvkRp6flQwA"
      },
      "source": [
        "db = sum(loss)\n",
        "print(\"db\", db)\n",
        "\n",
        "dw = np.zeros(w.shape[0])\n",
        "for k in range(2):\n",
        "  dw += loss[k] * x[k:k+F]\n",
        "print(\"dw\", dw)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "td9egwYRSf6j"
      },
      "source": [
        "dx = np.sum([np.r_[0, loss[1]*w], np.r_[loss[0]*w, 0]], axis=0)\n",
        "dx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KPbFRiy07kD"
      },
      "source": [
        "### 2.1.1（解答）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvVK8dluOwlO"
      },
      "source": [
        "Nout = ((len(x) + 2*P - F)/S)+1\n",
        "Nout"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWusObzQrzAX"
      },
      "source": [
        "## 畳み込み層"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1IyVS8VU0BW"
      },
      "source": [
        "class SimpleConv1d:\n",
        "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
        "        self.optimizer = optimizer\n",
        "        self.n_nodes1 = n_nodes1\n",
        "        self.n_nodes2 = n_nodes2\n",
        "        # self.W = initializer.W(self.n_nodes1, self.n_nodes2)\n",
        "        # self.B = initializer.B(self.n_nodes2)\n",
        "        self.w = np.array([3, 5, 7])\n",
        "        self.b = np.array([1])\n",
        "\n",
        "        self.P = 0\n",
        "        self.S = 1\n",
        "        self.F = len(self.w)\n",
        "\n",
        "        self.dw = np.array([])\n",
        "        self.dx = np.array([])\n",
        "\n",
        "# 問題1--------------------------------------------------------------\n",
        "    # フォワードプロパゲーション時の処理\n",
        "    def forward(self, x):\n",
        "      for m in range(self.F-1):\n",
        "        y_hat1[m] = np.sum((x[m:m+self.F])*self.w)+self.b\n",
        "      return y_hat1\n",
        "\n",
        "    # バックプロパゲーション時の処理\n",
        "    def backward(self, X, da):\n",
        "      db = np.sum(da)\n",
        "      dw = np.zeros(self.w.shape[0])\n",
        "      for i in range(int(self._output_size(X))):\n",
        "        dw += da[i] * x[i:i+self.F]\n",
        "\n",
        "      dx = np.sum([np.r_[0, da[1]*self.w], np.r_[da[0]*self.w, 0]], axis=0)\n",
        "      return db, dw, dx\n",
        "\n",
        "# 問題2--------------------------------------------------------------\n",
        "    def _output_size(self, X):\n",
        "      self.Nout = int((len(X)) + (2*self.P) - self.F) / self.S + 1\n",
        "      return self.Nout"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mn4mv_GdLzvL"
      },
      "source": [
        "test = SimpleConv1d(1,2,3,4)\n",
        "test.forward(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zooglL-Ly8v"
      },
      "source": [
        "test._output_size(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1OmbHrySoXe"
      },
      "source": [
        "db, dw, dx = test.backward(x, loss)\n",
        "print(\"db\", db, \"dw\", dw, \"dx\",dx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0jRPnJD-Nbs"
      },
      "source": [
        "### 4.1.1（解答）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tBAVHJ9-OTi"
      },
      "source": [
        "class SimpleConv1_1d:\n",
        "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
        "        self.optimizer = optimizer\n",
        "        self.n_nodes1 = n_nodes1\n",
        "        self.n_nodes2 = n_nodes2\n",
        "        # self.W = initializer.W(self.n_nodes1, self.n_nodes2)\n",
        "        # self.B = initializer.B(self.n_nodes2)\n",
        "\n",
        "        # diver確認用サンプルデータ\n",
        "        self.W = np.array([[[1,1,1],[1,1,1]],[[1,1,1],[2,1,1]],[[2,1,1],[1,1,2]]])\n",
        "        self.b =np.array([3, 2, 1]) \n",
        "\n",
        "        self.P = 0           # パディング(今回未使用)\n",
        "        self.S = 1           # ストライド値\n",
        "        self.F = int(self.W.shape[2])  # フィルターサイズ\n",
        "\n",
        "        self.dw = np.array([])\n",
        "        self.dx = np.array([])\n",
        "\n",
        "# 問題1--------------------------------------------------------------\n",
        "    # フォワードプロパゲーション時の処理\n",
        "    def forward(self, x):\n",
        "      self.X = x\n",
        "      self.Nout = self._output_size(self.X)\n",
        "\n",
        "      # 出力値を記載する枠を作成\n",
        "      a = np.zeros([self.W.shape[0], self.Nout])\n",
        "      for j in range(self.F):\n",
        "        for i in range(self.Nout):\n",
        "          # 1.フィルターを移動させ、w重みを掛け合わす。\n",
        "          # 2.上記作業をチャンネル回数繰り返す。\n",
        "          a[j,i] = np.sum(self.X[:, i:i+self.F]*self.W[j])+self.b[j]\n",
        "          #                       -----------\n",
        "          #                       列の指定（フィルター)[i]から[i+F]までの値を返す。\n",
        "      return a\n",
        "\n",
        "      # 更新\n",
        "      self = self.optimizer.updatbe(self)\n",
        "      return self\n",
        "\n",
        "    # バックプロパゲーション時の処理\n",
        "    def backward(self, loss):\n",
        "      db = np.sum(loss)\n",
        "      dw = np.zeros(self.W.shape)\n",
        "      dx = np.zeros(self.X.shape)\n",
        "      for j in range(self.W.shape[0]):\n",
        "        for i in range(self.W.shape[1]):\n",
        "\n",
        "          # dwの式　W.shape(2,3)を１つとして、1つ目のfor分で行軸をスライドさせ、２つ目のfor分でチャネルを更新。\n",
        "          dw[j] += loss[j,i] * self.X[:,i:i+3]\n",
        "\n",
        "        # dxの式 W.shape(1,3)を１つとして、loss[0,0]*W[0]+loss[0,1]*W[0]と、\n",
        "        # loss[0,0]*W[1]+loss[0,1]*W[1]でfor文でチャネルを更新して、各要素ごとに足して合計を返す。\n",
        "        dx += np.r_[[np.r_[np.sum([loss[i,0]*self.W[i,0,:]], axis=0), 0] + np.r_[0,np.sum([loss[i,1]*self.W[i,0,:]], axis=0)]],\n",
        "                     [np.r_[np.sum([loss[i,0]*self.W[i,1,:]], axis=0), 0] + np.r_[0,np.sum([loss[i,1]*self.W[i,1,:]], axis=0)]]]\n",
        "      return db, dw, dx\n",
        "\n",
        "# 問題2--------------------------------------------------------------\n",
        "    def _output_size(self, X):\n",
        "      Nout = int(((X.shape[1]) + (2*self.P) - self.F) / self.S + 1)\n",
        "      return Nout"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-6govpl-OQP"
      },
      "source": [
        "X = np.array([[1, 2, 3, 4], [2, 3, 4, 5]]) # shape(2, 4)で、（入力チャンネル数、特徴量数）である。\n",
        "W = np.array([[[1,1,1],[1,1,1]],[[1,1,1],[2,1,1]],[[2,1,1],[1,1,2]]])\n",
        "B = np.array([1, 2, 3]) # （出力チャンネル数）\n",
        "loss = np.array([[52,56],[32,35],[9,11]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "now0vfk4-ONl"
      },
      "source": [
        "test2 = SimpleConv1_1d(1,2,3,4)\n",
        "test2.forward(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZMVafKBO3fQ"
      },
      "source": [
        "stride = stride\n",
        "padding = padding\n",
        "\n",
        "A = X\n",
        "output_size, chanel_size, filter_size = self.W.shape\n",
        "feature_size = self.A.shape[2]\n",
        "sample_size = self.A.shape[0]\n",
        "\n",
        "a = np.zeros([sample_size, output_size, feature_size-2])\n",
        "for samples in range(sample_size):\n",
        "    for output in range(output_size):\n",
        "        for j in range(filter_size - 1):\n",
        "            sig = 0\n",
        "            for chanel in range(chanel_size):\n",
        "                for i in range(filter_size):\n",
        "                    sig += X[samples, chanel, i+j] * self.W[output, chanel, j]\n",
        "            a[samples, output, j] = sig + b[output]\n",
        "\n",
        "return a"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52xJ6vTqPWnJ"
      },
      "source": [
        "output_size, chanel_size, filter_size = W.shape\n",
        "output_size\n",
        "chanel_size\n",
        "filter_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgHlEv9fxgl-"
      },
      "source": [
        "### 4.2.1 (解答）バックプロパゲーション"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z23N6zZmIoTX"
      },
      "source": [
        "db, dw, dx = test2.backward(loss)\n",
        "print(\"db\",db)\n",
        "print(\"dw\",dw)\n",
        "print(\"dx\", dx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0gpU4n1xOji"
      },
      "source": [
        "dw = np.zeros(W.shape)\n",
        "for j in range(3):\n",
        "  for i in range(2):\n",
        "    dw[j] += loss[j,i] * X[:,i:i+3]\n",
        "dw"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FavGZ4bawy-R"
      },
      "source": [
        "x = np.zeros(X.shape)\n",
        "for i in range(3):\n",
        "  x += np.r_[[np.r_[np.sum([loss[i,0]*W[i,0,:]], axis=0), 0] + np.r_[0,np.sum([loss[i,1]*W[i,0,:]], axis=0)]],\n",
        "             [np.r_[np.sum([loss[i,0]*W[i,1,:]], axis=0), 0] + np.r_[0,np.sum([loss[i,1]*W[i,1,:]], axis=0)]]]\n",
        "x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPfEot4nU3UK"
      },
      "source": [
        "# 3.検証\n",
        "\n",
        "##【問題8】学習と推定  \n",
        "これまで使ってきたニューラルネットワークの全結合層の一部をConv1dに置き換えてMNISTを学習・推定し、Accuracyを計算してください。\n",
        "\n",
        "\n",
        "出力層だけは全結合層をそのまま使ってください。ただし、チャンネルが複数ある状態では全結合層への入力は行えません。その段階でのチャンネルは1になるようにするか、 平滑化 を行なってください。\n",
        "\n",
        "\n",
        "画像に対しての1次元畳み込みは実用上は行わないことのため、精度は問いません。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Oatnu0s9Cg0"
      },
      "source": [
        "# メインクラス\n",
        "class ScratchDeepNeuralNetworkClassifier():\n",
        "  def __init__(self, epoch_num=1, batch_size=20, verbose=True): \n",
        "        self.epoch = epoch_num\n",
        "        self.batch_size = batch_size\n",
        "        self.verbose = verbose\n",
        "\n",
        "  def fit(self, initializing, act, optimization, X, y, X_val=None, y_val=None, sigma=0.01, lr=0.01, n_features=784, n_nodes1=400, n_nodes2=200, n_output=10):  \n",
        "        self.initializing = initializing # 初期化クラスの設定\n",
        "        self.activation= act # 活性化関数クラスの設定\n",
        "        self.optimization= optimization # 活性化関数クラスの設定\n",
        "        self.sigma = sigma\n",
        "        self.lr = lr\n",
        "        self.n_features = n_features\n",
        "        self.n_nodes1 = n_nodes1\n",
        "        self.n_nodes2 = n_nodes2\n",
        "        self.n_output = n_output\n",
        "        self.loss = np.zeros(self.epoch)\n",
        "        self.val_loss = np.zeros(self.epoch)\n",
        "        self.h = None #最適化クラス adagraidで使用\n",
        "\n",
        "        # 最適化クラスの設定\n",
        "        optimizer1 = self.optimization(self.lr)\n",
        "        optimizer2 = self.optimization(self.lr)\n",
        "        optimizer3 = self.optimization(self.lr)\n",
        "\n",
        "        # 全結合層にインスタンスを渡す\n",
        "        self.FC1 = FC(self.n_features, self.n_nodes1, self.initializing(self.sigma), optimizer1)\n",
        "        self.activation1 = self.activation() #　活性化関数の設定\n",
        "        self.FC2 = FC(self.n_nodes1, self.n_nodes2, self.initializing(self.sigma), optimizer2)\n",
        "        self.activation2 = self.activation() #　活性化関数の設定\n",
        "        self.FC3 = FC(self.n_nodes2, self.n_output, self.initializing(self.sigma), optimizer3)\n",
        "        self.activation3 = Softmax()\n",
        "\n",
        "        # エポック数分の学習\n",
        "        for i in range(self.epoch):\n",
        "          # ミニバッチの作成\n",
        "          get_mini_batch = GetMiniBatch(X, y, self.batch_size)\n",
        "          # 1エポック（全バッチ）の学習\n",
        "          for x_min, y_min in get_mini_batch:\n",
        "            y_hat = self._forward_propagation(x_min)\n",
        "            loss = self._back_propagation(X=y_hat, Y=y_min)\n",
        "            # print(\"y_hat\",y_hat[:1])\n",
        "\n",
        "          self.loss[i] += loss\n",
        "          if (type(X_val) != bool):\n",
        "            self.val = 1\n",
        "            y_hat_val = self._forward_propagation(X_val)\n",
        "            loss_val = -np.mean(y_val_one_hot * np.log(y_hat_val + 1e-7))\n",
        "            self.val_loss[i] += loss_val\n",
        "\n",
        "          # self.acc_val[i] = accuracy_score(np.argmax(y_val, axis=1), np.argmax(y_hat_val, axis=1))\n",
        "          # verboseをTrueにした際は学習過程を出力\n",
        "          if self.verbose :\n",
        "            print(f\"--{i+1}回目~loss~-------\\n{self.loss[i]}\")\n",
        "            print(f\"--{i+1}回目~loss_val~---\\n{self.val_loss[i]}\")\n",
        "            # print(f'epoch:{self.epoch:>3} loss:{self.loss:>8,.3f}')\n",
        "\n",
        "  # フォワードプロパゲーションの実行\n",
        "  def _forward_propagation(self, X):\n",
        "      A1 = self.FC1.forward(X)\n",
        "      Z1 = self.activation1.forward(A1)\n",
        "      A2 = self.FC2.forward(Z1)\n",
        "      Z2 = self.activation2.forward(A2)\n",
        "      A3 = self.FC3.forward(Z2)\n",
        "      Z3 = self.activation3.forward(A3)\n",
        "      return Z3\n",
        "\n",
        "  # バックプロパゲーションの実行\n",
        "  def _back_propagation(self, X, Y):\n",
        "      dA3, loss = self.activation3.backward(X, Y) # 交差エントロピー誤差とソフトマックスを合わせている\n",
        "      dZ2 = self.FC3.backward(dA3)\n",
        "      dA2 = self.activation2.backward(dZ2)\n",
        "      dZ1 = self.FC2.backward(dA2)\n",
        "      dA1 = self.activation1.backward(dZ1)\n",
        "      dZ0 = self.FC1.backward(dA1) # dZ0は使用しない\n",
        "      return loss\n",
        "\n",
        "  def predict(self, X):\n",
        "      y_hat = self._forward_propagation(X)\n",
        "      return np.argmax(y_hat, axis=1)\n",
        "\n",
        "  def plot_cost(self):\n",
        "      plt.title(\"Num_of_Iteration vs Loss\")\n",
        "      plt.xlabel(\"Num_of_Iteration\")\n",
        "      plt.ylabel(\"Loss\")\n",
        "      a = range(self.epoch)\n",
        "      plt.plot(range(1, self.epoch+1), self.loss, color=\"b\", label=\"train_loss\")\n",
        "      if self.val ==1:\n",
        "          plt.plot(range(1, self.epoch+1), self.val_loss, color=\"orange\", label=\"val_loss\")\n",
        "      plt.grid()\n",
        "      plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OumpXwgX80II"
      },
      "source": [
        "# チャンネル数１の畳み込みネットワーク\n",
        "class SimpleConv1_1d:\n",
        "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
        "        self.optimizer = optimizer\n",
        "        self.n_nodes1 = n_nodes1\n",
        "        self.n_nodes2 = n_nodes2\n",
        "        # self.W = initializer.W(self.n_nodes1, self.n_nodes2)\n",
        "        # self.B = initializer.B(self.n_nodes2)\n",
        "\n",
        "        # diver確認用サンプルデータ\n",
        "        self.W = np.array([[[1,1,1],[1,1,1]],[[1,1,1],[2,1,1]],[[2,1,1],[1,1,2]]])\n",
        "        self.b =np.array([3, 2, 1]) \n",
        "\n",
        "        self.P = 0           # パディング(今回未使用)\n",
        "        self.S = 1           # ストライド値\n",
        "        self.F = int(self.W.shape[2])  # フィルターサイズ\n",
        "\n",
        "        self.dw = np.array([])\n",
        "        self.dx = np.array([])\n",
        "\n",
        "# 問題1--------------------------------------------------------------\n",
        "    # フォワードプロパゲーション時の処理\n",
        "    def forward(self, x):\n",
        "      self.X = x\n",
        "      self.Nout = self._output_size(self.X)\n",
        "\n",
        "      # 出力値を記載する枠を作成\n",
        "      a = np.zeros([self.W.shape[0], self.Nout])\n",
        "      for j in range(self.F):\n",
        "        for i in range(self.Nout):\n",
        "          # 1.フィルターを移動させ、w重みを掛け合わす。\n",
        "          # 2.上記作業をチャンネル回数繰り返す。\n",
        "          a[j,i] = np.sum(self.X[:, i:i+self.F]*self.W[j])+self.b[j]\n",
        "          #                       -----------\n",
        "          #                       列の指定（フィルター)[i]から[i+F]までの値を返す。\n",
        "      return a\n",
        "\n",
        "      # 更新\n",
        "      self = self.optimizer.updatbe(self)\n",
        "      return self\n",
        "\n",
        "    # バックプロパゲーション時の処理\n",
        "    def backward(self, loss):\n",
        "      db = np.sum(loss)\n",
        "      dw = np.zeros(self.W.shape)\n",
        "      dx = np.zeros(self.X.shape)\n",
        "      for j in range(self.W.shape[0]):\n",
        "        for i in range(self.W.shape[1]):\n",
        "\n",
        "          # dwの式　W.shape(2,3)を１つとして、1つ目のfor分で行軸をスライドさせ、２つ目のfor分でチャネルを更新。\n",
        "          dw[j] += loss[j,i] * self.X[:,i:i+3]\n",
        "\n",
        "        # dxの式 W.shape(1,3)を１つとして、loss[0,0]*W[0]+loss[0,1]*W[0]と、\n",
        "        # loss[0,0]*W[1]+loss[0,1]*W[1]でfor文でチャネルを更新して、各要素ごとに足して合計を返す。\n",
        "        dx += np.r_[[np.r_[np.sum([loss[i,0]*self.W[i,0,:]], axis=0), 0] + np.r_[0,np.sum([loss[i,1]*self.W[i,0,:]], axis=0)]],\n",
        "                     [np.r_[np.sum([loss[i,0]*self.W[i,1,:]], axis=0), 0] + np.r_[0,np.sum([loss[i,1]*self.W[i,1,:]], axis=0)]]]\n",
        "      return db, dw, dx\n",
        "\n",
        "# 問題2--------------------------------------------------------------\n",
        "    def _output_size(self, X):\n",
        "      Nout = int(((X.shape[1]) + (2*self.P) - self.F) / self.S + 1)\n",
        "      return Nout"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPTM4YV39gO1"
      },
      "source": [
        "# 全結合層クラス\n",
        "class FC:\n",
        "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
        "        self.optimizer = optimizer\n",
        "        self.n_nodes1 = n_nodes1\n",
        "        self.n_nodes2 = n_nodes2\n",
        "        self.W = initializer.W(self.n_nodes1, self.n_nodes2)\n",
        "        self.B = initializer.B(self.n_nodes2)\n",
        "        # 初期化\n",
        "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
        "\n",
        "    # フォワードプロパゲーション時の処理\n",
        "    def forward(self, X):\n",
        "      self.X = X\n",
        "      out = X@self.W+self.B\n",
        "      return out\n",
        "    \n",
        "    # バックプロパゲーション時の処理\n",
        "    def backward(self, dA):\n",
        "      self.dZ = dA@(self.W.T)\n",
        "      self.dW = (self.X.T)@dA\n",
        "      self.dB = np.sum(dA, axis=0)\n",
        "      \n",
        "      # 更新\n",
        "      self = self.optimizer.update(self)\n",
        "      return self.dZ"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Jhu_rJd9gU3"
      },
      "source": [
        "# 初期化クラス\n",
        "# SimpleInitializer ------------------------------------------------------------\n",
        "class SimpleInitializer:\n",
        "    def __init__(self, sigma):\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def W(self, n_nodes1, n_nodes2):\n",
        "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
        "        return W\n",
        "\n",
        "    def B(self, n_nodes2):\n",
        "        return np.zeros(n_nodes2)\n",
        "\n",
        "# XavierInitializer ------------------------------------------------------------\n",
        "class XavierInitializer:\n",
        "    def __init__(self, sigma):\n",
        "      self.sigma = sigma\n",
        "\n",
        "    def W(self, n_nodes1, n_nodes2):\n",
        "        W = np.random.randn(n_nodes1, n_nodes2)/np.sqrt(n_nodes1)\n",
        "        return W\n",
        "\n",
        "    def B(self, n_nodes2):\n",
        "        return np.zeros(n_nodes2)\n",
        "\n",
        "# He-----------------------------------------------------------------------------\n",
        "class HeInitializer:\n",
        "    def __init__(self, sigma):\n",
        "      self.sigma = sigma\n",
        "\n",
        "    def W(self, n_nodes1, n_nodes2):\n",
        "        W = np.random.randn(n_nodes1, n_nodes2) * np.sqrt(2/n_nodes1) \n",
        "        return W\n",
        "\n",
        "    def B(self, n_nodes2):\n",
        "        return np.zeros(n_nodes2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRTtZxoA9ga5"
      },
      "source": [
        "# 最適化クラス\n",
        "# SGD---------------------------------------------\n",
        "class SGD:\n",
        "    def __init__(self, lr):\n",
        "        self.lr = lr\n",
        "\n",
        "    def update(self, layer):\n",
        "      layer.W -= self.lr * layer.dW\n",
        "      layer.B -= self.lr * layer.dB\n",
        "      return layer\n",
        "\n",
        "# AdaGrad----------------------------------------\n",
        "class AdaGrad:\n",
        "    def __init__(self, lr):\n",
        "      self.lr = lr\n",
        "      self.hw = 0\n",
        "      self.hb = 0\n",
        "  \n",
        "    def update(self, layer):\n",
        "        self.hw += layer.dW * layer.dW\n",
        "        self.hb = layer.dB * layer.dB\n",
        "        layer.W -= self.lr * layer.dW / (np.sqrt(self.hw) +1e-7)\n",
        "        layer.B -= self.lr * layer.dB / (np.sqrt(self.hb) +1e-7)\n",
        "        return layer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAndz8vR9gYS"
      },
      "source": [
        "# 活性化関数クラス\n",
        "# ソフトマックス関数のクラス----------------------------------------------\n",
        "class Softmax:\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  # forward時の処理\n",
        "  def forward(self, X):\n",
        "    y_hat = np.exp(X) / np.sum(np.exp(X), axis = 1).reshape(-1,1)\n",
        "    return y_hat\n",
        "\n",
        "  # backward時の処理\n",
        "  def backward(self, X, Y):\n",
        "    loss = -np.mean(Y * np.log(X + 1e-7))\n",
        "    dA3 = X - Y\n",
        "    return dA3, loss\n",
        "\n",
        "# ReLU関数のクラス----------------------------------------------\n",
        "class ReLU:\n",
        "  def __init__(self):\n",
        "        pass\n",
        "  # forward時の処理\n",
        "  def forward(self, X):\n",
        "    self.X =X\n",
        "    return np.maximum(0, X)\n",
        "    \n",
        "  # backward時の処理\n",
        "  def backward(self, dout):\n",
        "    return np.where(self.X > 0, dout, 0)\n",
        "\n",
        "# tanh関数のクラス----------------------------------------------\n",
        "class Tanh:\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  # forward時の処理\n",
        "  def forward(self, X):\n",
        "    self.out = np.tanh(X)\n",
        "    return self.out\n",
        "\n",
        "  # backward時の処理\n",
        "  def backward(self, X):\n",
        "    return X*(1-self.out**2)\n",
        "\n",
        "# sigmoid関数のクラス----------------------------------------------\n",
        "class Sigmoid:\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  # forward時の処理\n",
        "  def forward(self, z):\n",
        "    self.out = 1 / (1+np.exp(-z))\n",
        "    return self.out\n",
        "\n",
        "  # backward時の処理\n",
        "  def backward(self, z):\n",
        "    return z*(1-self.out)*self.out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UDiVexjLvLk"
      },
      "source": [
        "### 前処理"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2f8VReAI9gRy"
      },
      "source": [
        "class GetMiniBatch:\n",
        "    \"\"\"\n",
        "    ミニバッチを取得するイテレータ\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : 次の形のndarray, shape (n_samples, n_features)\n",
        "      訓練データ\n",
        "    y : 次の形のndarray, shape (n_samples, 1)\n",
        "      正解値\n",
        "    batch_size : int\n",
        "      バッチサイズ\n",
        "    seed : int\n",
        "      NumPyの乱数のシード\n",
        "    \"\"\"\n",
        "    def __init__(self, X, y, batch_size = 20, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self._X = X[shuffle_index]\n",
        "        self._y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self._X[p0:p1], self._y[p0:p1]        \n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self._X[p0:p1], self._y[p0:p1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7vbiylz9gM0"
      },
      "source": [
        "from keras.datasets import mnist\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# 平滑化（flatten）\n",
        "X_train_fltten = X_train.reshape(-1, 784)\n",
        "X_test_fltten = X_test.reshape(-1, 784)\n",
        "\n",
        "# float化と0or1処理\n",
        "X_train_flt = X_train_fltten.astype(np.float)\n",
        "X_test_flt = X_test_fltten.astype(np.float)\n",
        "X_train_flt /= 255\n",
        "X_test_flt /= 255\n",
        "\n",
        "# one hot処理\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
        "y_test_one_hot = enc.transform(y_test[:, np.newaxis])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "949gNi7o9gLZ"
      },
      "source": [
        "# train, testの分割\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_val, y_train_one_hot, y_val_one_hot = train_test_split(np.array(X_train_flt), np.array(y_train_one_hot), test_size=0.2)\n",
        "print(\"x_train\",x_train.shape, \"x_val\", x_val.shape, \"y_train_one_hot.shape\", y_train_one_hot.shape, \"y_val_one_hot\", y_val_one_hot.shape) # (48000, 784)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYyixhizLy5s"
      },
      "source": [
        "sdnn = ScratchDeepNeuralNetworkClassifier(epoch_num=7)\n",
        "sdnn.fit(HeInitializer, ReLU, AdaGrad, x_train, y_train_one_hot, x_val, y_val_one_hot, sigma=0.01, lr=0.01)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7IvL1crLy8z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wuMve5rLy3O"
      },
      "source": [
        "a = np.zeros([sample_size, output_size, feature_size-2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LCkyKvgLy2E"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}