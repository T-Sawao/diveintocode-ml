{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "term2_sprint11.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOdGsr02EvZJKvFmNtU5qwR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/T-Sawao/diveintocode-ml3/blob/main/term2_sprint11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HczNh-9AOdE"
      },
      "source": [
        "## 2.1次元の畳み込みニューラルネットワークスクラッチ\n",
        "\n",
        "畳み込みニューラルネットワーク（CNN） のクラスをスクラッチで作成していきます。NumPyなど最低限のライブラリのみを使いアルゴリズムを実装していきます。\n",
        "\n",
        "\n",
        "このSprintでは1次元の 畳み込み層 を作成し、畳み込みの基礎を理解することを目指します。次のSprintでは2次元畳み込み層とプーリング層を作成することで、一般的に画像に対して利用されるCNNを完成させます。\n",
        "\n",
        "\n",
        "クラスの名前はScratch1dCNNClassifierとしてください。クラスの構造などは前のSprintで作成したScratchDeepNeuralNetrowkClassifierを参考にしてください。\n",
        "\n",
        "\n",
        "**1次元畳み込み層とは**  \n",
        "CNNでは画像に対しての2次元畳み込み層が定番ですが、ここでは理解しやすくするためにまずは1次元畳み込み層を実装します。1次元畳み込みは実用上は自然言語や波形データなどの 系列データ で使われることが多いです。\n",
        "\n",
        "\n",
        "畳み込みは任意の次元に対して考えることができ、立体データに対しての3次元畳み込みまではフレームワークで一般的に用意されています。\n",
        "\n",
        "\n",
        "**データセットの用意**  \n",
        "検証には引き続きMNISTデータセットを使用します。1次元畳み込みでは全結合のニューラルネットワークと同様に平滑化されたものを入力します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Us6zhHb2HbS"
      },
      "source": [
        "## 【問題1】チャンネル数を1に限定した1次元畳み込み層クラスの作成\n",
        "チャンネル数を1に限定した1次元畳み込み層のクラスSimpleConv1dを作成してください。基本構造は前のSprintで作成した全結合層のFCクラスと同じになります。なお、重みの初期化に関するクラスは必要に応じて作り変えてください。Xavierの初期値などを使う点は全結合層と同様です。\n",
        "\n",
        "ここでは パディング は考えず、ストライド も1に固定します。また、複数のデータを同時に処理することも考えなくて良く、バッチサイズは1のみに対応してください。この部分の拡張はアドバンス課題とします。\n",
        "\n",
        "フォワードプロパゲーションの数式は以下のようになります。\n",
        "$$\\alpha_i = \\sum_{s=0}^{F-1}x_{(i+s)}W_s + b$$\n",
        "\n",
        "$a_i$ : 出力される配列のi番目の値\n",
        "\n",
        "$F$ : フィルタのサイズ\n",
        "\n",
        "$x_{(i+s})$ : 入力の配列の(i+s)番目の値\n",
        "\n",
        "$w_s$ : 重みの配列のs番目の値\n",
        "\n",
        "$b$ : バイアス項\n",
        "\n",
        "全てスカラーです。\n",
        "\n",
        "次に更新式です。ここがAdaGradなどに置き換えられる点は全結合層と同様です。\n",
        "$$w'_s = w_s -\\alpha\\frac{\\partial L}{\\partial w_s}$$$$b' = b - \\alpha\\frac{\\partial L}{\\partial b}$$\n",
        "\n",
        "$\\alpha$ : 学習率\n",
        "\n",
        "$\\frac{\\partial L}{\\partial w_s}$ : w_s に関する損失 $L$ の勾配\n",
        "\n",
        "$\\frac{\\partial L}{\\partial b}$ : $b$ に関する損失 $L$ の勾配\n",
        "\n",
        "勾配 $\\frac{\\partial L}{\\partial w_s}$ や $\\frac{\\partial L}{\\partial b}$ を求めるためのバックプロパゲーションの数式が以下です。\n",
        "\n",
        "$$\\frac{\\partial L}{\\partial w_s} = \\sum_{i=0}^{N_{out}-1}\\frac{\\partial L}{\\partial a_i}x_{(i+s}$$$$\\frac{\\partial L}{\\partial b} =  \\sum_{i=0}^{N_{out}-1}\\frac{\\partial L}{\\partial a_i}$$\n",
        "$\\frac{\\partial L}{\\partial a_i}$: 勾配の配列のi番目の値\n",
        "\n",
        "$N_{out} : 出力のサイズ\n",
        "\n",
        "前の層に流す誤差の数式は以下です。　　$$\\frac{\\partial L}{\\partial x_j} = \\sum_{s=0}^{F-1}\\frac{\\partial L}{\\partial a_{j-s}}w_s$$\n",
        "\n",
        "$\\frac{\\partial L}{\\partial x_j}$ : 前の層に流す誤差の配列のj番目の値\n",
        "\n",
        "ただし、 $j−s&lt;0$ または$j−s&gt;N_{out}−1$ のとき$\\frac{\\partial L}{\\partial a_{j-s}}=0$ です。\n",
        "\n",
        "全結合層との大きな違いは、重みが複数の特徴量に対して共有されていることです。この場合は共有されている分の誤差を全て足すことで勾配を求めます。計算グラフ上での分岐はバックプロパゲーションの際に誤差の足し算をすれば良いことになります。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfEovpOnkz6h"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as setattr\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sGj38cePWNL"
      },
      "source": [
        "x = np.array([1,2,3,4])\n",
        "y = np.array([45, 70])\n",
        "w = np.array([3, 5, 7])\n",
        "b = np.array([1])\n",
        "\n",
        "F = w.shape[0]\n",
        "P = 0\n",
        "S = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6BUjrV6kkjL"
      },
      "source": [
        "### 1.1.1（解答）フォワードプロパゲーション"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kktxKUTOfwH9"
      },
      "source": [
        "y_hat = np.zeros(F-1)\n",
        "for i in range(F-1):\n",
        "  count = 0\n",
        "  for s in range(w.shape[0]):\n",
        "    count += x[i+s] * w[s]\n",
        "  y_hat[i] = count + b\n",
        "y_hat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HPePtapk2fA"
      },
      "source": [
        "1.1.2（別解)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9kgqMOgaQHb"
      },
      "source": [
        "y_hat1 = np.zeros(F-1)\n",
        "for m in range(F-1):\n",
        "  y_hat1[m] = np.sum((x[m:m+F])*w)+b\n",
        "print(y_hat1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-sI4k_qlINK"
      },
      "source": [
        "### 1.2.1（解答）更新式"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sfI2EiZtQuJ"
      },
      "source": [
        "loss = y - y_hat\n",
        "loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F08pX1fQsTxN"
      },
      "source": [
        "### 1.3.1（解答）バックプロパゲーション"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbCVaqBqQ7xT"
      },
      "source": [
        "db = np.sum(loss)\n",
        "db"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPvkRp6flQwA"
      },
      "source": [
        "dw = np.zeros(w.shape[0])\n",
        "for k in range(2):\n",
        "  dw += loss[k] * x[k:k+F]\n",
        "print(\"dw\", dw)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "td9egwYRSf6j"
      },
      "source": [
        "dx = np.sum([np.r_[0, loss[1]*w], np.r_[loss[0]*w, 0]], axis=0)\n",
        "dx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHRCFj5k2d_1"
      },
      "source": [
        "## 【問題2】1次元畳み込み後の出力サイズの計算¶\n",
        "畳み込みを行うと特徴量の数が変化します。どのように変化するかは以下の数式から求められます。パディングやストライドも含めています。この計算を行う関数を作成してください。\n",
        "$$N_{out} = \\frac{N_{in} + 2P - F}{S} + 1$$\n",
        "\n",
        "$N_{out}$ : 出力のサイズ（特徴量の数）\n",
        "\n",
        "$N_{in}$ : 入力のサイズ（特徴量の数）\n",
        "\n",
        "$P$ : ある方向へのパディングの数\n",
        "\n",
        "$F$ : フィルタのサイズ\n",
        "\n",
        "$S$ : ストライドのサイズ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KPbFRiy07kD"
      },
      "source": [
        "### 2.1.1（解答）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvVK8dluOwlO"
      },
      "source": [
        "Nout = ((len(x) + 2*P - F)/S)+1\n",
        "Nout"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWusObzQrzAX"
      },
      "source": [
        "## 【問題3】小さな配列での1次元畳み込み層の実験\n",
        "次に示す小さな配列でフォワードプロパゲーションとバックプロパゲーションが正しく行えているか確認してください。\n",
        "\n",
        "入力$x$、重み$w$、バイアス$b$を次のようにします。\n",
        "\n",
        "- x = np.array([1,2,3,4])\n",
        "- w = np.array([3, 5, 7])\n",
        "- b = np.array([1])  \n",
        "\n",
        "フォワードプロパゲーションをすると出力は次のようになります。\n",
        "\n",
        "- a = np.array([35, 50])\n",
        "\n",
        "次にバックプロパゲーションを考えます。誤差は次のようであったとします。\n",
        "\n",
        "-delta_a = np.array([10, 20])\n",
        "\n",
        "バックプロパゲーションをすると次のような値になります。\n",
        "\n",
        "- delta_b = np.array([30])\n",
        "- delta_w = np.array([50, 80, 110])\n",
        "- delta_x = np.array([30, 110, 170, 140])\n",
        "\n",
        "**実装上の工夫**  \n",
        "畳み込みを実装する場合は、まずはfor文を重ねていく形で構いません。しかし、できるだけ計算は効率化させたいため、以下の式を一度に計算する方法を考えることにします。\n",
        "$$a_i = \\sum_{s=0}^{F-1}x_{i+s}w_s + b$$\n",
        "\n",
        "バイアス項は単純な足し算のため、重みの部分を見ます。$$\\sum_{s=0}^{F-1}x_{i+s}w_s$$\n",
        "\n",
        "これは、xの一部を取り出した配列とwの配列の内積です。具体的な状況を考えると、以下のようなコードで計算できます。この例では流れを分かりやすくするために、各要素同士でアダマール積を計算してから合計を計算しています。これは結果的に内積と同様です。\n",
        "\n",
        "- x = np.array([1, 2, 3, 4])\n",
        "- w = np.array([3, 5, 7])\n",
        "- a = np.empty((2, 3))\n",
        "- indexes0 = np.array([0, 1, 2]).astype(np.int)\n",
        "- indexes1 = np.array([1, 2, 3]).astype(np.int)\n",
        "- a[0] = x[indexes0]w # x[indexes0]は([1, 2, 3])である \n",
        "- a[1] = x[indexes1]w # x[indexes1]は([2, 3, 4])である\n",
        "- a = a.sum(axis=1)\n",
        "\n",
        "ndarrayは配列を使ったインデックス指定ができることを利用した方法です。\n",
        "\n",
        "また、二次元配列を使えば一次元配列から二次元配列が取り出せます。\n",
        "\n",
        "- x = np.array([1, 2, 3, 4])\n",
        "- indexes = np.array([[0, 1, 2], [1, 2, 3]]).astype(np.int)\n",
        "- print(x[indexes]) # ([[1, 2, 3], [2, 3, 4]])\n",
        "\n",
        "このこととブロードキャストなどをうまく組み合わせることで、一度にまとめて計算することも可能です。\n",
        "\n",
        "畳み込みの計算方法に正解はないので、自分なりに効率化していってください。\n",
        "\n",
        "**《参考》**  \n",
        "以下のページのInteger array indexingの部分がこの方法についての記述です。\n",
        "\n",
        "Indexing — NumPy v1.17 Manual  \n",
        "https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuBV1Uhi7Kit"
      },
      "source": [
        "## 3.1.1(解答)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1IyVS8VU0BW"
      },
      "source": [
        "class SimpleConv1d:\n",
        "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
        "        self.optimizer = optimizer\n",
        "        self.n_nodes1 = n_nodes1\n",
        "        self.n_nodes2 = n_nodes2\n",
        "        # self.W = initializer.W(self.n_nodes1, self.n_nodes2)\n",
        "        # self.B = initializer.B(self.n_nodes2)\n",
        "        self.w = np.array([3, 5, 7])\n",
        "        self.b = np.array([1])\n",
        "        self.P = 0\n",
        "        self.S = 1\n",
        "        self.F = len(self.w)\n",
        "\n",
        "        self.dw = np.array([])\n",
        "        self.dx = np.array([])\n",
        "\n",
        "# 問題1--------------------------------------------------------------\n",
        "    # フォワードプロパゲーション時の処理\n",
        "    def forward(self, x):\n",
        "      for m in range(self.F-1):\n",
        "        y_hat1[m] = np.sum((x[m:m+self.F])*self.w)+self.b\n",
        "      return y_hat1\n",
        "\n",
        "    # バックプロパゲーション時の処理\n",
        "    def backward(self, X, da):\n",
        "      db = np.sum(da)\n",
        "      dw = np.zeros(self.w.shape[0])\n",
        "      for i in range(int(self._output_size(X))):\n",
        "        dw += da[i] * x[i:i+self.F]\n",
        "\n",
        "      dx = np.sum([np.r_[0, da[1]*self.w], np.r_[da[0]*self.w, 0]], axis=0)\n",
        "      return db, dw, dx\n",
        "\n",
        "# 問題2--------------------------------------------------------------\n",
        "    def _output_size(self, X):\n",
        "      self.Nout = int((len(X)) + (2*self.P) - self.F) / self.S + 1\n",
        "      return self.Nout"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mn4mv_GdLzvL"
      },
      "source": [
        "test = SimpleConv1d(1,2,3,4)\n",
        "test.forward(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zooglL-Ly8v"
      },
      "source": [
        "test._output_size(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1OmbHrySoXe"
      },
      "source": [
        "db, dw, dx = test.backward(x, loss)\n",
        "print(\"db\", db, \"dw\", dw, \"dx\",dx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPc-gEqR3gjZ"
      },
      "source": [
        "## 【問題4】チャンネル数を限定しない1次元畳み込み層クラスの作成\n",
        "チャンネル数を1に限定しない1次元畳み込み層のクラスConv1dを作成してください。\n",
        "\n",
        "例えば以下のようなx, w, bがあった場合は、\n",
        "\n",
        "- x = np.array([[1, 2, 3, 4], [2, 3, 4, 5]]) # shape(2, 4)で、（入力チャンネル数、特徴量数）である。\n",
        "- w = np.ones((3, 2, 3)) # 例の簡略化のため全て1とする。(出力チャンネル数、入力チャンネル数、フィルタサイズ)である。\n",
        "- b = np.array([1, 2, 3]) # （出力チャンネル数）\n",
        "\n",
        "出力は次のようになります。\n",
        "\n",
        "- a = np.array([[16, 22], [17, 23], [18, 24]]) # shape(3, 2)で、（出力チャンネル数、特徴量数）である。\n",
        "\n",
        "入力が2チャンネル、出力が3チャンネルの例です。計算グラフを書いた上で、バックプロパゲーションも手計算で考えてみましょう。計算グラフの中には和と積しか登場しないので、微分を新たに考える必要はありません。\n",
        "\n",
        "**《補足》**  \n",
        "チャンネル数を加える場合、配列をどういう順番にするかという問題があります。(バッチサイズ、チャンネル数、特徴量数)または(バッチサイズ、特徴量数、チャンネル数)が一般的で、ライブラリによって順番は異なっています。（切り替えて使用できるものもあります）\n",
        "\n",
        "今回のスクラッチでは自身の実装上どちらが効率的かを考えて選んでください。上記の例ではバッチサイズは考えておらず、(チャンネル数、特徴量数)です。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0jRPnJD-Nbs"
      },
      "source": [
        "### 4.1.1（解答）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tBAVHJ9-OTi"
      },
      "source": [
        "class SimpleConv1_1d:\n",
        "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
        "        self.optimizer = optimizer\n",
        "        self.n_nodes1 = n_nodes1\n",
        "        self.n_nodes2 = n_nodes2\n",
        "        # self.W = initializer.W(self.n_nodes1, self.n_nodes2)\n",
        "        # self.B = initializer.B(self.n_nodes2)\n",
        "\n",
        "        # diver確認用サンプルデータ\n",
        "        self.W = np.array([[[1,1,1],[1,1,1]],[[1,1,1],[2,1,1]],[[2,1,1],[1,1,2]]])\n",
        "        self.b =np.array([3, 2, 1]) \n",
        "\n",
        "        self.P = 0 # パディング(今回未使用)\n",
        "        self.S = 1 # ストライド値\n",
        "        self.F = int(self.W.shape[2])  # フィルターサイズ\n",
        "\n",
        "        self.dw = np.array([])\n",
        "        self.dx = np.array([])\n",
        "\n",
        "# 問題1--------------------------------------------------------------\n",
        "    # フォワードプロパゲーション時の処理\n",
        "    def forward(self, x):\n",
        "      self.X = x\n",
        "      self.Nout = self._output_size(self.X)\n",
        "\n",
        "      # 出力値を記載する枠を作成\n",
        "      a = np.zeros([self.W.shape[0], self.Nout])\n",
        "      for j in range(self.F):\n",
        "        for i in range(self.Nout):\n",
        "          # 1.フィルターを移動させ、w重みを掛け合わす。\n",
        "          # 2.上記作業をチャンネル回数繰り返す。\n",
        "          a[j,i] = np.sum(self.X[:, i:i+self.F]*self.W[j])+self.b[j]\n",
        "          #                       -----------\n",
        "          #                       列の指定（フィルター)[i]から[i+F]までの値を返す。\n",
        "      return a\n",
        "\n",
        "      # 更新\n",
        "      self = self.optimizer.updatbe(self)\n",
        "      return self\n",
        "\n",
        "    # バックプロパゲーション時の処理\n",
        "    def backward(self, loss):\n",
        "      db = np.sum(loss)\n",
        "      dw = np.zeros(self.W.shape)\n",
        "      dx = np.zeros(self.X.shape)\n",
        "      for j in range(self.W.shape[0]):\n",
        "        for i in range(self.W.shape[1]):\n",
        "\n",
        "          # dwの式　W.shape(2,3)を１つとして、1つ目のfor分で行軸をスライドさせ、２つ目のfor分でチャネルを更新。\n",
        "          dw[j] += loss[j,i] * self.X[:,i:i+3]\n",
        "\n",
        "        # dxの式 W.shape(1,3)を１つとして、loss[0,0]*W[0]+loss[0,1]*W[0]と、\n",
        "        # loss[0,0]*W[1]+loss[0,1]*W[1]でfor文でチャネルを更新して、各要素ごとに足して合計を返す。\n",
        "        dx += np.r_[[np.r_[np.sum([loss[i,0]*self.W[i,0,:]], axis=0), 0] + np.r_[0,np.sum([loss[i,1]*self.W[i,0,:]], axis=0)]],\n",
        "                     [np.r_[np.sum([loss[i,0]*self.W[i,1,:]], axis=0), 0] + np.r_[0,np.sum([loss[i,1]*self.W[i,1,:]], axis=0)]]]\n",
        "      return db, dw, dx\n",
        "\n",
        "# 問題2--------------------------------------------------------------\n",
        "    def _output_size(self, X):\n",
        "      Nout = int(((X.shape[1]) + (2*self.P) - self.F) / self.S + 1)\n",
        "      return Nout"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-6govpl-OQP"
      },
      "source": [
        "X = np.array([[1, 2, 3, 4], [2, 3, 4, 5]]) # shape(2, 4)で、（入力チャンネル数、特徴量数）である。\n",
        "W = np.array([[[1,1,1],[1,1,1]],[[1,1,1],[2,1,1]],[[2,1,1],[1,1,2]]])\n",
        "B = np.array([1, 2, 3]) # （出力チャンネル数）\n",
        "loss = np.array([[52,56],[32,35],[9,11]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "now0vfk4-ONl"
      },
      "source": [
        "test2 = SimpleConv1_1d(1,2,3,4)\n",
        "test2.forward(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgHlEv9fxgl-"
      },
      "source": [
        "### 4.2.1 (解答）バックプロパゲーション"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z23N6zZmIoTX"
      },
      "source": [
        "db, dw, dx = test2.backward(loss)\n",
        "print(\"db\",db)\n",
        "print(\"dw\",dw)\n",
        "print(\"dx\", dx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0gpU4n1xOji"
      },
      "source": [
        "dw = np.zeros(W.shape)\n",
        "for j in range(3):\n",
        "  for i in range(2):\n",
        "    dw[j] += loss[j,i] * X[:,i:i+3]\n",
        "dw"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FavGZ4bawy-R"
      },
      "source": [
        "x = np.zeros(X.shape)\n",
        "for i in range(3):\n",
        "  x += np.r_[[np.r_[np.sum([loss[i,0]*W[i,0,:]], axis=0), 0] + np.r_[0,np.sum([loss[i,1]*W[i,0,:]], axis=0)]],\n",
        "             [np.r_[np.sum([loss[i,0]*W[i,1,:]], axis=0), 0] + np.r_[0,np.sum([loss[i,1]*W[i,1,:]], axis=0)]]]\n",
        "x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eh680InL36op"
      },
      "source": [
        "## 【問題5】（アドバンス課題）パディングの実装\n",
        "畳み込み層にパディングの機能を加えてください。1次元配列の場合、前後にn個特徴量を増やせるようにしてください。\n",
        "\n",
        "\n",
        "最も単純なパディングは全て0で埋める ゼロパディング であり、CNNでは一般的です。他に端の値を繰り返す方法などもあります。\n",
        "\n",
        "\n",
        "フレームワークによっては、元の入力のサイズを保つようにという指定をすることができます。この機能も持たせておくと便利です。なお、NumPyにはパディングの関数が存在します。\n",
        "\n",
        "\n",
        "numpy.pad — NumPy v1.17 Manual  \n",
        "https://docs.scipy.org/doc/numpy/reference/generated/numpy.pad.html\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smDm7woQ3_We"
      },
      "source": [
        "## 【問題6】（アドバンス課題）ミニバッチへの対応\n",
        "ここまでの課題はバッチサイズ1で良いとしてきました。しかし、実際は全結合層同様にミニバッチ学習が行われます。Conv1dクラスを複数のデータが同時に計算できるように変更してください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UMu837g4EKv"
      },
      "source": [
        "## 【問題7】（アドバンス課題）任意のストライド数\n",
        "ストライドは1限定の実装をしてきましたが、任意のストライド数に対応できるようにしてください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPfEot4nU3UK"
      },
      "source": [
        "# 3.検証\n",
        "\n",
        "##【問題8】学習と推定  \n",
        "これまで使ってきたニューラルネットワークの全結合層の一部をConv1dに置き換えてMNISTを学習・推定し、Accuracyを計算してください。\n",
        "\n",
        "\n",
        "出力層だけは全結合層をそのまま使ってください。ただし、チャンネルが複数ある状態では全結合層への入力は行えません。その段階でのチャンネルは1になるようにするか、 平滑化 を行なってください。\n",
        "\n",
        "\n",
        "画像に対しての1次元畳み込みは実用上は行わないことのため、精度は問いません。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Oatnu0s9Cg0"
      },
      "source": [
        "# メインクラス\n",
        "class ScratchDeepNeuralNetworkClassifier():\n",
        "  def __init__(self, filter_num, lr=0.01, sigma=0.01, epoch_num=1, batch_size=20, n_output=10): \n",
        "    self.Fn = filter_num\n",
        "    self.lr = lr\n",
        "    self.sigma = sigma\n",
        "    self.epoch = epoch_num\n",
        "    self.batch_size = batch_size\n",
        "    self.n_output = n_output\n",
        "    \n",
        "  def fit(self, X, y, ln_ch, initializing, activations, filter=3, stride=1, pad=0, opt=\"SGD\"):\n",
        "    self.n_features = X.shape[-1]\n",
        "    self.loss = np.zeros(epoch)\n",
        "    self.activation_list = []\n",
        "    sefl.FC_list = []\n",
        "    self.conv_list = []\n",
        "    self.F = filter\n",
        "    self.S = stride\n",
        "    self.P = pad\n",
        "  \n",
        "    n_node1 = self.n_features\n",
        "    self.flat_out_ch, self.flat_out_w = 0, 0\n",
        "    \n",
        "    # インスタンス生成----------------------------------------\n",
        "    for i in range(len(initializing)):\n",
        "\n",
        "      # 初期値-------------------------------------------\n",
        "      if initializing[i] == \"XavierInitializer\":\n",
        "        initializing = XavierInitializer(self.sigma)\n",
        "      elif initializing[i] == \"HeInitializer\":\n",
        "        initializing = HeInitializer(self.sigma)\n",
        "      else:\n",
        "        initializing = SimpleInitializer(self.sigma)\n",
        "\n",
        "      # 最適化-------------------------------------------\n",
        "      if opt == \"AdaGrad\":\n",
        "        optimizer = AdaGrad(self.lr)\n",
        "      else:\n",
        "        optimizer = SGD(self.lr)\n",
        "\n",
        "      # layer -------------------------------------------\n",
        "      if i < len(self.Fn):\n",
        "        n_node2 = self._output_size(n_node1)\n",
        "        self.conv_list.append(Conv1_1d(In_ch, self.Fn[i], n_node1, n_node2, filter, stride, pad, initializing, optimizer))\n",
        "      else:\n",
        "        self.FC_list.append(FC(n_node1, self.n_output, initializing, optimizer))\n",
        "          \n",
        "      if i == len(self.Fn)-1:\n",
        "        self.flat_out_ch, self.flat_out_w = self.Fn[-1], n_node2\n",
        "        n_node1 = self.Fn[-1]*n_node2\n",
        "      else:\n",
        "        n_node1 = n_node2\n",
        "      \n",
        "      # 活性化関数インスタンス生成----------------\n",
        "      if i < len(self.Fn):\n",
        "        if activations[i] == \"ReLU\":\n",
        "          self.activation_list.append(ReLU())\n",
        "        elif activations[i] == \"sigmoid\":\n",
        "          self.activation_list.append(Sigmoid())\n",
        "        else:\n",
        "          self.activation_list.append(Tanh())\n",
        "      else:\n",
        "        self.activation_list.append(Softmax())\n",
        "      # -----------------------------------------------------------------end\n",
        "\n",
        "    # エポック数分の学習\n",
        "    for i in range(self.epoch):\n",
        "      A = self.conv_list[0].forward(X)    # 畳み込み層 1層目\n",
        "      Z = self.activation_list[0].forward(A)\n",
        "      \n",
        "      for j in range(1, len(self.conv_list) -1):    # 畳み込み層 2層目以降\n",
        "          A = self.conv_list[j].forward(Z)\n",
        "          Z = self.activation_list[j].forward(A)\n",
        "      \n",
        "      # conv→FCのreshape バッチサイズ=1なので1\n",
        "      Z = Z.reshape(1, self.flat_Out_ch*self.flat_Out_w)\n",
        "      \n",
        "      A = self.FC_list[-1].forward(Z)    # 出力層\n",
        "      Z = self.activation_list[-1].forward(A)\n",
        "      \n",
        "      dA = self.activation_list[-1].backward(y) # 出力層\n",
        "      dZ = self.FC_list[-1].backward(dA)\n",
        "      # FC→convのreshape\n",
        "      dZ = dZ.reshape(self.flat_Out_ch, self.flat_Out_w)\n",
        "      \n",
        "      for j in reversed(range(1, len(self.conv_list))):    # 中間層\n",
        "          dA = self.activation_list[j].backward(dZ)\n",
        "          dZ = self.conv_list[j].backward(dA)\n",
        "\n",
        "      dA = self.activation_list[0].backward(dZ)    # 入力層\n",
        "      dZ = self.conv_list[0].backward(dA)\n",
        "          \n",
        "          \n",
        "      yp = self.predict(X)\n",
        "      # 損失関数\n",
        "      self.loss[i] = -np.sum(y * np.log(self.predict_y))/ self.predict_y.shape[1]\n",
        "\n",
        "\n",
        "  def predict(self, X):\n",
        "    A = self.conv_list[0].forward(X)    # 入力層\n",
        "    Z = self.activation_list[0].forward(A)\n",
        "    \n",
        "    for j in range(1, len(self.conv_list)-1):    # 中間層&出力層\n",
        "        A = self.conv_list[j].forward(Z)\n",
        "        Z = self.activation_list[j].forward(A)\n",
        "        \n",
        "    # conv→FCのreshape バッチサイズ=1なので1\n",
        "    Z = Z.reshape(1, self.flat_Out_ch*self.flat_Out_w)\n",
        "    \n",
        "    A = self.FC_list[-1].forward(Z)\n",
        "    Z = self.activation_list[-1].forward(A)\n",
        "    self.predict_y = Z.copy()\n",
        "    return np.argsort(Z)[:,-1]\n",
        "\n",
        "  def _output_size(self, X):\n",
        "    Nout = int(((X.shape[1]) + (2*self.P) - self.F) / self.S + 1)\n",
        "    return Nout"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OumpXwgX80II"
      },
      "source": [
        "# チャンネル数１の畳み込みネットワーク\n",
        "class Conv1_1d:\n",
        "    def __init__(self, n_nodes1, n_nodes2, filter, stride, pad, initializer, optimizer):\n",
        "        self.optimizer = optimizer\n",
        "        self.P = pad    # パディング(今回未使用)\n",
        "        self.S = stride  # ストライド値\n",
        "        self.F = filter   # フィルターサイズ\n",
        "        self.W = initializer.W(n_nodes1, n_nodes2, self.F)\n",
        "        self.B = initializer.B(n_nodes2)\n",
        "\n",
        "# 問題1--------------------------------------------------------------\n",
        "    # フォワードプロパゲーション時の処理\n",
        "    def forward(self, x):\n",
        "      self.X = x\n",
        "      self.Nout = self._output_size(self.X)\n",
        "\n",
        "      a = np.zeros([self.W.shape[0], self.Nout])\n",
        "      for j in range(self.F):\n",
        "        for i in range(self.Nout):\n",
        "          a[j,i] = np.sum(self.X[:, i:i+self.F]*self.W[j])+self.b[j]\n",
        "      return a\n",
        "\n",
        "      # 更新\n",
        "      self = self.optimizer.updatbe(self)\n",
        "      return self\n",
        "\n",
        "    # バックプロパゲーション時の処理\n",
        "    def backward(self, loss):\n",
        "      db = np.sum(loss)\n",
        "      dw = np.zeros(self.W.shape)\n",
        "      dx = np.zeros(self.X.shape)\n",
        "      for j in range(self.W.shape[0]):\n",
        "        for i in range(self.W.shape[1]):\n",
        "          dw[j] += loss[j,i] * self.X[:,i:i+3]\n",
        "\n",
        "        dx += np.r_[[np.r_[np.sum([loss[i,0]*self.W[i,0,:]], axis=0), 0] + np.r_[0,np.sum([loss[i,1]*self.W[i,0,:]], axis=0)]],\n",
        "                     [np.r_[np.sum([loss[i,0]*self.W[i,1,:]], axis=0), 0] + np.r_[0,np.sum([loss[i,1]*self.W[i,1,:]], axis=0)]]]\n",
        "      return db, dw, dx\n",
        "\n",
        "# 問題2--------------------------------------------------------------\n",
        "    def _output_size(self, X):\n",
        "      Nout = int(((X.shape[1]) + (2*self.P) - self.F) / self.S + 1)\n",
        "      return Nout"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPTM4YV39gO1"
      },
      "source": [
        "# 全結合層クラス\n",
        "class FC:\n",
        "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
        "        self.optimizer = optimizer\n",
        "        self.W = initializer.W(n_nodes1, n_nodes2)\n",
        "        self.B = initializer.B(n_nodes2)\n",
        "        # 初期化\n",
        "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
        "\n",
        "    # フォワードプロパゲーション時の処理\n",
        "    def forward(self, X):\n",
        "      self.X = X\n",
        "      out = X@self.W+self.B\n",
        "      return out\n",
        "    \n",
        "    # バックプロパゲーション時の処理\n",
        "    def backward(self, dA):\n",
        "      self.dZ = dA@(self.W.T)\n",
        "      self.dW = (self.X.T)@dA\n",
        "      self.dB = np.sum(dA, axis=0)\n",
        "      \n",
        "      # 更新\n",
        "      self = self.optimizer.update(self)\n",
        "      return self.dZ"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Jhu_rJd9gU3"
      },
      "source": [
        "# 初期化クラス\n",
        "# SimpleInitializer ------------------------------------------------------------\n",
        "class SimpleInitializer:\n",
        "    def __init__(self, sigma):\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def W(self, n_nodes1, n_nodes2, filter=None):\n",
        "      if filter == None:\n",
        "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
        "      else:\n",
        "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2, filter)\n",
        "      return W\n",
        "\n",
        "    def B(self, n_nodes2):\n",
        "      B = self.sigma * np.random.randn(n_nodes2)\n",
        "      return B\n",
        "\n",
        "# XavierInitializer ------------------------------------------------------------\n",
        "class XavierInitializer:\n",
        "    def __init__(self, sigma):\n",
        "      self.sigma = sigma/np.sqrt(n_nodes1)\n",
        "\n",
        "    def W(self, n_nodes1, n_nodes2, filter=None):\n",
        "      if filter == None:\n",
        "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
        "      else:\n",
        "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2, filter)\n",
        "      return W\n",
        "\n",
        "    def B(self, n_nodes2):\n",
        "      B = self.sigma * np.random.randn(n_nodes2)\n",
        "      return B\n",
        "\n",
        "# He-----------------------------------------------------------------------------\n",
        "class HeInitializer:\n",
        "    def __init__(self, sigma):\n",
        "      self.sigma = sigma * np.sqrt(2/n_nodes1) \n",
        "\n",
        "    def W(self, n_nodes1, n_nodes2, filter=None):\n",
        "      if filter == None:\n",
        "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
        "      else:\n",
        "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2, filter)\n",
        "      return W\n",
        "\n",
        "    def B(self, n_nodes2):\n",
        "      B = self.sigma * np.random.randn(n_nodes2)\n",
        "      return B"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRTtZxoA9ga5"
      },
      "source": [
        "# 最適化クラス\n",
        "# SGD---------------------------------------------\n",
        "class SGD:\n",
        "    def __init__(self, lr):\n",
        "        self.lr = lr\n",
        "\n",
        "    def update(self, layer):\n",
        "      layer.W -= self.lr * layer.dW\n",
        "      layer.B -= self.lr * layer.dB\n",
        "      return layer\n",
        "\n",
        "# AdaGrad----------------------------------------\n",
        "class AdaGrad:\n",
        "    def __init__(self, lr):\n",
        "      self.lr = lr\n",
        "      self.hw = 0\n",
        "      self.hb = 0\n",
        "  \n",
        "    def update(self, layer):\n",
        "      self.hw += layer.dW * layer.dW\n",
        "      self.hb = layer.dB * layer.dB\n",
        "      layer.W -= self.lr * layer.dW / (np.sqrt(self.hw) +1e-7)\n",
        "      layer.B -= self.lr * layer.dB / (np.sqrt(self.hb) +1e-7)\n",
        "      return layer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAndz8vR9gYS"
      },
      "source": [
        "# 活性化関数クラス\n",
        "# ソフトマックス関数のクラス----------------------------------------------\n",
        "class Softmax:\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  # forward時の処理\n",
        "  def forward(self, X):\n",
        "    y_hat = np.exp(X) / np.sum(np.exp(X), axis = 1).reshape(-1,1)\n",
        "    return y_hat\n",
        "\n",
        "  # backward時の処理\n",
        "  def backward(self, X, Y):\n",
        "    loss = -np.mean(Y * np.log(X + 1e-7))\n",
        "    dA3 = X - Y\n",
        "    return dA3, loss\n",
        "\n",
        "# ReLU関数のクラス----------------------------------------------\n",
        "class ReLU:\n",
        "  def __init__(self):\n",
        "        pass\n",
        "  # forward時の処理\n",
        "  def forward(self, X):\n",
        "    self.X =X\n",
        "    return np.maximum(0, X)\n",
        "    \n",
        "  # backward時の処理\n",
        "  def backward(self, dout):\n",
        "    return np.where(self.X > 0, dout, 0)\n",
        "\n",
        "# tanh関数のクラス----------------------------------------------\n",
        "class Tanh:\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  # forward時の処理\n",
        "  def forward(self, X):\n",
        "    self.out = np.tanh(X)\n",
        "    return self.out\n",
        "\n",
        "  # backward時の処理\n",
        "  def backward(self, X):\n",
        "    return X*(1-self.out**2)\n",
        "\n",
        "# sigmoid関数のクラス----------------------------------------------\n",
        "class Sigmoid:\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  # forward時の処理\n",
        "  def forward(self, z):\n",
        "    self.out = 1 / (1+np.exp(-z))\n",
        "    return self.out\n",
        "\n",
        "  # backward時の処理\n",
        "  def backward(self, z):\n",
        "    return z*(1-self.out)*self.out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UDiVexjLvLk"
      },
      "source": [
        "### 前処理"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2f8VReAI9gRy"
      },
      "source": [
        "class GetMiniBatch:\n",
        "    \"\"\"\n",
        "    ミニバッチを取得するイテレータ\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : 次の形のndarray, shape (n_samples, n_features)\n",
        "      訓練データ\n",
        "    y : 次の形のndarray, shape (n_samples, 1)\n",
        "      正解値\n",
        "    batch_size : int\n",
        "      バッチサイズ\n",
        "    seed : int\n",
        "      NumPyの乱数のシード\n",
        "    \"\"\"\n",
        "    def __init__(self, X, y, batch_size = 20, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self._X = X[shuffle_index]\n",
        "        self._y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self._X[p0:p1], self._y[p0:p1]        \n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self._X[p0:p1], self._y[p0:p1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7vbiylz9gM0"
      },
      "source": [
        "from keras.datasets import mnist\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# 平滑化（flatten）\n",
        "X_train_fltten = X_train.reshape(-1, 784)\n",
        "X_test_fltten = X_test.reshape(-1, 784)\n",
        "\n",
        "# float化と0or1処理\n",
        "X_train_flt = X_train_fltten.astype(np.float)\n",
        "X_test_flt = X_test_fltten.astype(np.float)\n",
        "X_train_flt /= 255\n",
        "X_test_flt /= 255\n",
        "\n",
        "# one hot処理\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
        "y_test_one_hot = enc.transform(y_test[:, np.newaxis])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "949gNi7o9gLZ"
      },
      "source": [
        "# train, testの分割\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_val, y_train_one_hot, y_val_one_hot = train_test_split(np.array(X_train_flt), np.array(y_train_one_hot), test_size=0.2)\n",
        "print(\"x_train\",x_train.shape, \"x_val\", x_val.shape, \"y_train_one_hot.shape\", y_train_one_hot.shape, \"y_val_one_hot\", y_val_one_hot.shape) # (48000, 784)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYyixhizLy5s"
      },
      "source": [
        "sdnn = ScratchDeepNeuralNetworkClassifier(epoch_num=7)\n",
        "sdnn.fit(HeInitializer, ReLU, AdaGrad, x_train, y_train_one_hot, x_val, y_val_one_hot, sigma=0.01, lr=0.01)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7IvL1crLy8z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LCkyKvgLy2E"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}