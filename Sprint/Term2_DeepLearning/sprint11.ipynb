{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "term2_sprint11.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMB01rCPAEtLD4nL1tlgTK4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/T-Sawao/diveintocode-ml3/blob/main/term2_sprint11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcWKJfDCgaLi"
      },
      "source": [
        "# **Sprint11 深層学習スクラッチ 畳み込みニューラルネットワーク1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCK2jhtxgjWg"
      },
      "source": [
        "## 1.このSprintについて\n",
        "\n",
        "**Sprintの目的**  \n",
        "スクラッチを通してCNNの基礎を理解する\n",
        "\n",
        "**どのように学ぶか**  \n",
        "スクラッチで1次元用畳み込みニューラルネットワークを実装した後、学習と検証を行なっていきます。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HczNh-9AOdE"
      },
      "source": [
        "## 2.1次元の畳み込みニューラルネットワークスクラッチ\n",
        "\n",
        "畳み込みニューラルネットワーク（CNN） のクラスをスクラッチで作成していきます。NumPyなど最低限のライブラリのみを使いアルゴリズムを実装していきます。\n",
        "\n",
        "\n",
        "このSprintでは1次元の 畳み込み層 を作成し、畳み込みの基礎を理解することを目指します。次のSprintでは2次元畳み込み層とプーリング層を作成することで、一般的に画像に対して利用されるCNNを完成させます。\n",
        "\n",
        "\n",
        "クラスの名前はScratch1dCNNClassifierとしてください。クラスの構造などは前のSprintで作成したScratchDeepNeuralNetrowkClassifierを参考にしてください。\n",
        "\n",
        "\n",
        "**1次元畳み込み層とは**  \n",
        "CNNでは画像に対しての2次元畳み込み層が定番ですが、ここでは理解しやすくするためにまずは1次元畳み込み層を実装します。1次元畳み込みは実用上は自然言語や波形データなどの 系列データ で使われることが多いです。\n",
        "\n",
        "\n",
        "畳み込みは任意の次元に対して考えることができ、立体データに対しての3次元畳み込みまではフレームワークで一般的に用意されています。\n",
        "\n",
        "\n",
        "**データセットの用意**  \n",
        "検証には引き続きMNISTデータセットを使用します。1次元畳み込みでは全結合のニューラルネットワークと同様に平滑化されたものを入力します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Us6zhHb2HbS"
      },
      "source": [
        "## 【問題1】チャンネル数を1に限定した1次元畳み込み層クラスの作成\n",
        "チャンネル数を1に限定した1次元畳み込み層のクラスSimpleConv1dを作成してください。基本構造は前のSprintで作成した全結合層のFCクラスと同じになります。なお、重みの初期化に関するクラスは必要に応じて作り変えてください。Xavierの初期値などを使う点は全結合層と同様です。\n",
        "\n",
        "ここでは パディング は考えず、ストライド も1に固定します。また、複数のデータを同時に処理することも考えなくて良く、バッチサイズは1のみに対応してください。この部分の拡張はアドバンス課題とします。\n",
        "\n",
        "フォワードプロパゲーションの数式は以下のようになります。\n",
        "$$\\alpha_i = \\sum_{s=0}^{F-1}x_{(i+s)}W_s + b$$\n",
        "\n",
        "$a_i$ : 出力される配列のi番目の値\n",
        "\n",
        "$F$ : フィルタのサイズ\n",
        "\n",
        "$x_{(i+s})$ : 入力の配列の(i+s)番目の値\n",
        "\n",
        "$w_s$ : 重みの配列のs番目の値\n",
        "\n",
        "$b$ : バイアス項\n",
        "\n",
        "全てスカラーです。\n",
        "\n",
        "次に更新式です。ここがAdaGradなどに置き換えられる点は全結合層と同様です。\n",
        "$$w'_s = w_s -\\alpha\\frac{\\partial L}{\\partial w_s}$$$$b' = b - \\alpha\\frac{\\partial L}{\\partial b}$$\n",
        "\n",
        "$\\alpha$ : 学習率\n",
        "\n",
        "$\\frac{\\partial L}{\\partial w_s}$ : w_s に関する損失 $L$ の勾配\n",
        "\n",
        "$\\frac{\\partial L}{\\partial b}$ : $b$ に関する損失 $L$ の勾配\n",
        "\n",
        "勾配 $\\frac{\\partial L}{\\partial w_s}$ や $\\frac{\\partial L}{\\partial b}$ を求めるためのバックプロパゲーションの数式が以下です。\n",
        "\n",
        "$$\\frac{\\partial L}{\\partial w_s} = \\sum_{i=0}^{N_{out}-1}\\frac{\\partial L}{\\partial a_i}x_{(i+s}$$$$\\frac{\\partial L}{\\partial b} =  \\sum_{i=0}^{N_{out}-1}\\frac{\\partial L}{\\partial a_i}$$\n",
        "$\\frac{\\partial L}{\\partial a_i}$: 勾配の配列のi番目の値\n",
        "\n",
        "$N_{out} : 出力のサイズ\n",
        "\n",
        "前の層に流す誤差の数式は以下です。　　$$\\frac{\\partial L}{\\partial x_j} = \\sum_{s=0}^{F-1}\\frac{\\partial L}{\\partial a_{j-s}}w_s$$\n",
        "\n",
        "$\\frac{\\partial L}{\\partial x_j}$ : 前の層に流す誤差の配列のj番目の値\n",
        "\n",
        "ただし、 $j-s&lt;0$ または $j-s&gt;N_{out}-1$ のとき$\\frac{\\partial L}{\\partial a_{j-s}}=0$ です。\n",
        "\n",
        "全結合層との大きな違いは、重みが複数の特徴量に対して共有されていることです。この場合は共有されている分の誤差を全て足すことで勾配を求めます。計算グラフ上での分岐はバックプロパゲーションの際に誤差の足し算をすれば良いことになります。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHRCFj5k2d_1"
      },
      "source": [
        "## 【問題2】1次元畳み込み後の出力サイズの計算¶\n",
        "畳み込みを行うと特徴量の数が変化します。どのように変化するかは以下の数式から求められます。パディングやストライドも含めています。この計算を行う関数を作成してください。\n",
        "$$N_{out} = \\frac{N_{in} + 2P - F}{S} + 1$$\n",
        "\n",
        "$N_{out}$ : 出力のサイズ（特徴量の数）\n",
        "\n",
        "$N_{in}$ : 入力のサイズ（特徴量の数）\n",
        "\n",
        "$P$ : ある方向へのパディングの数\n",
        "\n",
        "$F$ : フィルタのサイズ\n",
        "\n",
        "$S$ : ストライドのサイズ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROkTCMHvnxe2"
      },
      "source": [
        "### 1.1.1（解答）\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfEovpOnkz6h"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as setattr\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-MY-ybio6h1"
      },
      "source": [
        "class Conv1(): \n",
        "    def __init__(self, stride=1, pad=0):\n",
        "        self.stride = stride\n",
        "        self.pad = pad\n",
        "\n",
        "        # ウェイト&バイアスと、勾配のサンプルデータ\n",
        "        self.w = np.array([3, 5, 7])\n",
        "        self.b = np.array([1])\n",
        "\n",
        "        # フィルタ設定\n",
        "        self.in_ch = 1                    # 入力チャンネル数\n",
        "        self.out_ch = 1                   # 出力チャネル数=フィルタ枚数\n",
        "        self.filter_w = self.w.shape[-1]    # フィルタ幅\n",
        "\n",
        "# 問題1--------------------------------------------------------------\n",
        "    # フォワードプロパゲーション時の処理\n",
        "    def forward(self, X):\n",
        "        self.X = X\n",
        "        self.in_size = X.shape[-1]         # 入力サイズ=入力特徴量\n",
        "        self.out_size = self.output_size() # 出力サイズ=出力特徴量\n",
        "\n",
        "        A = np.zeros(self.out_size) # 0のベクトル、shape(出力サイズ)\n",
        "        for i in range(self.out_size): # 出力サイズ\n",
        "            A[i] = np.sum(self.X[i:(i+self.filter_w)]*self.w)+self.b\n",
        "        return A\n",
        "\n",
        "    # バックプロパゲーション時の処理\n",
        "    def backward(self, da):\n",
        "        db = np.sum(da)\n",
        "\n",
        "        dw = np.zeros((self.in_ch, self.filter_w)) # 0のベクトル、shape(入力チャネル数,フィルタ幅)\n",
        "        for i in range(self.out_size): # 出力サイズ\n",
        "            dw += da[i] * self.X[i : (i + self.filter_w)]\n",
        "\n",
        "        dx = np.zeros(self.in_size) # 0のベクトル、shape(入力サイズ)\n",
        "        for i in range(self.out_size): # 出力サイズ\n",
        "            dx[0+self.stride*i : self.filter_w + self.stride*i] += da[i] * self.w\n",
        "    \n",
        "        print(f\"db:{db} dw{dw} dx{dx}\")\n",
        "\n",
        "# 問題2ーーーーーーーーーーーーーーーーーーーーーーーーーー\n",
        "    def output_size(self):   \n",
        "        out_size = (self.in_size + 2*self.pad - self.filter_w)/self.stride + 1\n",
        "        return int(out_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWusObzQrzAX"
      },
      "source": [
        "## 【問題3】小さな配列での1次元畳み込み層の実験\n",
        "次に示す小さな配列でフォワードプロパゲーションとバックプロパゲーションが正しく行えているか確認してください。\n",
        "\n",
        "入力$x$、重み$w$、バイアス$b$を次のようにします。\n",
        "\n",
        "- x = np.array([1,2,3,4])\n",
        "- w = np.array([3, 5, 7])\n",
        "- b = np.array([1])  \n",
        "\n",
        "フォワードプロパゲーションをすると出力は次のようになります。\n",
        "\n",
        "- a = np.array([35, 50])\n",
        "\n",
        "次にバックプロパゲーションを考えます。誤差は次のようであったとします。\n",
        "\n",
        "-delta_a = np.array([10, 20])\n",
        "\n",
        "バックプロパゲーションをすると次のような値になります。\n",
        "\n",
        "- delta_b = np.array([30])\n",
        "- delta_w = np.array([50, 80, 110])\n",
        "- delta_x = np.array([30, 110, 170, 140])\n",
        "\n",
        "**実装上の工夫**  \n",
        "畳み込みを実装する場合は、まずはfor文を重ねていく形で構いません。しかし、できるだけ計算は効率化させたいため、以下の式を一度に計算する方法を考えることにします。\n",
        "$$a_i = \\sum_{s=0}^{F-1}x_{i+s}w_s + b$$\n",
        "\n",
        "バイアス項は単純な足し算のため、重みの部分を見ます。$$\\sum_{s=0}^{F-1}x_{i+s}w_s$$\n",
        "\n",
        "これは、xの一部を取り出した配列とwの配列の内積です。具体的な状況を考えると、以下のようなコードで計算できます。この例では流れを分かりやすくするために、各要素同士でアダマール積を計算してから合計を計算しています。これは結果的に内積と同様です。\n",
        "\n",
        "- x = np.array([1, 2, 3, 4])\n",
        "- w = np.array([3, 5, 7])\n",
        "- a = np.empty((2, 3))\n",
        "- indexes0 = np.array([0, 1, 2]).astype(np.int)\n",
        "- indexes1 = np.array([1, 2, 3]).astype(np.int)\n",
        "- a[0] = x[indexes0]w # x[indexes0]は([1, 2, 3])である \n",
        "- a[1] = x[indexes1]w # x[indexes1]は([2, 3, 4])である\n",
        "- a = a.sum(axis=1)\n",
        "\n",
        "ndarrayは配列を使ったインデックス指定ができることを利用した方法です。\n",
        "\n",
        "また、二次元配列を使えば一次元配列から二次元配列が取り出せます。\n",
        "\n",
        "- x = np.array([1, 2, 3, 4])\n",
        "- indexes = np.array([[0, 1, 2], [1, 2, 3]]).astype(np.int)\n",
        "- print(x[indexes]) # ([[1, 2, 3], [2, 3, 4]])\n",
        "\n",
        "このこととブロードキャストなどをうまく組み合わせることで、一度にまとめて計算することも可能です。\n",
        "\n",
        "畳み込みの計算方法に正解はないので、自分なりに効率化していってください。\n",
        "\n",
        "**《参考》**  \n",
        "以下のページのInteger array indexingの部分がこの方法についての記述です。\n",
        "\n",
        "Indexing — NumPy v1.17 Manual  \n",
        "https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWlw1g0TyvAw"
      },
      "source": [
        "### 3.1.1（解答） forwardプロパゲーション"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_aNON5R5Vf1"
      },
      "source": [
        "# サンプルデータ\n",
        "x = np.array([1, 2, 3, 4])\n",
        "\n",
        "conv1 = Conv1()\n",
        "conv1.forward(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgXhLRZ9zmjv"
      },
      "source": [
        "### 3.2.1（解答）backプロパゲーション"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIR4LeQxzFYI"
      },
      "source": [
        "# サンプルデータ\n",
        "da = np.array([10, 20])\n",
        "\n",
        "conv1.backward(da)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPc-gEqR3gjZ"
      },
      "source": [
        "## 【問題4】チャンネル数を限定しない1次元畳み込み層クラスの作成\n",
        "チャンネル数を1に限定しない1次元畳み込み層のクラスConv1dを作成してください。\n",
        "\n",
        "例えば以下のようなx, w, bがあった場合は、\n",
        "\n",
        "- x = np.array([[1, 2, 3, 4], [2, 3, 4, 5]]) # shape(2, 4)で、（入力チャンネル数、特徴量数）である。\n",
        "- w = np.ones((3, 2, 3)) # 例の簡略化のため全て1とする。(出力チャンネル数、入力チャンネル数、フィルタサイズ)である。\n",
        "- b = np.array([1, 2, 3]) # （出力チャンネル数）\n",
        "\n",
        "出力は次のようになります。\n",
        "\n",
        "- a = np.array([[16, 22], [17, 23], [18, 24]]) # shape(3, 2)で、（出力チャンネル数、特徴量数）である。\n",
        "\n",
        "入力が2チャンネル、出力が3チャンネルの例です。計算グラフを書いた上で、バックプロパゲーションも手計算で考えてみましょう。計算グラフの中には和と積しか登場しないので、微分を新たに考える必要はありません。\n",
        "\n",
        "**《補足》**  \n",
        "チャンネル数を加える場合、配列をどういう順番にするかという問題があります。(バッチサイズ、チャンネル数、特徴量数)または(バッチサイズ、特徴量数、チャンネル数)が一般的で、ライブラリによって順番は異なっています。（切り替えて使用できるものもあります）\n",
        "\n",
        "今回のスクラッチでは自身の実装上どちらが効率的かを考えて選んでください。上記の例ではバッチサイズは考えておらず、(チャンネル数、特徴量数)です。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0jRPnJD-Nbs"
      },
      "source": [
        "### 4.1.1（解答）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9HFhknz0R_W"
      },
      "source": [
        "class Conv1_1(): \n",
        "    def __init__(self, stride=1, pad=0):\n",
        "        self.stride = stride\n",
        "        self.pad = pad\n",
        "\n",
        "        # ウェイト&バイアスと、勾配のサンプルデータ\n",
        "        self.W = np.ones((3, 2, 3)) # 例の簡略化のため全て1とする。(出力チャンネル数、入力チャンネル数、フィルタサイズ)である。\n",
        "        self.B = np.array([1, 2, 3]) # （出力チャンネル数）\n",
        "\n",
        "# 問題1--------------------------------------------------------------\n",
        "    # フォワードプロパゲーション時の処理\n",
        "    def forward(self, X):\n",
        "        self.X = X[np.newaxis,:]\n",
        "\n",
        "        #shapeの取得\n",
        "        N, C, W   = self.X.shape #(サンプル数, 入力チャンネル, 特徴量数)\n",
        "        FN, C, FS = self.W.shape #(出力チャンネル,  入力チャンネル, フィルタサイズ)\n",
        "\n",
        "        #出力のサイズ(特徴量方向)\n",
        "        self.out_w = self.output_size(W, FS)\n",
        "\n",
        "        #出力の形\n",
        "        A = np.zeros((N, FN, self.out_w)) #(バッチサイズ, 出力チャンネル数, 特徴量数)\n",
        "\n",
        "        #サンプル数\n",
        "        for i in range(N):\n",
        "            #出力チャンネル数\n",
        "            for j in range(FN):\n",
        "                #入力チャンネル数\n",
        "                for k in range(C):\n",
        "                    #特徴量数\n",
        "                    for l in range(self.out_w):\n",
        "                        #足しあげる\n",
        "                        A[i, j, l] += np.sum(self.X[i, k, l: l+FS] * self.W[j, k, :] ) \n",
        "        #バイアスを足す\n",
        "        A += self.B.reshape(1, -1, 1)\n",
        "        return A\n",
        "\n",
        "    # バックプロパゲーション時の処理\n",
        "    def backward(self, dA):\n",
        "        #shapeの取得\n",
        "        FN, C, FS = self.W.shape #(出力チャンネル,  入力チャンネル, フィルタサイズ)\n",
        "        N, C, W   = self.X.shape #(サンプル数, 入力チャンネル, 特徴量数)\n",
        "\n",
        "        #da(バッチサイズ, 出力チャンネル数, 特徴量数)\n",
        "        dA = dA[np.newaxis, :]\n",
        "\n",
        "        #ｗの勾配\n",
        "        dW = np.zeros(self.W.shape)\n",
        "\n",
        "        #xの勾配\n",
        "        dX = np.zeros(self.X.shape)\n",
        "\n",
        "        #サンプル数\n",
        "        for i in range(N):\n",
        "            #出力チャンネル\n",
        "            for j in range(FN):\n",
        "                #入力チャンネル\n",
        "                for k in range(C):\n",
        "                    #フィルタサイズ\n",
        "                    for l in range(FS):\n",
        "                        #出力特徴量\n",
        "                        for m in range(self.out_w):\n",
        "                            #足しあげる\n",
        "                            dW[j, k, l] += dA[i, j, m] * self.X[i, k, l + m]\n",
        "                            dX[i, k, l + m] += dA[i, j, m] * self.W[j, k, l]\n",
        "\n",
        "        #bの勾配\n",
        "        dB = dA.sum(axis=2)\n",
        "        return dX\n",
        "\n",
        "# 問題2--------------------------------------------------------------\n",
        "    def output_size(self, W, FS):   \n",
        "        out_size = (W + 2*self.pad - FS)/self.stride + 1\n",
        "        return int(out_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sO0EVaWk7zvp"
      },
      "source": [
        "x = np.array([[1, 2, 3, 4], [2, 3, 4, 5]])  # shape(2, 4)で、（入力チャンネル数、特徴量数）である。"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2BCJpKO6906"
      },
      "source": [
        "conv1_1 = Conv1_1()\n",
        "conv1_1.forward(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZSFv9ll7pNQ"
      },
      "source": [
        "#バックプロバケーションの初期値\n",
        "da = np.array([[10, 20], [10, 20], [10, 20]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4U7h8f676gv"
      },
      "source": [
        "conv1_1.backward(da)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eh680InL36op"
      },
      "source": [
        "## 【問題5】（アドバンス課題）パディングの実装\n",
        "畳み込み層にパディングの機能を加えてください。1次元配列の場合、前後にn個特徴量を増やせるようにしてください。\n",
        "\n",
        "\n",
        "最も単純なパディングは全て0で埋める ゼロパディング であり、CNNでは一般的です。他に端の値を繰り返す方法などもあります。\n",
        "\n",
        "\n",
        "フレームワークによっては、元の入力のサイズを保つようにという指定をすることができます。この機能も持たせておくと便利です。なお、NumPyにはパディングの関数が存在します。\n",
        "\n",
        "\n",
        "numpy.pad — NumPy v1.17 Manual  \n",
        "https://docs.scipy.org/doc/numpy/reference/generated/numpy.pad.html\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1CPC6FnN2La"
      },
      "source": [
        "### 5.1.1（解答なし）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smDm7woQ3_We"
      },
      "source": [
        "## 【問題6】（アドバンス課題）ミニバッチへの対応\n",
        "ここまでの課題はバッチサイズ1で良いとしてきました。しかし、実際は全結合層同様にミニバッチ学習が行われます。Conv1dクラスを複数のデータが同時に計算できるように変更してください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EJ0NgtY5lqe"
      },
      "source": [
        "### 6.1.1（解答）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCRqwtsJ9Cv3"
      },
      "source": [
        "# ミニバッチ処理のクラス\n",
        "class GetMiniBatch:\n",
        "    \"\"\"\n",
        "    ミニバッチを取得するイテレータ\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : 次の形のndarray, shape (n_samples, n_features)\n",
        "      訓練データ\n",
        "    y : 次の形のndarray, shape (n_samples, 1)\n",
        "      正解値\n",
        "    batch_size : int\n",
        "      バッチサイズ\n",
        "    seed : int\n",
        "      NumPyの乱数のシード\n",
        "    \"\"\"\n",
        "    def __init__(self, X, y, batch_size = 20, seed=None):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self._X = X[shuffle_index]\n",
        "        self._y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    \n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self._X[p0:p1], self._y[p0:p1] \n",
        "    \n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "    \n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self._X[p0:p1], self._y[p0:p1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UMu837g4EKv"
      },
      "source": [
        "## 【問題7】（アドバンス課題）任意のストライド数\n",
        "ストライドは1限定の実装をしてきましたが、任意のストライド数に対応できるようにしてください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqpy93LtN_Qg"
      },
      "source": [
        "### 7.1.1（解答なし）\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPfEot4nU3UK"
      },
      "source": [
        "# 3.検証\n",
        "\n",
        "##【問題8】学習と推定  \n",
        "これまで使ってきたニューラルネットワークの全結合層の一部をConv1dに置き換えてMNISTを学習・推定し、Accuracyを計算してください。\n",
        "\n",
        "\n",
        "出力層だけは全結合層をそのまま使ってください。ただし、チャンネルが複数ある状態では全結合層への入力は行えません。その段階でのチャンネルは1になるようにするか、 平滑化 を行なってください。\n",
        "\n",
        "\n",
        "画像に対しての1次元畳み込みは実用上は行わないことのため、精度は問いません。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zJ4ZonTfzsy"
      },
      "source": [
        "### 8.1.1 （準備）MNISTデータセット"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sGj38cePWNL"
      },
      "source": [
        "from keras.datasets import mnist\n",
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiVaErxqmlK6"
      },
      "source": [
        "# 平準化\n",
        "X_train = X_train.reshape(-1, 1, 784)\n",
        "X_test = X_test.reshape(-1, 1, 784)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zozKaRhgmlBG"
      },
      "source": [
        "# float化と0or1処理\n",
        "X_train = X_train.astype(np.float)\n",
        "X_test = X_test.astype(np.float)\n",
        "X_train /= 255\n",
        "X_test /= 255"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35LqXp1p2w4T"
      },
      "source": [
        "x_train, x_val, y_train, y_val = train_test_split(X_train, Y_train, test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSHjB-alooNa"
      },
      "source": [
        "print(\"x_train.shape\", x_train.shape)\n",
        "print(\"x_val.shape\", x_val.shape)\n",
        "print(\"y_train.shape\", y_train.shape)\n",
        "print(\"y_val.shape\", y_val.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSskPMS6pmm6"
      },
      "source": [
        "### 8.2.1 解答用コード"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Oatnu0s9Cg0"
      },
      "source": [
        "# メインクラス\n",
        "class ScratchDeepNeuralNetworkClassifier():\n",
        "    \"\"\"\n",
        "    ニューラルネットワーク分類器\n",
        "\n",
        "    Parameters\n",
        "    --------------\n",
        "    epoc : int\n",
        "        エポック数\n",
        "    activaiton : {'sigmoid', 'tanh', 'relu'} default 'relu'\n",
        "        活性化関数の種類\n",
        "    solver :  {'sgd', 'adagrad'}, default 'adam'\n",
        "        最適化手法の種類\n",
        "    alpha : float\n",
        "        学習率\n",
        "    batch_size : int\n",
        "        バッチサイズ\n",
        "    initial : {'simple', 'xavier', 'he'} default 'he'\n",
        "        重みの初期化方法\n",
        "    sigma : float\n",
        "        重みパラメータ(ガウス分布の標準偏差)\n",
        "    n_nodes1 : int\n",
        "        1層目の数\n",
        "    n_nodes2 : int\n",
        "        2層目の数\n",
        "    n_output : int\n",
        "        出力層の数\n",
        "    verbose : bool\n",
        "        学習過程の出力の有無\n",
        "        \n",
        "    Attributes\n",
        "    -------------\n",
        "    conv1 : インスタンス\n",
        "        畳み込み層のインスタンス\n",
        "    FC1 :  インスタンス\n",
        "        結合層のインスタンス\n",
        "    FC2 :   インスタンス\n",
        "        結合層のインスタンス\n",
        "    FC3 :   インスタンス\n",
        "        結合層のインスタンス\n",
        "    activation1 : インスタンス\n",
        "        活性化関数のインスタンス\n",
        "    activation2 : インスタンス\n",
        "        活性化関数のインスタンス\n",
        "    activation3 : インスタンス\n",
        "        活性化関数のインスタンス\n",
        "    loss_list : list\n",
        "        学習用データの損失を記録するリスト\n",
        "    mini_loss_list : list\n",
        "        学習用データの損失を記録するリスト(ミニバッチごと)\n",
        "    val_loss_list : list\n",
        "        検証用データの損失を記録するリスト\n",
        "    mini_val_loss_list : list\n",
        "        検証用データの損失を記録するリスト(ミニバッチごと)\n",
        "    flat : インスタンス\n",
        "        平滑化のインスタンス\n",
        "    \"\"\"\n",
        "    def __init__(self, epoch=10, activation='relu', optimization='adagrad', alpha=0.005,\n",
        "                             batch_size=10, initialization='he', sigma=0.01, n_nodes1=50, \n",
        "                             n_nodes2=25, filter_num=3,  filter_size=7, verbose=True):\n",
        "        #ハイパーパラメータ \n",
        "        self.epoch        = epoch       #エポック数\n",
        "        self.activation    = activation   #活性化関数\n",
        "        self.optimization = optimization #最適化の手法\n",
        "        self.alpha        = alpha        #学習率\n",
        "        self.batch_size  = batch_size   #バッチサイズ\n",
        "        self.initialization = initialization  #重みの初期化方法\n",
        "        self.sigma       = sigma        #重みパラメータ\n",
        "        self.n_nodes1   = n_nodes1    #1層目のノード数\n",
        "        self.n_nodes2   = n_nodes2    #2層目のノード数\n",
        "        self.filter_num   = filter_num   #フィルタの数\n",
        "        self.filter_size   = filter_size    #フィルタのサイズ\n",
        "        self.verbose     = verbose      #学習過程の出力(True : 有, False : 無)\n",
        "        \n",
        "        #インスタンス変数\n",
        "        self.conv1        = None #畳み込み層のインスタンス\n",
        "        self.FC1          = None #結合層のインスタンス\n",
        "        self.FC2          = None #結合層のインスタンス\n",
        "        self.activation1   = None #活性化関数のインスタンス\n",
        "        self.activation2   = None #活性化関数のインスタンス\n",
        "        self.activation3   = None #活性化関数のインスタンス\n",
        "        self.loss_list      = None #学習用データの損失を記録する配列\n",
        "        self.mini_loss_list = None #学習用データの損失を記録する配列(ミニバッチごと)\n",
        "        self.val_loss_list  = None #検証用データの損失を記録する配列\n",
        "        self.mini_loss_list = None #検証用データの損失を記録する配列(ミニバッチごと)\n",
        "        self.flat           = None #平滑化インスタンス\n",
        "    \n",
        "    def fit(self, X, y, X_val=None, y_val=None):\n",
        "        \"\"\"\n",
        "        ニューラルネットワーク分類器を学習する。\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 次の形のndarray, shape (n_samples, n_features)\n",
        "            学習用データの特徴量\n",
        "        y : 次の形のndarray, shape (n_samples, )\n",
        "            学習用データの正解値\n",
        "        X_val : 次の形のndarray, shape (n_samples, n_features)\n",
        "            検証用データの特徴量\n",
        "        y_val : 次の形のndarray, shape (n_samples, )\n",
        "            検証用データの正解値\n",
        "        \"\"\"            \n",
        "        #one_hotエンコーディング\n",
        "        n_output = np.unique(y).shape[0]\n",
        "        y_onehot = self._one_hot(y, n_output)\n",
        "        \n",
        "        #ミニバッチを取得するクラスをインスタンス化\n",
        "        train_mini_batch = GetMiniBatch(X, y_onehot, self.batch_size)\n",
        "        \n",
        "        #valがあるとき\n",
        "        if X_val is not None and y_val is not None:       \n",
        "            y_val_onehot = self._one_hot(y_val, n_output) #one-hotエンコーディング\n",
        "            test_mini_batch = GetMiniBatch(X_val, y_val_onehot) #インスタンス化\n",
        "\n",
        "        #活性化関数の選択\n",
        "        if self.activation == 'sigmoid':\n",
        "            activate1 = Sigmoid()\n",
        "            activate2 = Sigmoid()\n",
        "            activate3 = Sigmoid()\n",
        "        elif self.activation == 'tanh':\n",
        "            activate1 = Tanh()\n",
        "            activate2 = Tanh()\n",
        "            activate3 = Tanh()\n",
        "        elif self.activation == 'relu':\n",
        "            activate1 = Relu()\n",
        "            activate2 = Relu()\n",
        "            activate3 = Relu()\n",
        "\n",
        "        \n",
        "        #最適化手法の選択\n",
        "        if self.optimization == 'sgd':\n",
        "            optimizer1 = SGD(self.alpha)\n",
        "            optimizer2 = SGD(self.alpha)\n",
        "            optimizer3 = SGD(self.alpha) \n",
        "            optimizer4 = SGD(self.alpha)\n",
        "            optimizer5 = SGD(self.alpha) \n",
        "        elif self.optimization == 'adagrad':\n",
        "            optimizer1 = AdaGrad(self.alpha)\n",
        "            optimizer2 = AdaGrad(self.alpha)\n",
        "            optimizer3 = AdaGrad(self.alpha)\n",
        "            optimizer4 = AdaGrad(self.alpha)\n",
        "            optimizer5 = AdaGrad(self.alpha)\n",
        "            \n",
        "        #重みの初期化方法の選択\n",
        "        if self.initialization == 'simple':\n",
        "            initializer1 = SimpleInitializer(self.sigma)\n",
        "            initializer2 = SimpleInitializer(self.sigma)\n",
        "            initializer3 = SimpleInitializer(self.sigma)\n",
        "            initializer4 = SimpleInitializer(self.sigma)\n",
        "            initializer5 = SimpleInitializer(self.sigma)\n",
        "        elif self.initialization == 'xavier':\n",
        "            initializer1 = XavierInitializer()\n",
        "            initializer2 = XavierInitializer()\n",
        "            initializer3 = XavierInitializer()\n",
        "            initializer4 = XavierInitializer()\n",
        "            initializer5 = XavierInitializer()\n",
        "        elif self.initialization == 'he':\n",
        "            initializer1 = HeInitializer()\n",
        "            initializer2 = HeInitializer()\n",
        "            initializer3 = HeInitializer()\n",
        "            initializer4 = HeInitializer()\n",
        "            initializer5 = HeInitializer()\n",
        "        \n",
        "        #畳み込み層\n",
        "        self.conv1 = Conv1_2(initializer1, optimizer1, self.filter_num, X.shape[1], self.filter_size)\n",
        "        self.activation1 = activate1\n",
        "        \n",
        "        #平滑化クラスのインスタンス化\n",
        "        self.flat = Flatten()\n",
        "        out = self.filter_num * (X.shape[2] - (self.filter_size - 1))\n",
        "\n",
        "        #結合層および活性化関数クラスのインスタンス化\n",
        "        self.FC1 = FC(out, self.n_nodes1, initializer2, optimizer2) #第1層\n",
        "        self.activation2 = activate2\n",
        "        self.FC2 = FC(self.n_nodes1, self.n_nodes2, initializer3, optimizer3) #第2層\n",
        "        self.activation3 = activate3\n",
        "        self.FC3 = FC(self.n_nodes2, y_onehot.shape[1], initializer4, optimizer4) #第3層\n",
        "        self.activation4 = Softmax()\n",
        "        \n",
        "        #損失を記録するリスト(エポックごと)\n",
        "        self.loss_list = []\n",
        "        self.val_loss_list= []\n",
        "        \n",
        "        #エポック数だけ繰り返す\n",
        "        for i in range(self.epoch):\n",
        "            \n",
        "            #損失を記録するリスト(イテレーション)\n",
        "            self.mini_loss_list = []\n",
        "            \n",
        "            #イテレーション数実行\n",
        "            for mini_X_train, mini_y_train in train_mini_batch:\n",
        "                \n",
        "                #フォワード\n",
        "                #畳み込み(1層目)\n",
        "                A1 = self.conv1.forward(mini_X_train) \n",
        "                Z1 = self.activation1.forward(A1)    \n",
        "                \n",
        "                #平滑化\n",
        "                F1 = self.flat.forward(Z1)\n",
        "                \n",
        "                #全結合層\n",
        "                A2 = self.FC1.forward(F1)                   \n",
        "                Z2 = self.activation2.forward(A2) \n",
        "                A3 = self.FC2.forward(Z2)             \n",
        "                Z3 = self.activation3.forward(A3)\n",
        "                A4 = self.FC3.forward(Z3)             \n",
        "                Z4 = self.activation4.forward(A4)\n",
        "            \n",
        "            #バックワード\n",
        "                #全結合層\n",
        "                dA4, mini_loss = self.activation4.backward(mini_y_train) \n",
        "                dZ4 = self.FC3.backward(dA4)          \n",
        "                dA3 = self.activation3.backward(dZ4) \n",
        "                dZ3 = self.FC2.backward(dA3)            \n",
        "                dA2 = self.activation2.backward(dZ3) \n",
        "                dZ2 = self.FC1.backward(dA2)    \n",
        "                \n",
        "                #shapeを戻す\n",
        "                dF1 = self.flat.backward(dZ2)\n",
        "                \n",
        "                #畳み込み層\n",
        "                dA2 = self.activation1.backward(dF1)\n",
        "                dZ1 = self.conv1.backward(dA2)\n",
        "\n",
        "                #イテレーションごとの損失をリストに格納\n",
        "                self.mini_loss_list.append(mini_loss)\n",
        "\n",
        "            #1エポックの損失をリストに格納\n",
        "            loss = np.mean(self.mini_loss_list)\n",
        "            self.loss_list.append(loss)\n",
        "\n",
        "            \n",
        "            #valがあるときもイテレーション数実行\n",
        "            if X_val is not None and y_val is not None:\n",
        "                \n",
        "                self.mini_val_loss_list = []\n",
        "                for mini_X_val, mini_y_val in test_mini_batch:\n",
        "              \n",
        "                    #確率を予測\n",
        "                    A1 = self.conv1.forward(mini_X_val)\n",
        "                    Z1 = self.activation1.forward(A1)\n",
        "                    F1 = self.flat.forward(Z1)\n",
        "                    A2 = self.FC1.forward(F1)\n",
        "                    Z2 = self.activation2.forward(A2)\n",
        "                    A3 = self.FC2.forward(Z2)\n",
        "                    Z3 = self.activation3.forward(A3)\n",
        "                    A4 = self.FC3.forward(Z3) \n",
        "                    Z4 = self.activation4.forward(A4)\n",
        "\n",
        "                    #損失を計算\n",
        "                    _, mini_val_loss = self.activation4.backward(mini_y_val)\n",
        "\n",
        "                    #イテレーションごとの損失をリストに格納\n",
        "                    self.mini_val_loss_list.append(mini_val_loss)\n",
        "\n",
        "                #1エポックの損失をリストに格納\n",
        "                val_loss = np.mean(self.mini_val_loss_list)\n",
        "                self.val_loss_list.append(val_loss)\n",
        "\n",
        "                \n",
        "            #学習過程を出力する場合\n",
        "            if self.verbose == True:\n",
        "                print('学習用データの学習過程' + str(i + 1) + 'epoc目 : ' + str(self.loss_list[i]))\n",
        "\n",
        "                #検証用データあり\n",
        "                if X_val is not None or y_val is not None:\n",
        "                    print('検証用データの学習過程' + str(i + 1) + 'epoc目 : ' + str(self.val_loss_list[i]))\n",
        "            \n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        ニューラルネットワーク分類器を使い推定する。\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 次の形のndarray, shape (n_samples, n_features)\n",
        "            サンプル\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        次の形のndarray, shape (n_samples, 1)\n",
        "            推定結果\n",
        "        \"\"\"\n",
        "        #フォワード\n",
        "        #畳み込み(1層目)\n",
        "        A1 = self.conv1.forward(X) \n",
        "        Z1 = self.activation1.forward(A1) \n",
        "        \n",
        "        #平滑化\n",
        "        F1 = self.flat.forward(Z1)\n",
        "        \n",
        "        #全結合層\n",
        "        A2 = self.FC1.forward(F1)\n",
        "        Z2 = self.activation2.forward(A2)\n",
        "        A3 = self.FC2.forward(Z2)\n",
        "        Z3 = self.activation3.forward(A3)\n",
        "        A4 = self.FC3.forward(Z3)\n",
        "        y_pred = self.activation4.forward(A4)  \n",
        "                       \n",
        "        return np.argmax(y_pred, axis=1)\n",
        "            \n",
        "\n",
        "    def _one_hot(self, y, n_output):\n",
        "        '''\n",
        "        one-hotエンコーディングを行う関数\n",
        "       \n",
        "       Parameters\n",
        "        ---------------\n",
        "        y : 次の形のndarray, shape (n_features, 1)\n",
        "            正解ラベルのベクトル\n",
        "        n_output : int\n",
        "            正解ラベルのユニーク値\n",
        "            \n",
        "        Returns\n",
        "        ----------\n",
        "        one_hot.T : 次の形のndarray, shape (n_features, n_output)\n",
        "        '''\n",
        "        #0配列を用意\n",
        "        one_hot = np.zeros((n_output, y.shape[0]))\n",
        "        \n",
        "        #0配列の該当する位置に1を挿入\n",
        "        for idx, val in enumerate(y.astype(int)):\n",
        "            one_hot[val, idx] = 1\n",
        "\n",
        "        return one_hot.T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OumpXwgX80II"
      },
      "source": [
        "class Conv1_2():\n",
        "    \"\"\"\n",
        "    チャンネル数を指定しない1次元畳み込み層クラス\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_nodes1 : int\n",
        "      前の層のノード数\n",
        "    n_nodes2 : int\n",
        "      後の層のノード数\n",
        "    initializer : 初期化方法のインスタンス\n",
        "    optimizer : 最適化手法のインスタンス\n",
        "    W : 重み\n",
        "    B : バイアス\n",
        "    X : 入力\n",
        "    out_w : 出力のサイズ\n",
        "    \"\"\"\n",
        "    def __init__(self, initializer, optimizer, filter_num, C, filter_size):\n",
        "        self.optimizer = optimizer\n",
        "        # 初期化\n",
        "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
        "        self.W = initializer.W(filter_num=filter_num, C=C, filter_size=filter_size)\n",
        "        self.B = initializer.B(filter_num)\n",
        "        \n",
        "        #インスタンス変数\n",
        "        self.X = None\n",
        "        self.out_w = None\n",
        "\n",
        "# 問題1--------------------------------------------------------------\n",
        "    # フォワードプロパゲーション時の処理\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        フォワードプロバケーション\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 入力\n",
        "            \n",
        "        Returns\n",
        "        ----------\n",
        "        A : 出力\n",
        "        \"\"\"\n",
        "        self.X = X\n",
        "\n",
        "        #shapeの取得\n",
        "        batch, C, W = self.X.shape #(バッチサイズ, 入力チャンネル, 特徴量)\n",
        "        FN, C, FS = self.W.shape #(出力チャンネル,  入力チャンネル, フィルタサイズ)\n",
        "\n",
        "        #出力のサイズ(特徴量方向)\n",
        "        self.out_w = self._n_out(W, 0, FS, 1)\n",
        "\n",
        "        #出力の形\n",
        "        A = np.zeros((batch, FN, self.out_w)) #(バッチサイズ, 出力チャンネル数, 特徴量数)\n",
        "\n",
        "        #バッチ数\n",
        "        for i in range(batch):\n",
        "            #出力チャンネル数\n",
        "            for j in range(FN):\n",
        "                #入力チャンネル数\n",
        "                for k in range(C):\n",
        "                    #特徴量数\n",
        "                    for l in range(self.out_w):\n",
        "                        #足しあげる\n",
        "                        A[i, j, l] += np.sum(self.X[i, k, l: l + FS] * self.W[j, k, :] ) \n",
        "\n",
        "        #バイアスを足す\n",
        "        A += self.B.reshape(1, -1, 1)\n",
        "        \n",
        "        return A\n",
        "\n",
        "    def backward(self, dA):\n",
        "        \"\"\"\n",
        "        バックプロバケーション\n",
        "        Parameters\n",
        "        ----------\n",
        "        dA : 後ろから流れてきた勾配\n",
        "            \n",
        "        Returns\n",
        "        ----------\n",
        "        dX : 前に流す勾配\n",
        "        \"\"\"\n",
        "        FN, C, FS = self.W.shape # (出力チャンネル数、入力チャンネル数、フィルタサイズ)\n",
        "        batch, C, W = self.X.shape # (バッチサイズ、入力チャンネル数、特徴量数)\n",
        "\n",
        "        #空の配列\n",
        "        self.dW = np.zeros(self.W.shape) #Wの勾配\n",
        "        dX = np.zeros(self.X.shape) #Xの勾配\n",
        "\n",
        "        #バッチサイズ\n",
        "        for i in range(batch):\n",
        "            #出力チャンネル\n",
        "            for j in range(FN):\n",
        "                #入力チャンネル\n",
        "                for k in range(C):\n",
        "                    #フィルタサイズ\n",
        "                    for l in range(FS):\n",
        "                        #出力特徴量\n",
        "                        for m in range(self.out_w):\n",
        "                            #足しあげる\n",
        "                            self.dW[j, k, l] += dA[i, j, m] * self.X[i, k, l + m]\n",
        "                            dX[i, k, l + m] += dA[i, j, m] * self.W[j, k, l]\n",
        "\n",
        "        #bの勾配\n",
        "        self.dB = dA.sum(axis=2)\n",
        "        \n",
        "        # 重み、バイアスの更新\n",
        "        self = self.optimizer.update(self)\n",
        "        \n",
        "        return dX\n",
        "\n",
        "    \n",
        "    def _n_out(self, n_in, P, F, S):\n",
        "        '''\n",
        "        出力サイズの計算\n",
        "        \n",
        "        Parameters\n",
        "        --------------\n",
        "        n_in : int\n",
        "            入力のサイズ\n",
        "        P : int\n",
        "            パディング数\n",
        "        F : \n",
        "            フィルター(重み)のサイズ\n",
        "        S : \n",
        "            ストライドのサイズ\n",
        "            \n",
        "        returns\n",
        "        ---------\n",
        "        n_out : int\n",
        "            出力のサイズ\n",
        "        '''\n",
        "        n_out = int(((n_in + 2 * P - F) / S ) + 1)\n",
        "        \n",
        "        return n_out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdP7svp5-9oO"
      },
      "source": [
        "# 平滑化クラス\n",
        "class Flatten():\n",
        "    '''\n",
        "    Attribute\n",
        "    -----------\n",
        "    X : 次の形のndarray, shape (N, C, H ,W)\n",
        "        入力\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        self.X_shape = None\n",
        "    \n",
        "    def forward(self, X):\n",
        "        '''\n",
        "        フォワード\n",
        "\n",
        "        Parameters\n",
        "        -------------\n",
        "        X : 次の形のndarray, shape (N, C, H ,W)\n",
        "            入力\n",
        "            \n",
        "        Returns\n",
        "        ----------\n",
        "        X_1d : 次の形のndarray, shape (N, W)\n",
        "        '''\n",
        "        #1次元にする\n",
        "        X_1d = X.reshape(X.shape[0], -1)\n",
        "        \n",
        "        #shapeを記録\n",
        "        self.X_shape = X.shape\n",
        "        \n",
        "        return X_1d\n",
        "    \n",
        "\n",
        "    def backward(self, X):\n",
        "        '''\n",
        "        バックワード\n",
        "\n",
        "        Parameters\n",
        "        -------------\n",
        "        X : 次の形のndarray, shape (N, W)\n",
        "            入力\n",
        "            \n",
        "        Returns\n",
        "        ----------\n",
        "        X : 次の形のndarray, shape (N, C, H ,W)\n",
        "        '''\n",
        "        #shapeを戻す\n",
        "        X = X.reshape(self.X_shape)\n",
        "        \n",
        "        return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPTM4YV39gO1"
      },
      "source": [
        "# 全結合層クラス\n",
        "class FC:\n",
        "    \"\"\"\n",
        "    ノード数n_nodes1からn_nodes2への全結合層\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_nodes1 : int\n",
        "      前の層のノード数\n",
        "    n_nodes2 : int\n",
        "      後の層のノード数\n",
        "    initializer : 初期化方法のインスタンス\n",
        "    optimizer : 最適化手法のインスタンス\n",
        "    \"\"\"\n",
        "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
        "        self.optimizer = optimizer\n",
        "        # 初期化\n",
        "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
        "        self.W = initializer.W(n_nodes1, n_nodes2)\n",
        "        self.B = initializer.B(n_nodes2)\n",
        "        \n",
        "        #インスタンス変数\n",
        "        self.X = None\n",
        "\n",
        "    \n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        フォワードプロバケーション\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
        "            入力\n",
        "            \n",
        "        Returns\n",
        "        ----------\n",
        "        Z2 : 次の形のndarray, shape (batch_size, n_nodes2)\n",
        "            出力\n",
        "        \"\"\"\n",
        "        self.X = X.copy()\n",
        "        Z2 = np.dot(X, self.W) + self.B\n",
        "        \n",
        "        return Z2\n",
        "    \n",
        "    \n",
        "    def backward(self, dA):\n",
        "        \"\"\"\n",
        "        バックプロバケーション\n",
        "        Parameters\n",
        "        ----------\n",
        "        dA : 次の形のndarray, shape(batch_size, n_nodes2)\n",
        "            後ろから流れてきた勾配\n",
        "            \n",
        "        Returns\n",
        "        ----------\n",
        "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
        "            前に流す勾配\n",
        "        \"\"\"\n",
        "        self.dB = dA \n",
        "        self.dW = np.dot(self.X.T, dA) \n",
        "        dZ = np.dot(dA, self.W.T) \n",
        "        \n",
        "        # 重み、バイアスの更新\n",
        "        self = self.optimizer.update(self)\n",
        "        return dZ"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Jhu_rJd9gU3"
      },
      "source": [
        "# 初期化クラス\n",
        "# SimpleInitializer ------------------------------------------------------------\n",
        "class SimpleInitializer:\n",
        "    \"\"\"\n",
        "    ガウス分布によるシンプルな初期化\n",
        "    Parameters\n",
        "    ----------\n",
        "    sigma : float\n",
        "      ガウス分布の標準偏差\n",
        "    \"\"\"\n",
        "    def __init__(self, sigma):\n",
        "        self.sigma = sigma #重みパラメータ\n",
        "        \n",
        "\n",
        "    def W(self, n_nodes1=None, n_nodes2=None, \n",
        "                  filter_num=None, C=None, filter_size=None):\n",
        "        \"\"\"\n",
        "        重みの初期化\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes1 : int\n",
        "            前の層のノード数\n",
        "        n_nodes2 : int\n",
        "            後の層のノード数\n",
        "        filter_num : int\n",
        "            フィルター数\n",
        "        C : int\n",
        "            チャンネル数\n",
        "        filter_size : int\n",
        "            フィルターのサイズ(縦横同じ)\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        W : 次の形のndarray, shape (n_nodes1, n_nodes2)\n",
        "            重み\n",
        "        \"\"\"\n",
        "        #畳み込み\n",
        "        if filter_num is not None and C is not None and filter_size is not None:\n",
        "            W =  self.sigma * np.random.randn(filter_num, C, filter_size) \n",
        "       \n",
        "        #全結合\n",
        "        if n_nodes1 is not None and n_nodes2 is not None:\n",
        "            W =  self.sigma * np.random.randn(n_nodes1, n_nodes2) \n",
        "\n",
        "        return W\n",
        "    \n",
        "    \n",
        "    def B(self, n_nodes2):\n",
        "        \"\"\"\n",
        "        バイアスの初期化\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes2 : int\n",
        "            後の層のノード数\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        B :次の形のndarray, shape (n_nodes2)\n",
        "            バイアス\n",
        "        \"\"\"\n",
        "        B = np.zeros(n_nodes2)\n",
        "        \n",
        "        return B\n",
        "\n",
        "# XavierInitializer ------------------------------------------------------------\n",
        "class XavierInitializer:\n",
        "    \"\"\"\n",
        "    Xavierの初期値のクラス\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        pass\n",
        "        \n",
        "\n",
        "    def W(self, n_nodes1=None, n_nodes2=None, \n",
        "                  filter_num=None, C=None, filter_size=None):\n",
        "        \"\"\"\n",
        "        重みの初期化\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes1 : int\n",
        "          前の層のノード数\n",
        "        n_nodes2 : int\n",
        "          後の層のノード数\n",
        "        filter_num : int\n",
        "            フィルター数\n",
        "        C : int\n",
        "            チャンネル数\n",
        "        filter_size : int\n",
        "            フィルタサイズ\n",
        "        Returns\n",
        "        ----------\n",
        "        W : 次の形のndarray, shape (n_nodes1, n_nodes2)\n",
        "            重み\n",
        "        \"\"\"\n",
        "        #畳み込み層\n",
        "        if filter_num and C and filter_size is not None:\n",
        "            W =  np.random.randn(filter_num, C, filter_size) / np.sqrt(filter/num) \n",
        "        \n",
        "        #全結合層\n",
        "        else:\n",
        "            W = np.random.randn(n_nodes1, n_nodes2) / np.sqrt(n_nodes1) \n",
        "    \n",
        "        return W\n",
        "    \n",
        "    \n",
        "    def B(self, n_nodes2):\n",
        "        \"\"\"\n",
        "        バイアスの初期化\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes2 : int\n",
        "          後の層のノード数\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        B : 次の形のndarray, shape (n_nodes2, )\n",
        "            バイアス\n",
        "        \"\"\"\n",
        "        B = np.zeros(n_nodes2) \n",
        "        \n",
        "        return B\n",
        "# He-----------------------------------------------------------------------------\n",
        "class HeInitializer:\n",
        "    \"\"\"\n",
        "    Heの初期値のクラス\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        pass\n",
        "        \n",
        "\n",
        "    def W(self, n_nodes1=None, n_nodes2=None, \n",
        "                  filter_num=None, C=None, filter_size=None):\n",
        "        \"\"\"\n",
        "        重みの初期化\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes1 : int\n",
        "          前の層のノード数\n",
        "        n_nodes2 : int\n",
        "          後の層のノード数\n",
        "        filter_num : int\n",
        "            フィルター数\n",
        "        C : int\n",
        "            チャンネル数\n",
        "        filter_size : int\n",
        "            フィルタサイズ\n",
        "        Returns\n",
        "        ----------\n",
        "        W : 次の形のndarray, shape (n_nodes1, n_nodes2)\n",
        "            重み\n",
        "        \"\"\"\n",
        "        #畳み込み層\n",
        "        if filter_num and C and filter_size is not None:\n",
        "            W = np.random.randn(filter_num, C, filter_size) * np.sqrt(2 / filter_num)\n",
        "       \n",
        "        #全結合層\n",
        "        else:\n",
        "            W = np.random.randn(n_nodes1, n_nodes2) * np.sqrt(2 / n_nodes1)\n",
        "    \n",
        "        return W\n",
        "    \n",
        "    \n",
        "    def B(self, n_nodes2):\n",
        "        \"\"\"\n",
        "        バイアスの初期化\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes2 : int\n",
        "            後の層のノード数\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        B : 次の形のndarray, shape (n_nodes2, )\n",
        "            バイアス\n",
        "        \"\"\"\n",
        "        B = np.zeros(n_nodes2) \n",
        "        \n",
        "        return B"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRTtZxoA9ga5"
      },
      "source": [
        "# 最適化クラス\n",
        "# SGD---------------------------------------------\n",
        "class SGD:\n",
        "    \"\"\"\n",
        "    確率的勾配降下法\n",
        "    Parameters\n",
        "    ----------\n",
        "    alpha : 学習率\n",
        "    \"\"\"\n",
        "    def __init__(self, alpha):\n",
        "        self.alpha = alpha\n",
        "        \n",
        "\n",
        "    def update(self, layer):\n",
        "        \"\"\"\n",
        "        ある層の重みやバイアスの更新\n",
        "        Parameters\n",
        "        ----------\n",
        "        layer : 更新前の層のインスタンス\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        layer : 更新後の層のインスタンス\n",
        "        \"\"\"\n",
        "        #バッチサイズで割る\n",
        "        layer.W -= self.alpha* layer.dW / layer.dB.shape[0] #(n_nodes1, n_nodes2)\n",
        "        layer.B -= self.alpha* layer.dB.mean(axis=0) #(n_nodes2)\n",
        "        \n",
        "        return layer\n",
        "# AdaGrad----------------------------------------\n",
        "class AdaGrad:\n",
        "    \"\"\"\n",
        "    AdaGradの最適化\n",
        "    Parameters\n",
        "    ----------\n",
        "    alpha : 学習率\n",
        "    \"\"\"\n",
        "    def __init__(self, alpha):\n",
        "        self.alpha = alpha\n",
        "        self.H_W= None\n",
        "        self.H_B = None\n",
        "        \n",
        "\n",
        "    def update(self, layer):\n",
        "        \"\"\"\n",
        "        ある層の重みやバイアスの更新\n",
        "        Parameters\n",
        "        ----------\n",
        "        layer : 更新前の層のインスタンス\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        layer : 更新後の層のインスタンス\n",
        "        \"\"\"\n",
        "        #self.Hの初期化\n",
        "        if self.H_W is None:\n",
        "            self.H_W = np.zeros(layer.W.shape)\n",
        "        if self.H_B is None:\n",
        "            self.H_B = np.zeros(layer.B.shape)\n",
        "        \n",
        "        #更新    \n",
        "        self.H_W += (layer.dW / layer.dB.shape[0]) ** 2 #重みの勾配の二乗和\n",
        "        self.H_B += (layer.dB.mean(axis=0)) ** 2 #バイアスの二乗和\n",
        "        layer.W -= self.alpha / np.sqrt(self.H_W + 1e-7) * layer.dW / layer.dB.shape[0] #重み\n",
        "        layer.B -= self.alpha / np.sqrt(self.H_B + 1e-7) * layer.dB.mean(axis=0) #バイアス \n",
        "        \n",
        "        return layer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAndz8vR9gYS"
      },
      "source": [
        "# 活性化関数クラス\n",
        "# ソフトマックス関数のクラス----------------------------------------------\n",
        "class Softmax:\n",
        "    '''\n",
        "    ソフトマックス関数のクラス\n",
        "    Parameters\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        self.Z = None\n",
        "    \n",
        "    def forward(self, A):\n",
        "        '''\n",
        "        フォワードプロバケーション\n",
        "        Parameters\n",
        "        --------------\n",
        "        A : 次の形のndarray, shape (batch_size, n_output)\n",
        "            特徴量ベクトルと重みとバイアスを計算したもの\n",
        "        \n",
        "        Returns\n",
        "        ---------\n",
        "        Z : 次の形のndarray, shape (batch_size, n_output)\n",
        "            ソフトマックス関数の計算結果\n",
        "        '''\n",
        "        #オーバーフロー対策\n",
        "        c = np.max(A)\n",
        "        exp_A = np.exp(A - c)\n",
        "        \n",
        "        #分母\n",
        "        sum_exp_A = np.sum(exp_A, axis=1).reshape(-1, 1)\n",
        "\n",
        "        self.Z = exp_A / sum_exp_A\n",
        "        \n",
        "        return self.Z\n",
        "\n",
        "    \n",
        "    def backward(self, y):\n",
        "        '''\n",
        "        バックプロバケーション\n",
        "        Parameters\n",
        "        --------------\n",
        "        y : 次の形のndarray, shape (batch_size, n_output)\n",
        "            正解ラベルのベクトル\n",
        "        Z : 次の形のndarray, shape (batch_size, n_output)\n",
        "            フォワードプロバケーションの出力\n",
        "        \n",
        "        Returns\n",
        "        ---------\n",
        "        dA : 次の形のndarray, shape (batch_size, n_nodes1)  \n",
        "           Aに関する損失Lの勾配 \n",
        "        '''\n",
        "        #交差エントロピー誤差の計算\n",
        "        loss_sum = np.sum(y * np.log(self.Z), axis=1)\n",
        "        loss = -np.mean(loss_sum)\n",
        "        \n",
        "        #勾配の計算\n",
        "        dA = self.Z - y\n",
        "        \n",
        "        return dA, loss\n",
        "\n",
        "# ReLU関数のクラス----------------------------------------------\n",
        "class Relu:\n",
        "    '''\n",
        "    ReLU関数のクラス\n",
        "    Parameters\n",
        "    \n",
        "    '''\n",
        "    def __init__(self):\n",
        "        self.X = None\n",
        "\n",
        "    \n",
        "    def forward(self, A):\n",
        "        '''\n",
        "        フォワードプロバケーション\n",
        "        Parameters\n",
        "        --------------\n",
        "        A : 次の形のndarray, shape (batch_size, n_nodes1)\n",
        "            特徴量ベクトルと重みとバイアスを計算したもの\n",
        "        \n",
        "        Returns\n",
        "        ---------\n",
        "        Z : 次の形のndarray, shape (batch_size, n_nodes1)\n",
        "            活性化関数を計算したもの\n",
        "        '''\n",
        "        self.X = A.copy()\n",
        "        \n",
        "        #Aが0以下なら0にする\n",
        "        Z = np.maximum(0, A)\n",
        "        \n",
        "        return Z\n",
        "    \n",
        "    \n",
        "    def backward(self, dZ):\n",
        "        '''\n",
        "        バックプロバケーション\n",
        "        Parameters\n",
        "        --------------\n",
        "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
        "            Zに関する損失Lの勾配\n",
        "        \n",
        "        Returns\n",
        "        ---------\n",
        "        dA : 次の形のndarray, shape (batch_size, n_nodes1)  \n",
        "           Aに関する損失Lの勾配 \n",
        "        '''\n",
        "        #forwardで0以下の部分を0にする\n",
        "        dA = np.where(self.X > 0, dZ, 0)\n",
        "\n",
        "        return dA\n",
        "# tanh関数のクラス----------------------------------------------\n",
        "class Tanh:\n",
        "    '''\n",
        "    ハイパポリックタンジェント関数のクラス\n",
        "    Parameters\n",
        "    \n",
        "    '''\n",
        "    def __init__(self):\n",
        "        self.Z = None\n",
        "\n",
        "    \n",
        "    def forward(self, A):\n",
        "        '''\n",
        "        フォワードプロバケーション\n",
        "        Parameters\n",
        "        --------------\n",
        "        A : 次の形のndarray, shape (batch_size, n_nodes1)\n",
        "            特徴量ベクトルと重みとバイアスを計算したもの\n",
        "        \n",
        "        Returns\n",
        "        ---------\n",
        "        Z : 次の形のndarray, shape (batch_size, n_nodes1)\n",
        "            活性化関数を計算したもの\n",
        "        '''\n",
        "        self.Z =  np.tanh(A)\n",
        "        \n",
        "        return  self.Z\n",
        "    \n",
        "    \n",
        "    def backward(self, dZ):\n",
        "        '''\n",
        "        バックプロバケーション\n",
        "        Parameters\n",
        "        --------------\n",
        "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
        "            Zに関する損失Lの勾配\n",
        "        \n",
        "        Returns\n",
        "        ---------\n",
        "        dA : 次の形のndarray, shape (batch_size, n_nodes1)  \n",
        "           Aに関する損失Lの勾配 \n",
        "        '''\n",
        "        dA = dZ * (1 - self.Z**2)\n",
        "        \n",
        "        return dA\n",
        "# sigmoid関数のクラス----------------------------------------------\n",
        "class Sigmoid:\n",
        "    '''\n",
        "    シグモイド関数のクラス\n",
        "    Parameters\n",
        "    \n",
        "    '''\n",
        "    def __init__(self):\n",
        "        self.Z = None\n",
        "    \n",
        "\n",
        "    def forward(self, A):\n",
        "        '''\n",
        "        フォワードプロバケーション\n",
        "        Parameters\n",
        "        --------------\n",
        "        A : 次の形のndarray, shape (batch_size, n_nodes1)\n",
        "            特徴量ベクトルと重みとバイアスを計算したもの\n",
        "        \n",
        "        Returns\n",
        "        ---------\n",
        "        Z : 次の形のndarray, shape (batch_size, n_nodes1)\n",
        "            活性化関数を計算したもの\n",
        "        '''\n",
        "        self.Z = 1 / (1 + np.exp(-A)) \n",
        "        \n",
        "        return self.Z\n",
        "    \n",
        "    \n",
        "    def backward(self, dZ):\n",
        "        '''\n",
        "        バックプロバケーション\n",
        "        Parameters\n",
        "        --------------\n",
        "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
        "            Zに関する損失Lの勾配\n",
        "        \n",
        "        Returns\n",
        "        ---------\n",
        "        dA : 次の形のndarray, shape (batch_size, n_nodes1)  \n",
        "           Aに関する損失Lの勾配 \n",
        "        '''\n",
        "        dA = dZ * (1 - self.Z) * self.Z\n",
        "        \n",
        "        return dA"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpkGeyFwUtvs"
      },
      "source": [
        "### 8.3.1（解答）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FT4KLRDKqhnZ"
      },
      "source": [
        "CNN = ScratchDeepNeuralNetworkClassifier()\n",
        "CNN.fit(x_train[:1000], y_train[:1000], x_val[:200], y_val[:200])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0tXW9Mx8YRI"
      },
      "source": [
        "y_pred = CNN.predict(X_test)\n",
        "y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYcSPRLvEJdy"
      },
      "source": [
        "Y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Lk8jgEtM3XB"
      },
      "source": [
        "def cost_curve(loss, val_loss):\n",
        "    '''\n",
        "    学習曲線を出力する関数\n",
        "    '''\n",
        "    p1= plt.plot(loss, label='loss')\n",
        "    p2 = plt.plot(val_loss, label='val_loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Cost')\n",
        "    plt.title('Cost curve')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NopFeVIVGBY9"
      },
      "source": [
        "cost_curve(CNN.loss_list, CNN.val_loss_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q26PeUZl7Fqp"
      },
      "source": [
        "def evalution(y_test, y_pred):\n",
        "    '''\n",
        "     分類問題の指標値を出力する関数\n",
        " \n",
        "     Paraeters\n",
        "     -------------\n",
        "    y_test : 次の形のndarray, shape (n_samples, )\n",
        "        正解値\n",
        "    y_pred : 次の形のndarray, shape (n_samples, )\n",
        "        予測したラベル\n",
        "    '''\n",
        "    #accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print('accuracy :', accuracy)\n",
        "\n",
        "    #precision\n",
        "    precision = precision_score(y_test, y_pred, average='macro')\n",
        "    print('precision :', precision)\n",
        "\n",
        "    #recall\n",
        "    recall = recall_score(y_test, y_pred, average='macro')\n",
        "    print('recall :', recall)\n",
        "\n",
        "    #f1\n",
        "    f1 = f1_score(y_test, y_pred, average='macro')\n",
        "    print('f1 :', f1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSgTy2EE_28w"
      },
      "source": [
        "evalution(Y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}