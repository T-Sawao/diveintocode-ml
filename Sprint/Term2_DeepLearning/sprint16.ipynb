{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "term2_sprint16.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPY5NohXV5ZVmKjUV8VU0iw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/T-Sawao/diveintocode-ml3/blob/main/term2_sprint16.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cB1flJPOOCt"
      },
      "source": [
        "# term2_sprint16 論文紹介"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gySrI7OMwr1z"
      },
      "source": [
        "# 1.このSprintについて\n",
        "\n",
        "Sprintの目的\n",
        "関心分野の研究動向を掴めるようにする\n",
        "論文の要旨を紹介できるようにする\n",
        "\n",
        "どのように学ぶか\n",
        "論文を短期間で読み資料を作成します"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85dgGM8OxTLZ"
      },
      "source": [
        "# 2.論文紹介\n",
        "\n",
        "機械学習分野のある分野に関して短期間で研究動向を把握することを目指します。\n",
        "\n",
        "\n",
        "自分自身が興味のある分野に対し、論文5本の要旨をまとめたスライドを用意し、5分程度で紹介してください。\n",
        "\n",
        "**課題提出方法**   \n",
        "スライドをSpeaker DeckやSlideshareなどに投稿し、そのURLを提出してください。スライドは外部に公開することを意識して作成してください"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBziaFQC-Be-"
      },
      "source": [
        "### 1.1.1（解答）\n",
        "要件\n",
        "- 聞いた人がその分野について何らかの傾向をつかめる発表にする。  \n",
        "⇒画像認識の分野でも注目され始めたtransformerを元に、自然言語処理のtransfomer以降の分岐となる論文を調査する。\n",
        "\n",
        "- 論文の選定理由を答えられるようにする。  \n",
        "⇒transformerという技術への興味と、興味を持っている自然言語処理の最新技術に迫りたい。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GM_y5i46_FHs"
      },
      "source": [
        "### 1.2.1（解答）論文"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHiy6zs0xyHa"
      },
      "source": [
        "**論文タイトル1**  \n",
        "# an_image_is_worth_16x16_words_transformers_for_image_recognition_at_scale\n",
        "\n",
        "URL  \n",
        "https://arxiv.org/pdf/2010.11929.pdf\n",
        "\n",
        "**著者**  \n",
        "匿名\n",
        "\n",
        "**提出年**  \n",
        "2020年10月\n",
        "\n",
        "**内容（要するに？？(何をどうやって検証したのかを短く。)**  \n",
        "Transformerは自然言語処理でよく利用されている。画像認識では１部分的な技術だけが使われており、純粋にTransformerを利用した技術はなく、未だにCNNが一線で活躍している。大量の事前学習を行える状況下では、Transformerを利用することで、CNNよりも優れた変わる畳み込みネットワークを構築することが出来る。\n",
        "\n",
        "**有効性の検証（何が新規性として強調されている？？）**  \n",
        "人気のある画像分類技術としてResNet、Noisy StudentとJFT-3000Mデータセットを事前学習したVision Transformer(ViT)で比較。SataモデルのResNet、Noisy Studentの精度を更新しただけでなく、学習時間が4分1以上短縮された。\n",
        "\n",
        "**技術の手法（技術・手法・アイデアなどで、すごいと思った点。）**  \n",
        "ImageNet-21k(クラス21,000個の計1,400万枚)を事前学習に使用したケースにおいて、精度は若干落ちるが学習にかかる時間が約１０分の１に短縮された点。\n",
        "\n",
        "**議論の有無（この論文の限界として感じたことがあれば。）**  \n",
        "画像データが1億個時点で、ResNetの精度を超える。\n",
        "3000万時点でのパフォーマンスはResNetよりも20%近く劣る。  \n",
        "\n",
        "**先行研究との比較（Discussionで興味深い仮説や解釈があれば。）**  \n",
        "すべての比較でResNetを上回り、事前学習に必要な計算時間も大幅に軽減された。\n",
        "\n",
        "**次に読むべき論文（この論文を読んでさらに知りたいと思ったこと。）**  \n",
        "- 比較対象にある最新の手法の一つであるNois Student\n",
        "- Attention Is All You Need  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTjIxe3C6rd8"
      },
      "source": [
        "**論文タイトル2**  \n",
        "#  Attention Is All You Need\n",
        "\n",
        "URL  \n",
        "https://arxiv.org/abs/1706.03762  \n",
        "\n",
        "**著者**  \n",
        "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,lukaszkaiser\n",
        "\n",
        "**提出年**  \n",
        "2017年\n",
        "\n",
        "**内容（要するに？？(何をどうやって検証したのかを短く。)**  \n",
        "Atentionメカニズムを導入することでRNNやCNNを排除したネットワーク。  \n",
        "\n",
        "\n",
        "**有効性の検証（何が新規性として強調されている？？）**  \n",
        "シンプルかつ並列化可能なため他のタスクに置いても活用できる  \n",
        "\n",
        "\n",
        "**技術の手法（技術・手法・アイデアなどで、すごいと思った点。）**  \n",
        "\n",
        "Attentionメカニズムを搭載したエンコーダとデコーダー構造  \n",
        "\n",
        "**議論の有無（この論文の限界として感じたことがあれば。）**  \n",
        "出力は大幅な構造的な影響を受けてながくなってしまう、また小さな学習ではRNNモデルよりは劣っていた。\n",
        "\n",
        "\n",
        "**先行研究との比較（Discussionで興味深い仮説や解釈があれば。）**  \n",
        "アンサンブル学習も含めた最先端モデルと比較してトレーニングコストが優れていた。\n",
        "\n",
        "**次に読むべき論文（この論文を読んでさらに知りたいと思ったこと。）**  \n",
        "\n",
        "BEAT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEj4Fa54yZHu"
      },
      "source": [
        "**論文タイトル3**  \n",
        "# BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n",
        "\n",
        "\n",
        "URL  \n",
        "https://arxiv.org/abs/1810.04805\n",
        "\n",
        "**著者**  \n",
        "Jacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova Google AI Language\n",
        "\n",
        "**提出年**  \n",
        "2019年\n",
        "\n",
        "**内容（要するに？？(何をどうやって検証したのかを短く。)**  \n",
        "すべてのレイヤーで左右両方のコンテキストを共同で調整することにより、ラベルのないテキストから深い双方向表現を事前トレーニングするように設計することで、事前にトレーニングされたBERTモデルを1つの追加出力レイヤーで微調整して、質問応答や言語推論などの幅広いタスク用の最先端のモデルを実質的に作成することなく作成できる。\n",
        "\n",
        "**有効性の検証（何が新規性として強調されている？？）**  \n",
        "豊富な事前学習は深い単方向アーキテクチャの恩恵を受けていたが、双方向アーキテクチャとなることで幅広いNLPタスクに取り組むことが出来る様になった。\n",
        "\n",
        "**技術の手法（技術・手法・アイデアなどで、すごいと思った点。）**  \n",
        "事前トレーニングと微調整という２つのステップがあり、事前トレーニングでは教師なし学習を行い、事前トレーニングで使用したパラメーターで初期化され、全てのパラメーターは事前学習から正解ラベル付きデータを使用して微調整する。\n",
        "\n",
        "**議論の有無（この論文の限界として感じたことがあれば。）**  \n",
        "特になし\n",
        "\n",
        "**先行研究との比較（Discussionで興味深い仮説や解釈があれば。）**  \n",
        "GLUEテスト、SQuAD1.1、SQuAD2.0、SWAGの開発とテストの精度、異常４つのテストで検証をおこなった。全ての結果で優秀な評価を得た。\n",
        "\n",
        "**次に読むべき論文（この論文を読んでさらに知りたいと思ったこと。）**  \n",
        "GTP3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3YByOSJ6rca"
      },
      "source": [
        "**論文タイトル4**  \n",
        "#  Language Models are Few-Shot Learners\n",
        "\n",
        "URL  \n",
        "https://arxiv.org/abs/2005.14165  \n",
        "\n",
        "\n",
        "**著者**  \n",
        "Tom B. Brown∗ Benjamin Mann∗ Nick Ryder∗ Melanie Subbiah∗\n",
        "Jared Kaplan† Prafulla Dhariwal Arvind Neelakantan Pranav Shyam Girish Sastry\n",
        "Amanda Askell Sandhini Agarwal Ariel Herbert-Voss Gretchen Krueger Tom Henighan\n",
        "Rewon Child Aditya Ramesh Daniel M. Ziegler Jeffrey Wu Clemens Winter\n",
        "Christopher Hesse Mark Chen Eric Sigler Mateusz Litwin Scott Gray\n",
        "Benjamin Chess Jack Clark Christopher Berner\n",
        "Sam McCandlish Alec Radford Ilya Sutskever Dario Amodei\n",
        "OpenAI\n",
        "\n",
        "**提出年**  \n",
        "2020年\n",
        "\n",
        "**内容（要するに？？(何をどうやって検証したのかを短く。)**  \n",
        "テキストの大規模なコーパスで事前学習することで、Few shotの学習効率が飛躍的に向上する事を発見した。具体的には1750億のパラメーターを持つGPT3は人間によって書かれた記事と区別が難しい記事を生成出来ることがわかった。  \n",
        "\n",
        "\n",
        "**有効性の検証（何が新規性として強調されている？？）**  \n",
        "以前のモデルの10倍以上のパラメーターを持ち、正確さにおいて35ポイントも精度を高めている。\n",
        "\n",
        "\n",
        "**技術の手法（技術・手法・アイデアなどで、すごいと思った点。）**  \n",
        "微調整すること無く、ゼロショット、ワンショット、および少数ショットの設定で、多くのNLPタスクとベンチマークで強力なパフォーマンスを示し、最先端の調整されたシステムに匹敵する。\n",
        "\n",
        "\n",
        "**議論の有無（この論文の限界として感じたことがあれば。）**  \n",
        "テキスト合成といくつかのNLPタスクに顕著な弱点があります。また表現や公平性などの点で様々な人々に害を及ぼす可能性がある。\n",
        "\n",
        "**先行研究との比較（Discussionで興味深い仮説や解釈があれば。）**  \n",
        "LAMBADAデータセットを利用し、SOTAモデルをzero shot学習で8%上回り、Few shotでは18%向上した。\n",
        "\n",
        "**次に読むべき論文（この論文を読んでさらに知りたいと思ったこと。）**  \n",
        "Scakubg Laws for Autoregressive Fenerative Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2JRReup6rLz"
      },
      "source": [
        "**論文タイトル5**  \n",
        "# Scaling Laws for Autoregressive Generative Modeling\n",
        "\n",
        "\n",
        "URL  \n",
        "https://arxiv.org/abs/2010.14701\n",
        "\n",
        "**著者**  \n",
        "Tom Henighan、Jared Kaplan、Mor Katz、Mark Chen、Christopher Hesse、Jacob Jackson、Heewoo Jun、Tom B. Brown、Prafulla Dhariwal、Scott Grey、Chris Hallacy、Benjamin Mann、Alec Radford、Aditya Ramesh、Nick Ryder、DanielM 。 Ziegler、John Schulman、Dario Amodei、Sam McCandlish\n",
        "\n",
        "**提出年**  \n",
        "2020年10月\n",
        "\n",
        "**内容（要するに？？(何をどうやって検証したのかを短く。)）**  \n",
        "生成画像モデリング、ビデオモデリング、マルチモーダル画像↔テキストモデル、および数学的問題解決の4つのドメインにおけるクロスエントロピー損失の経験的スケーリング法則は、事前学習済みニューラルネットワークのパフォーマンスに重要な影響を与え強化する。\n",
        "\n",
        "**有効性の検証（何が新規性として強調されている？？）**  \n",
        "長いトレーニング時間よりもモデルが大きいほど学習効率が高くなることがわかり、財務における業務でも同様のスケーリング法則が当てはまる。またそのことは一次関数的な普遍的な傾向を示した。\n",
        "\n",
        "**技術の手法（技術・手法・アイデアなどで、すごいと思った点。）**  \n",
        "エントロピーの識別は、正確な傾向の外挿によって可能になり、単一のモデルからの結果を使用して予測することはできないことがわかり、マルチモーダルモデルの画像とキャプション間の経験的な相互情報量に関する興味深いスケーリング則が観測されることがわかった。\n",
        "\n",
        "**議論の有無（この論文の限界として感じたことがあれば。）**  \n",
        "エントロピーの識別は、正確な傾向の外挿によって可能になるので、単一モデルからの結果を使用して予測することはできないこと。\n",
        "\n",
        "\n",
        "**先行研究との比較（Discussionで興味深い仮説や解釈があれば。）**  \n",
        "1画像(32x32)あたりの価値は約2~3語の価値しか無い事がわかった。\n",
        "\n",
        "**次に読むべき論文（この論文を読んでさらに知りたいと思ったこと。）**  \n",
        "- GTP３に関連する論文  \n",
        "- OpenAIの著者が出している論文  "
      ]
    }
  ]
}