{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sprint10 深層学習スクラッチ ディープニューラルネットワーク.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOhI4vpaiy30KtYAlbeGcqy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/T-Sawao/diveintocode-ml/blob/master/term2_sprint10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HczNh-9AOdE"
      },
      "source": [
        "## 2.ディープニューラルネットワークスクラッチ\n",
        "\n",
        "前回は3層のニューラルネットワークを作成しましたが、今回はこれを任意の層数に拡張しやすいものに書き換えていきます。その上で、活性化関数や初期値、最適化手法について発展的なものを扱えるようにしていきます。\n",
        "\n",
        "\n",
        "このようなスクラッチを行うことで、今後各種フレームワークを利用していくにあたり、内部の動きが想像できることを目指します。\n",
        "\n",
        "\n",
        "名前は新しくScratchDeepNeuralNetworkClassifierクラスとしてください。\n",
        "\n",
        "\n",
        "**層などのクラス化**  \n",
        "クラスにまとめて行くことで、構成を変更しやすい実装にしていきます。\n",
        "\n",
        "\n",
        "**手を加える箇所**\n",
        "\n",
        "\n",
        "- 層の数\n",
        "- 層の種類（今後畳み込み層など他のタイプの層が登場する）\n",
        "- 活性化関数の種類\n",
        "- 重みやバイアスの初期化方法\n",
        "- 最適化手法\n",
        "\n",
        "そのために、全結合層、各種活性化関数、重みやバイアスの初期化、最適化手法それぞれのクラスを作成します。\n",
        "\n",
        "\n",
        "実装方法は自由ですが、簡単な例を紹介します。サンプルコード1のように全結合層と活性化関数のインスタンスを作成し、サンプルコード2,3のようにして使用します。それぞれのクラスについてはこのあと解説します。\n",
        "\n",
        "\n",
        "《サンプルコード1》\n",
        "\n",
        "\n",
        "ScratchDeepNeuralNetworkClassifierのfitメソッド内"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OadFEqBkAjDE"
      },
      "source": [
        "# self.sigma : ガウス分布の標準偏差\n",
        "# self.lr : 学習率\n",
        "# self.n_nodes1 : 1層目のノード数\n",
        "# self.n_nodes2 : 2層目のノード数\n",
        "# self.n_output : 出力層のノード数\n",
        "# optimizer = SGD(self.lr)\n",
        "# self.FC1 = FC(self.n_features, self.n_nodes1, SimpleInitializer(self.sigma), optimizer)\n",
        "# self.activation1 = Tanh()\n",
        "# self.FC2 = FC(self.n_nodes1, self.n_nodes2, SimpleInitializer(self.sigma), optimizer)\n",
        "# self.activation2 = Tanh()\n",
        "# self.FC3 = FC(self.n_nodes2, self.n_output, SimpleInitializer(self.sigma), optimizer)\n",
        "# self.activation3 = Softmax()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQSyfMCtAioC"
      },
      "source": [
        "《サンプルコード2》\n",
        "\n",
        "\n",
        "イテレーションごとのフォワード"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnpfjFK8Ah8F"
      },
      "source": [
        "# A1 = self.FC1.forward(X)\n",
        "# Z1 = self.activation1.forward(A1)\n",
        "# A2 = self.FC2.forward(Z1)\n",
        "# Z2 = self.activation2.forward(A2)\n",
        "# A3 = self.FC3.forward(Z2)\n",
        "# Z3 = self.activation3.forward(A3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uryRtUs7Ahko"
      },
      "source": [
        "《サンプルコード3》\n",
        "\n",
        "\n",
        "イテレーションごとのバックワード\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmr_XTeQAg12"
      },
      "source": [
        "# dA3 = self.activation3.backward(Z3, Y) # 交差エントロピー誤差とソフトマックスを合わせている\n",
        "# dZ2 = self.FC3.backward(dA3)\n",
        "# dA2 = self.activation2.backward(dZ2)\n",
        "# dZ1 = self.FC2.backward(dA2)\n",
        "# dA1 = self.activation1.backward(dZ1)\n",
        "# dZ0 = self.FC1.backward(dA1) # dZ0は使用しない"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mxT5u6vBAF4"
      },
      "source": [
        "## 【問題1】全結合層のクラス化\n",
        "全結合層のクラス化を行なってください。\n",
        "\n",
        "\n",
        "以下に雛形を載せました。コンストラクタで重みやバイアスの初期化をして、あとはフォワードとバックワードのメソッドを用意します。重みW、バイアスB、およびフォワード時の入力Xをインスタンス変数として保持しておくことで、煩雑な入出力は不要になります。\n",
        "\n",
        "\n",
        "なお、インスタンスも引数として渡すことができます。そのため、初期化方法のインスタンスinitializerをコンストラクタで受け取れば、それにより初期化が行われます。渡すインスタンスを変えれば、初期化方法が変えられます。\n",
        "\n",
        "\n",
        "また、引数として自身のインスタンスselfを渡すこともできます。これを利用してself.optimizer.update(self)という風に層の重みの更新が可能です。更新に必要な値は複数ありますが、全て全結合層が持つインスタンス変数にすることができます。\n",
        "\n",
        "\n",
        "初期化方法と最適化手法のクラスについては後述します。\n",
        "\n",
        "\n",
        "《雛形》"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYb1IvpDA_Lx"
      },
      "source": [
        "class FC:\n",
        "    \"\"\"\n",
        "    ノード数n_nodes1からn_nodes2への全結合層\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_nodes1 : int\n",
        "      前の層のノード数\n",
        "    n_nodes2 : int\n",
        "      後の層のノード数\n",
        "    initializer : 初期化方法のインスタンス\n",
        "    optimizer : 最適化手法のインスタンス\n",
        "    \"\"\"\n",
        "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
        "        self.optimizer = optimizer\n",
        "        # 初期化\n",
        "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
        "        pass\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        フォワード\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
        "            入力\n",
        "        Returns\n",
        "        ----------\n",
        "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
        "            出力\n",
        "        \"\"\"        \n",
        "        pass\n",
        "        return A\n",
        "    def backward(self, dA):\n",
        "        \"\"\"\n",
        "        バックワード\n",
        "        Parameters\n",
        "        ----------\n",
        "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
        "            後ろから流れてきた勾配\n",
        "        Returns\n",
        "        ----------\n",
        "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
        "            前に流す勾配\n",
        "        \"\"\"\n",
        "        pass\n",
        "        # 更新\n",
        "        self = self.optimizer.update(self)\n",
        "        return dZ"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeTzsGKEA-7Z"
      },
      "source": [
        "## 【問題2】初期化方法のクラス化\n",
        "初期化を行うコードをクラス化してください。\n",
        "\n",
        "\n",
        "前述のように、全結合層のコンストラクタに初期化方法のインスタンスを渡せるようにします。以下の雛形に必要なコードを書き加えていってください。標準偏差の値（sigma）はコンストラクタで受け取るようにすることで、全結合層のクラス内にこの値（sigma）を渡さなくてすむようになります。\n",
        "\n",
        "\n",
        "これまで扱ってきた初期化方法はSimpleInitializerクラスと名付けることにします。\n",
        "\n",
        "\n",
        "《雛形》"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZixPcuwA-p1"
      },
      "source": [
        "class SimpleInitializer:\n",
        "    \"\"\"\n",
        "    ガウス分布によるシンプルな初期化\n",
        "    Parameters\n",
        "    ----------\n",
        "    sigma : float\n",
        "      ガウス分布の標準偏差\n",
        "    \"\"\"\n",
        "    def __init__(self, sigma):\n",
        "        self.sigma = sigma\n",
        "    def W(self, n_nodes1, n_nodes2):\n",
        "        \"\"\"\n",
        "        重みの初期化\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes1 : int\n",
        "          前の層のノード数\n",
        "        n_nodes2 : int\n",
        "          後の層のノード数\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        W :\n",
        "        \"\"\"\n",
        "        pass\n",
        "        return W\n",
        "    def B(self, n_nodes2):\n",
        "        \"\"\"\n",
        "        バイアスの初期化\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes2 : int\n",
        "          後の層のノード数\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        B :\n",
        "        \"\"\"\n",
        "        pass\n",
        "        return B"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIxhV8U3A-aN"
      },
      "source": [
        "## 【問題3】最適化手法のクラス化\n",
        "最適化手法のクラス化を行なってください。\n",
        "\n",
        "\n",
        "最適化手法に関しても初期化方法同様に全結合層にインスタンスとして渡します。バックワードのときにself.optimizer.update(self)のように更新できるようにします。以下の雛形に必要なコードを書き加えていってください。\n",
        "\n",
        "\n",
        "これまで扱ってきた最適化手法はSGDクラス（Stochastic Gradient Descent、確率的勾配降下法）として作成します。\n",
        "\n",
        "\n",
        "雛形\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qV2RaCiA-Ju"
      },
      "source": [
        "class SGD:\n",
        "    \"\"\"\n",
        "    確率的勾配降下法\n",
        "    Parameters\n",
        "    ----------\n",
        "    lr : 学習率\n",
        "    \"\"\"\n",
        "    def __init__(self, lr):\n",
        "        self.lr = lr\n",
        "    def update(self, layer):\n",
        "        \"\"\"\n",
        "        ある層の重みやバイアスの更新\n",
        "        Parameters\n",
        "        ----------\n",
        "        layer : 更新前の層のインスタンス\n",
        "        \"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsldMEDAA9Q0"
      },
      "source": [
        "## 【問題4】活性化関数のクラス化\n",
        "活性化関数のクラス化を行なってください。\n",
        "\n",
        "\n",
        "ソフトマックス関数のバックプロパゲーションには交差エントロピー誤差の計算も含む実装を行うことで計算が簡略化されます。\n",
        "\n",
        "\n",
        "発展的要素\n",
        "活性化関数や重みの初期値、最適化手法に関してこれまで見てきた以外のものを実装していきます。\n",
        "\n",
        "\n",
        "【問題5】ReLUクラスの作成\n",
        "現在一般的に使われている活性化関数であるReLU（Rectified Linear Unit）をReLUクラスとして実装してください。\n",
        "\n",
        "\n",
        "ReLUは以下の数式です。\n",
        "\n",
        "$$f(x) = ReLU(x) = \\begin{cases}\n",
        "x  & \\text{if $x>0$,}\\\\\n",
        "0 & \\text{if $x\\leqq0$.}\n",
        "\\end{cases}$$\n",
        "\n",
        "x\n",
        " : ある特徴量。スカラー\n",
        "\n",
        "\n",
        "実装上はnp.maximumを使い配列に対してまとめて計算が可能です。\n",
        "\n",
        "\n",
        "numpy.maximum — NumPy v1.15 Manual  \n",
        "https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.maximum.html\n",
        "\n",
        "\n",
        "\n",
        "一方、バックプロパゲーションのための \n",
        "x\n",
        " に関する \n",
        "f\n",
        "(\n",
        "x\n",
        ")\n",
        " の微分は以下のようになります。\n",
        "\n",
        " $$\\frac{\\partial f(x)}{\\partial x} = \\begin{cases}\n",
        "1  & \\text{if $x>0$,}\\\\\n",
        "0 & \\text{if $x\\leqq0$.}\n",
        "\\end{cases}$$\n",
        "\n",
        "数学的には微分可能ではないですが、 \n",
        "x\n",
        "=\n",
        "0\n",
        " のとき \n",
        "0\n",
        " とすることで対応しています。\n",
        "\n",
        "\n",
        "フォワード時の \n",
        "x\n",
        " の正負により、勾配を逆伝播するかどうかが決まるということになります。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXr2NP-uA87q"
      },
      "source": [
        "## 【問題6】重みの初期値\n",
        "ここまでは重みやバイアスの初期値は単純にガウス分布で、標準偏差をハイパーパラメータとして扱ってきました。しかし、どのような値にすると良いかが知られています。シグモイド関数やハイパボリックタンジェント関数のときは Xavierの初期値 （またはGlorotの初期値）、ReLUのときは Heの初期値 が使われます。\n",
        "\n",
        "\n",
        "XavierInitializerクラスと、HeInitializerクラスを作成してください。\n",
        "\n",
        "\n",
        "**Xavierの初期値**  \n",
        "Xavierの初期値における標準偏差 \n",
        "σ\n",
        " は次の式で求められます。\n",
        " $$\\sigma = \\frac{1}{\\sqrt{n}}$$\n",
        "\n",
        " n\n",
        "  : 前の層のノード数\n",
        "\n",
        "\n",
        "《論文》\n",
        "\n",
        "\n",
        "Glorot, X., & Bengio, Y. (n.d.). Understanding the difficulty of training deep feedforward neural networks.  \n",
        "http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf\n",
        "\n",
        "\n",
        "Heの初期値\n",
        "Heの初期値における標準偏差 \n",
        "σ\n",
        " は次の式で求められます。\n",
        " $$\\sigma = \\sqrt{\\frac{2}{n}}$$\n",
        " \n",
        " n\n",
        " : 前の層のノード数\n",
        "\n",
        "\n",
        "《論文》\n",
        "\n",
        "\n",
        "He, K., Zhang, X., Ren, S., & Sun, J. (2015). Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification.  \n",
        " https://arxiv.org/pdf/1502.01852.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ConYpCUuCaz6"
      },
      "source": [
        "## 【問題7】最適化手法\n",
        "学習率は学習過程で変化させていく方法が一般的です。基本的な手法である AdaGrad のクラスを作成してください。\n",
        "\n",
        "\n",
        "まず、これまで使ってきたSGDを確認します。\n",
        "\n",
        "$$W_i^{\\prime} = W_i - \\alpha E(\\frac{\\partial L}{\\partial W_i}) \\\\\n",
        "B_i^{\\prime} = B_i - \\alpha E(\\frac{\\partial L}{\\partial B_i})$$\n",
        "\n",
        "$\\alpha$ : 学習率（層ごとに変えることも可能だが、基本的には全て同じとする）\n",
        "\n",
        "$\\frac{\\partial L}{\\partial W_i}$ : $W_i$ に関する損失 $L$ の勾配\n",
        "\n",
        "$\\frac{\\partial L}{\\partial B_i}$ : $B_i$ に関する損失 $L$ の勾配\n",
        "\n",
        "$E()$ : ミニバッチ方向にベクトルの平均を計算\n",
        "\n",
        "続いて、AdaGradです。バイアスの数式は省略しますが、重みと同様のことをします。\n",
        "\n",
        "更新された分だけその重みに対する学習率を徐々に下げていきます。イテレーションごとの勾配の二乗和 $H$ を保存しておき、その分だけ学習率を小さくします。\n",
        "\n",
        "学習率は重み一つひとつに対して異なることになります。\n",
        "$$H_i^{\\prime}  = H_i+E(\\frac{\\partial L}{\\partial W_i})×E(\\frac{\\partial L}{\\partial W_i})\\\\\n",
        "W_i^{\\prime} = W_i - \\alpha \\frac{1}{\\sqrt{H_i^{\\prime} }} E(\\frac{\\partial L}{\\partial W_i}) $$\n",
        "\n",
        "H\n",
        "i\n",
        " : i層目に関して、前のイテレーションまでの勾配の二乗和（初期値は0）\n",
        "\n",
        "\n",
        "H\n",
        "′\n",
        "i\n",
        " : 更新した \n",
        "H\n",
        "i\n",
        "\n",
        "《論文》\n",
        "\n",
        "\n",
        "Duchi JDUCHI, J., & Singer, Y. (2011). Adaptive Subgradient Methods for Online Learning and Stochastic Optimization * Elad Hazan. Journal of Machine Learning Research (Vol. 12).  \n",
        "http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLTx9UQ-Cac7"
      },
      "source": [
        "## 【問題8】クラスの完成\n",
        "任意の構成で学習と推定が行えるScratchDeepNeuralNetworkClassifierクラスを完成させてください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdBrfLCmCaIk"
      },
      "source": [
        "# 3.検証\n",
        "\n",
        "## 【問題9】学習と推定\n",
        "層の数や活性化関数を変えたいくつかのネットワークを作成してください。そして、MNISTのデータを学習・推定し、Accuracyを計算してください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLbNUtJdDSa2"
      },
      "source": [
        "# 下記に解答用クラスなどをまとめております。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZefiE4VU2AH"
      },
      "source": [
        "## 前処理"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfEovpOnkz6h"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as setattr\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GzXHuMEwVd_"
      },
      "source": [
        "class GetMiniBatch:\n",
        "    \"\"\"\n",
        "    ミニバッチを取得するイテレータ\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : 次の形のndarray, shape (n_samples, n_features)\n",
        "      訓練データ\n",
        "    y : 次の形のndarray, shape (n_samples, 1)\n",
        "      正解値\n",
        "    batch_size : int\n",
        "      バッチサイズ\n",
        "    seed : int\n",
        "      NumPyの乱数のシード\n",
        "    \"\"\"\n",
        "    def __init__(self, X, y, batch_size = 20, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self._X = X[shuffle_index]\n",
        "        self._y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self._X[p0:p1], self._y[p0:p1]        \n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self._X[p0:p1], self._y[p0:p1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmRXmaCpY4Ps"
      },
      "source": [
        "from keras.datasets import mnist\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# 平滑化（flatten）\n",
        "X_train_fltten = X_train.reshape(-1, 784)\n",
        "X_test_fltten = X_test.reshape(-1, 784)\n",
        "\n",
        "# float化と0or1処理\n",
        "X_train_flt = X_train_fltten.astype(np.float)\n",
        "X_test_flt = X_test_fltten.astype(np.float)\n",
        "X_train_flt /= 255\n",
        "X_test_flt /= 255\n",
        "\n",
        "# one hot処理\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
        "y_test_one_hot = enc.transform(y_test[:, np.newaxis])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCLWp-O9fMn1"
      },
      "source": [
        "# train, testの分割\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_val, y_train_one_hot, y_val_one_hot = train_test_split(np.array(X_train_flt), np.array(y_train_one_hot), test_size=0.2)\n",
        "print(\"x_train\",x_train.shape, \"x_val\", x_val.shape, \"y_train_one_hot.shape\", y_train_one_hot.shape, \"y_val_one_hot\", y_val_one_hot.shape) # (48000, 784)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1X2D195xUqRu"
      },
      "source": [
        "### ベースクラス"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0j5MQlOyQ08L"
      },
      "source": [
        "class ScratchDeepNeuralNetworkClassifier():\n",
        "  def __init__(self, epoch_num=1, batch_size=2000, verbose=True): \n",
        "        self.epoch = epoch_num\n",
        "        self.batch_size = batch_size\n",
        "        self.verbose = verbose\n",
        "\n",
        "  def fit(self, initializing, act, optimization, X, y, X_val=None, y_val=None, sigma=0.01, lr=0.01, n_features=784, n_nodes1=400, n_nodes2=200, n_output=10):  \n",
        "        self.initializing = initializing # 初期化クラスの設定\n",
        "        self.activation= act # 活性化関数クラスの設定\n",
        "        self.optimization= optimization # 活性化関数クラスの設定\n",
        "        self.sigma = sigma\n",
        "        self.lr = lr\n",
        "        self.n_features = n_features\n",
        "        self.n_nodes1 = n_nodes1\n",
        "        self.n_nodes2 = n_nodes2\n",
        "        self.n_output = n_output\n",
        "        self.loss = np.zeros(self.epoch)\n",
        "        self.val_loss = np.zeros(self.epoch)\n",
        "        self.h = None #最適化クラス adagraidで使用\n",
        "\n",
        "        # 最適化クラスの設定\n",
        "        optimizer = self.optimization(self.lr)\n",
        "\n",
        "        # 全結合層にインスタンスを渡す\n",
        "        self.FC1 = FC(self.n_features, self.n_nodes1, self.initializing(self.sigma), optimizer)\n",
        "        self.activation1 = self.activation() #　活性化関数の設定\n",
        "        self.FC2 = FC(self.n_nodes1, self.n_nodes2, self.initializing(self.sigma), optimizer)\n",
        "        self.activation2 = self.activation() #　活性化関数の設定\n",
        "        self.FC3 = FC(self.n_nodes2, self.n_output, self.initializing(self.sigma), optimizer)\n",
        "        self.activation3 = Softmax()\n",
        "\n",
        "        # エポック数分の学習\n",
        "        for i in range(self.epoch):\n",
        "          # ミニバッチの作成\n",
        "          get_mini_batch = GetMiniBatch(X, y, self.batch_size)\n",
        "          # 1エポック（全バッチ）の学習\n",
        "          for x_min, y_min in get_mini_batch:\n",
        "            y_hat = self._forward_propagation(x_min)\n",
        "            print(\"y_hat2\", y_hat[0,:])\n",
        "            loss = self._back_propagation(X=y_hat, Y=y_min)\n",
        "            # print(\"y_hat\",y_hat[:1])\n",
        "\n",
        "          self.loss[i] += loss\n",
        "          if (type(X_val) != bool):\n",
        "            self.val = 1\n",
        "            y_hat_val = self._forward_propagation(X_val)\n",
        "            loss_val = -np.mean(y_val_one_hot * np.log(y_hat_val + 1e-7))\n",
        "            self.val_loss[i] += loss_val\n",
        "\n",
        "          # self.acc_val[i] = accuracy_score(np.argmax(y_val, axis=1), np.argmax(y_hat_val, axis=1))\n",
        "          # verboseをTrueにした際は学習過程を出力\n",
        "          if self.verbose :\n",
        "            print(f\"--{i+1}回目~loss~-------\\n{self.loss[i]}\")\n",
        "            print(f\"--{i+1}回目~loss_val~---\\n{self.val_loss[i]}\")\n",
        "            # print(f'epoch:{self.epoch:>3} loss:{self.loss:>8,.3f}')\n",
        "\n",
        "  # フォワードプロパゲーションの実行\n",
        "  def _forward_propagation(self, X):\n",
        "      A1 = self.FC1.forward(X)\n",
        "      Z1 = self.activation1.forward(A1)\n",
        "      A2 = self.FC2.forward(Z1)\n",
        "      Z2 = self.activation2.forward(A2)\n",
        "      A3 = self.FC3.forward(Z2)\n",
        "      Z3 = self.activation3.forward(A3)\n",
        "      return Z3\n",
        "\n",
        "  # バックプロパゲーションの実行\n",
        "  def _back_propagation(self, X, Y):\n",
        "      print(\"y_hat3\", X[0,:])\n",
        "      dA3, loss = self.activation3.backward(X, Y) # 交差エントロピー誤差とソフトマックスを合わせている\n",
        "      dZ2 = self.FC3.backward(dA3)\n",
        "      dA2 = self.activation2.backward(dZ2)\n",
        "      dZ1 = self.FC2.backward(dA2)\n",
        "      dA1 = self.activation1.backward(dZ1)\n",
        "      dZ0 = self.FC1.backward(dA1) # dZ0は使用しない\n",
        "      return loss\n",
        "\n",
        "  def predict(self, X):\n",
        "      y_hat = self._forward_propagation(X)\n",
        "      return np.argmax(y_hat, axis=1)\n",
        "\n",
        "  def plot_cost(self):\n",
        "      plt.title(\"Num_of_Iteration vs Loss\")\n",
        "      plt.xlabel(\"Num_of_Iteration\")\n",
        "      plt.ylabel(\"Loss\")\n",
        "      a = range(self.epoch)\n",
        "      plt.plot(range(1, self.epoch+1), self.loss, color=\"b\", label=\"train_loss\")\n",
        "      if self.val ==1:\n",
        "          plt.plot(range(1, self.epoch+1), self.val_loss, color=\"orange\", label=\"val_loss\")\n",
        "      plt.grid()\n",
        "      plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPfEot4nU3UK"
      },
      "source": [
        "### 全結合層のクラス"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1IyVS8VU0BW"
      },
      "source": [
        "class FC:\n",
        "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
        "        self.optimizer = optimizer\n",
        "        self.n_nodes1 = n_nodes1\n",
        "        self.n_nodes2 = n_nodes2\n",
        "        self.W = initializer.W(self.n_nodes1, self.n_nodes2)\n",
        "        self.B = initializer.B(self.n_nodes2)\n",
        "        # 初期化\n",
        "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
        "        # self.update_w = optimizer.update()\n",
        "        # self.update_b = optimizer.update()\n",
        "\n",
        "    # フォワードプロパゲーション時の処理\n",
        "    def forward(self, X):\n",
        "      self.X = X\n",
        "      out = X@self.W+self.B\n",
        "      return out\n",
        "    \n",
        "    # バックプロパゲーション時の処理\n",
        "    def backward(self, dA):\n",
        "      dZ = dA@(self.W.T)\n",
        "      dW = (self.X.T)@dA\n",
        "      dB = np.sum(dA, axis=0)\n",
        "      \n",
        "      # 更新\n",
        "      # self.W = self.update_w(dW, self.W)\n",
        "      # self.W = self.update_w(dB, self.B)\n",
        "      self.W = self.optimizer.update(dW, self.W)\n",
        "      self.B = self.optimizer.update(dB, self.B)\n",
        "      # print(\"W\", self.W[:100])\n",
        "      # print(\"B\", self.B[:100])\n",
        "      return dZ"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIvwIyo8U3sJ"
      },
      "source": [
        "### 初期化クラス"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrm7OHgNU2Ki"
      },
      "source": [
        "# SimpleInitializer ------------------------------------------------------------\n",
        "class SimpleInitializer:\n",
        "    def __init__(self, sigma):\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def W(self, n_nodes1, n_nodes2):\n",
        "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
        "        return W\n",
        "\n",
        "    def B(self, n_nodes2):\n",
        "        B = self.sigma * np.random.randn(1, n_nodes2)\n",
        "        return B\n",
        "\n",
        "# XavierInitializer ------------------------------------------------------------\n",
        "class XavierInitializer:\n",
        "    def __init__(self, Xavier):\n",
        "        self.Xavier = Xavier\n",
        "        \n",
        "\n",
        "    def W(self, n_nodes1, n_nodes2):\n",
        "        W = np.random.randn(n_nodes1, n_nodes2) * np.sqrt(1.0 / n_nodes1)\n",
        "        return W\n",
        "\n",
        "    def B(self, n_nodes2):\n",
        "        B = np.random.randn(1, n_nodes2) * np.sqrt(1.0 / 1.0)\n",
        "        return B\n",
        "\n",
        "# He-----------------------------------------------------------------------------\n",
        "class HeInitializer:\n",
        "    def __init__(self, He):\n",
        "        self.He = He\n",
        "\n",
        "    def W(self, n_nodes1, n_nodes2):\n",
        "        W = np.random.randn(n_nodes1, n_nodes2) * np.sqrt(2.0 / n_nodes1)\n",
        "        return W\n",
        "\n",
        "    def B(self, n_nodes2):\n",
        "        B = np.random.randn(1, n_nodes2) * np.sqrt(2.0 / 1)\n",
        "        return B"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Khu9OKHYU4SQ"
      },
      "source": [
        "### 最適化クラス"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXMTjo1SVLrB"
      },
      "source": [
        "# SGD---------------------------------------------\n",
        "class SGD:\n",
        "    def __init__(self, lr):\n",
        "        self.lr = lr\n",
        "\n",
        "    def update(self, dWorB, WorB):\n",
        "      self.WorB = WorB\n",
        "      self.WorB -= self.lr*dWorB\n",
        "      return self.WorB\n",
        "\n",
        "# AdaGrad----------------------------------------\n",
        "class  AdaGrad:\n",
        "    def __init__(self, lr):\n",
        "      self.lr = lr\n",
        "      self.H = 0\n",
        "\n",
        "    def update(self, dWorB, WorB):\n",
        "      print(\"H\", self.H)\n",
        "      self.H += dWorB**2\n",
        "      print(\"H\", self.H[0,0])\n",
        "      WorB -= self.lr * dWorB / np.sqrt(self.H)\n",
        "      return WorB"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFvs3cBEWDN9"
      },
      "source": [
        "### 活性化関数クラス"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bn2HG37fU2g8"
      },
      "source": [
        "# ソフトマックス関数のクラス----------------------------------------------\n",
        "class Softmax:\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  # forward時の処理\n",
        "  def forward(self, X):\n",
        "    y_hat = np.exp(X) / np.sum(np.exp(X), axis = 1).reshape(-1,1)\n",
        "    return y_hat\n",
        "\n",
        "  # backward時の処理\n",
        "  def backward(self, X, Y):\n",
        "    loss = -np.mean(Y * np.log(X + 10e-7))\n",
        "    print(\"X\", X[0,:])\n",
        "    print(\"Y\", Y[0,:])\n",
        "    dA3 = X - Y\n",
        "    print(\"dA3\", dA3[0,:])\n",
        "    return dA3, loss\n",
        "\n",
        "# ReLU関数のクラス----------------------------------------------\n",
        "class ReLU:\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  # forward時の処理\n",
        "  def forward(self, X):\n",
        "    print(\"re1\", X[0,:])\n",
        "    a = np.maximum(0, X)\n",
        "    print(\"re2\", a[0,:])\n",
        "    return a\n",
        "    \n",
        "  # backward時の処理\n",
        "  def backward(self, X):\n",
        "    print(\"re3\", X[0,:])\n",
        "    b = np.maximum(0, X)\n",
        "    print(\"re2\", b[0,:])\n",
        "    return b\n",
        "    print(\"re4\", np.maximum(0, X)[0,:])\n",
        "\n",
        "# tanh関数のクラス----------------------------------------------\n",
        "class Tanh:\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  # forward時の処理\n",
        "  def forward(self, X):\n",
        "    self.out = np.tanh(X)\n",
        "    return self.out\n",
        "\n",
        "  # backward時の処理\n",
        "  def backward(self, X):\n",
        "    return X*(1-np.tanh(self.out)**2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cp3rpF_DEqgG"
      },
      "source": [
        "### 学習"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7W4LPXkEdDj"
      },
      "source": [
        "sdnn = ScratchDeepNeuralNetworkClassifier(epoch_num=1)\n",
        "sdnn.fit(XavierInitializer, ReLU, SGD, x_train, y_train_one_hot, x_val, y_val_one_hot, sigma=0.01, lr=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ySQTmS949Jk"
      },
      "source": [
        "y_hat = sdnn.predict(X_test_flt)\n",
        "y_hat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcc3c_2XFwAz"
      },
      "source": [
        "y_test[:100]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekbY2LbyLR6o"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "accuracy_score(y_test, y_hat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ISd_qYYwTYH"
      },
      "source": [
        "sdnn.plot_cost()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fv4nsrFBwWNv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}