{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "term2_sprint21.ipynb",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "11_5N1w_-GXvQh_p1eSSx2dmxfpgozxhO",
      "authorship_tag": "ABX9TyNXrNjB7Fndq8bqNpr6AoVf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/T-Sawao/diveintocode-ml3/blob/main/term2_sprint21.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyPMOHrzWtUj"
      },
      "source": [
        "# Sprint21 自然言語処理入門\n",
        "\n",
        "## 1.このSprintについて\n",
        "\n",
        "### Sprintの目的\n",
        "- 自然言語処理の一連の流れを学ぶ\n",
        "- 自然言語のベクトル化の方法を学ぶ\n",
        "\n",
        "### どのように学ぶか\n",
        "自然言語処理定番のデータセットを用いて、一連の流れを見ていきます。\n",
        "\n",
        "## 2.自然言語のベクトル化\n",
        "自然言語処理（NLP, Natural Language Processing） とは人間が普段使っている 自然言語 をコンピュータに処理させる技術のことです。ここではその中でも、機械学習の入力として自然言語を用いることを考えていきます。\n",
        "\n",
        "多くの機械学習手法は 数値データ（量的変数） の入力を前提にしていますので、自然言語の テキストデータ を数値データに変換する必要があります。これを 自然言語のベクトル化 と呼びます。ベクトル化の際にテキストデータの特徴をうまく捉えられるよう、様々な手法が考えられてきていますので、このSprintではそれらを学びます。\n",
        "\n",
        "### 非構造化データ\n",
        "データの分類として、表に数値がまとめられたようなコンピュータが扱いやすい形を 構造化データ 、人間が扱いやすい画像・動画・テキスト・音声などを 非構造化データ と呼ぶことがあります。自然言語のベクトル化は、非構造化データを構造化データに変換する工程と言えます。同じ非構造化データでも、画像に対してはディープラーニングを用いる場合この変換作業はあまり必要がありませんでしたが、テキストにおいてはこれをどう行うかが重要です。\n",
        "\n",
        "### 自然言語処理により何ができるか\n",
        "機械学習の入力や出力に自然言語のテキストを用いることで様々なことができます。入力も出力もテキストである例としては 機械翻訳 があげられ、実用化されています。入力は画像で出力がテキストである 画像キャプション生成 やその逆の文章からの画像生成も研究が進んでいます。\n",
        "\n",
        "しかし、出力をテキストや画像のような非構造化データとすることは難易度が高いです。比較的簡単にできることとしては、入力をテキスト、出力をカテゴリーとする テキスト分類 です。\n",
        "\n",
        "アヤメやタイタニック、手書き数字のような定番の存在として、IMDB映画レビューデータセット の感情分析があります。レビューの文書が映画に対して肯定的か否定的かを2値分類します。文書ごとの肯定・否定はラベルが与えられています。このSprintではこれを使っていきます。\n",
        "\n",
        "## 3.IMDB映画レビューデータセットの準備\n",
        "IMDB映画レビューデータセットを準備します。\n",
        "\n",
        "### ダウンロード\n",
        "次のwgetコマンドによってダウンロードします。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BQ5Nqf0gvcr"
      },
      "source": [
        "import os\n",
        "os.chdir('/content/drive/MyDrive/Colab Notebooks/diveintocode-ml/term2/term2_sprintt21')\n",
        "print(os.getcwd())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHj5jHQkHHet"
      },
      "source": [
        "# # IMDBをカレントフォルダにダウンロード\n",
        "# !wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "# # 解凍\n",
        "# !tar zxf aclImdb_v1.tar.gz\n",
        "# # aclImdb/train/unsupはラベル無しのため削除\n",
        "# !rm -rf aclImdb/train/unsup\n",
        "# # IMDBデータセットの説明を表示\n",
        "# !cat aclImdb/README"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jFoQk1bgJFa"
      },
      "source": [
        "以下のサイトで公開されているデータセットです。\n",
        "\n",
        "\n",
        "Sentiment Analysis  \n",
        "http://ai.stanford.edu/~amaas/data/sentiment/ \n",
        "\n",
        "\n",
        "読み込み\n",
        "scikit-learnのload_filesを用いて読み込みます。\n",
        "\n",
        "\n",
        "sklearn.datasets.load_files — scikit-learn 0.21.3 documentation  \n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_files.html\n",
        "\n",
        "\n",
        "《読み込むコード》"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHxBtbDEKs0R"
      },
      "source": [
        "from sklearn.datasets import load_files\n",
        "train_review = load_files('aclImdb/train/', encoding='utf-8')\n",
        "x_train, y_train = train_review.data, train_review.target\n",
        "test_review = load_files('aclImdb/test/', encoding='utf-8')\n",
        "x_test, y_test = test_review.data, test_review.target\n",
        "# ラベルの0,1と意味の対応の表示\n",
        "print(train_review.target_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoAmhDERpi3U"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "print(np.array(x_train).shape)\n",
        "print(np.array(y_train).shape)\n",
        "print(np.array(x_test).shape)\n",
        "print(np.array(y_test).shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLQj9ZBuhVEH"
      },
      "source": [
        "### このデータセットについて\n",
        "中身を見てみると、英語の文章が入っていることが分かります。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MT5mT16LEPK"
      },
      "source": [
        "print(\"x : {}\".format(x_train[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOhvFtDjheov"
      },
      "source": [
        "IMDBはInternet Movie Databaseの略で、映画のデータベースサイトです。\n",
        "\n",
        "\n",
        "Ratings and Reviews for New Movies and TV Shows - IMDb  \n",
        "https://www.imdb.com/\n",
        "\n",
        "このサイトではユーザーが映画に対して1から10点の評価とコメントを投稿することができます。そのデータベースから訓練データは25000件、テストデータは25000件のデータセットを作成しています。\n",
        "\n",
        "\n",
        "4点以下を否定的、7点以下を肯定的なレビューとして2値のラベル付けしており、これにより感情の分類を行います。5,6点の中立的なレビューはデータセットに含んでいません。また、ラベルは訓練用・テスト用それぞれで均一に入っています。詳細はダウンロードしたREADMEを確認してください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPXCtBBEifCe"
      },
      "source": [
        "## 4.古典的な手法\n",
        "\n",
        "古典的ながら現在でも強力な手法であるBoWとTF-IDFを見ていきます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MayhnD74kRwJ"
      },
      "source": [
        "## 5.BoW\n",
        "\n",
        "単純ながら効果的な方法として BoW (Bag of Words) があります。これは、サンプルごとに単語などの 登場回数 を数えたものをベクトルとする方法です。単語をカテゴリとして捉え one-hot表現 していることになります。\n",
        "\n",
        "\n",
        "### 例\n",
        "例として、IMDBデータセットからある3文の最初の5単語を抜き出したものを用意しました。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7eNU3z8kY-U"
      },
      "source": [
        "mini_dataset = \\\n",
        "  [\"This movie is very good.\",\n",
        "  \"This film is a good\",\n",
        "  \"Very bad. Very, very bad.\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eQyh5UFpS4h"
      },
      "source": [
        "### この3文にBoWを適用させてみます。scikit-learnのCountVectorizerを利用します。\n",
        "\n",
        "\n",
        "sklearn.feature_extraction.text.CountVectorizer — scikit-learn 0.21.3 documentation  \n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Tq97l-5pfUT"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# テキストドキュメントのコレクションをトークンカウントのマトリックスに変換\n",
        "vectorizer = CountVectorizer(token_pattern=r'(?u)\\b\\w+\\b')\n",
        "bow = (vectorizer.fit_transform(mini_dataset)).toarray()\n",
        "        #------------------------------\n",
        "        # 語彙辞書を学び、ドキュメントと用語のマトリックスを返す\n",
        "# DataFrameにまとめる\n",
        "df = pd.DataFrame(bow, columns=vectorizer.get_feature_names())\n",
        "display(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07cvRzaHpqLK"
      },
      "source": [
        "例にあげた3文の中で登場する8種類の単語が列名になり、0,1,2番目のサンプルでそれらが何回登場しているかを示しています。2番目のサンプル「Very bad. Very, very bad.」ではbadが2回、veryが3回登場しています。列名になっている言葉はデータセットが持つ 語彙 と呼びます。\n",
        "\n",
        "\n",
        "テキストはBoWにより各サンプルが語彙数の次元を持つ特徴量となり、機械学習モデルへ入力できるようになります。この時使用したテキスト全体のことを コーパス と呼びます。語彙はコーパスに含まれる言葉よって決まり、それを特徴量としてモデルの学習を行います。そのため、テストデータではじめて登場する語彙はベクトル化される際に無視されます。\n",
        "\n",
        "\n",
        "### 前処理\n",
        "CountVectorizerクラスでは大文字は小文字に揃えるという 前処理 が自動的に行われています。こういった前処理は自然言語処理において大切で、不要な記号などの消去（テキストクリーニング）や表記揺れの統一といったことを別途行うことが一般的です。\n",
        "\n",
        "\n",
        "語形が「see」「saw」「seen」のように変化する単語に対して語幹に揃える ステミング と呼ばれる処理を行うこともあります。\n",
        "\n",
        "\n",
        "### トークン\n",
        "BoWは厳密には単語を数えているのではなく、 トークン（token） として定めた固まりを数えます。\n",
        "\n",
        "\n",
        "何をトークンとするかはCountVectorizerでは引数token_patternで 正規表現 の記法により指定されます。デフォルトはr'(?u)\\b\\w\\w+\\b'ですが、上の例ではr'(?u)\\b\\w+\\b'としています。\n",
        "\n",
        "\n",
        "デフォルトでは空白・句読点・スラッシュなどに囲まれた2文字以上の文字を1つのトークンとして抜き出すようになっているため、「a」や「I」などがカウントされません。英語では1文字の単語は文章の特徴をあまり表さないため、除外されることもあります。しかし、上の例では1文字の単語もトークンとして抜き出すように引数を指定しています。\n",
        "\n",
        "\n",
        "**《正規表現》**\n",
        "\n",
        "\n",
        "正規表現は前処理の際にも活用しますが、ここでは詳細は扱いません。Pythonではreモジュールによって正規表現操作ができます。\n",
        "\n",
        "\n",
        "re — 正規表現操作  \n",
        "https://docs.python.org/ja/3/library/re.html\n",
        "\n",
        "\n",
        "正規表現を利用する際はリアルタイムで結果を確認できる以下のようなサービスが便利です。\n",
        "\n",
        "\n",
        "Online regex tester and debugger: PHP, PCRE, Python, Golang and JavaScript  \n",
        "https://regex101.com/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAgS3ojsrhXb"
      },
      "source": [
        "### 0.0.1（予備知識）正規表現とは？\n",
        "https://userweb.mnet.ne.jp/nakama/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rw0jzcRrxjG"
      },
      "source": [
        "### 形態素解析\n",
        "英語などの多くの言語では空白という分かりやすい基準でトークン化が行えますが、日本語ではそれが行えません。\n",
        "\n",
        "\n",
        "日本語では名詞や助詞、動詞のように異なる 品詞 で分けられる単位で 分かち書き することになります。例えば「私はプログラミングを学びます」という日本語の文は「私/は/プログラミング/を/学び/ます」という風になります。\n",
        "\n",
        "\n",
        "これには MeCab や Janome のような形態素解析ツールを用います。Pythonから利用することも可能です。MeCabをウェブ上で簡単に利用できるWeb茶まめというサービスも国立国語研究所が提供しています。\n",
        "\n",
        "\n",
        "自然言語では新しい言葉も日々生まれますので、それにどれだけ対応できるかも大切です。MeCab用の毎週更新される辞書として mecab-ipadic-NEologd がオープンソースで存在しています。\n",
        "\n",
        "\n",
        "mecab-ipadic-neologd/README.ja.md at master · neologd/mecab-ipadic-neologd  \n",
        "https://github.com/neologd/mecab-ipadic-neologd/blob/master/README.ja.md\n",
        "\n",
        "\n",
        "### n-gram\n",
        "上のBoWの例では1つの単語（トークン）毎の登場回数を数えましたが、これでは語順は全く考慮されていません。\n",
        "\n",
        "\n",
        "考慮するために、隣あう単語同士をまとめて扱う n-gram という考え方を適用することがあります。2つの単語をまとめる場合は 2-gram (bigram) と呼び、次のようになります。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_CpdBsTsBtP"
      },
      "source": [
        "# ngram_rangeで利用するn-gramの範囲を指定する\n",
        "vectorizer = CountVectorizer(ngram_range=(2, 2), token_pattern=r'(?u)\\b\\w+\\b')\n",
        "bow_train = (vectorizer.fit_transform(mini_dataset)).toarray()\n",
        "            #--------------------------------\n",
        "            # 生の文書のすべてのトークンの語彙辞書を学びます。\n",
        "                                                  #--------------\n",
        "                                                  #疎行列（スパース行列）を効率的に扱う。\n",
        "df = pd.DataFrame(bow_train, columns=vectorizer.get_feature_names())\n",
        "display(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRWvCJ0OvlaO"
      },
      "source": [
        "### 0.0.2（予備知識） 疎行列（スパース行列）とは？  \n",
        "https://qiita.com/KQTS/items/e5500ba6e2681456e268"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_hFHQc6wNxr"
      },
      "source": [
        "2-gramにより「very good」と「very bad」が区別して数えられています。\n",
        "\n",
        "\n",
        "単語をまとめない場合は 1-gram (unigram) と呼びます。3つまとめる3-gram(trigram)など任意の数を考えることができます。1-gramと2-gramを組み合わせてBoWを行うといったこともあります。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EvV_elKwNbo"
      },
      "source": [
        "### 【問題1】BoWのスクラッチ実装\n",
        "以下の3文のBoWを求められるプログラムをscikit-learnを使わずに作成してください。1-gramと2-gramで計算してください。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AA0lwMEcwuyt"
      },
      "source": [
        "# 提供データ\n",
        "mini_dataset = \\\n",
        "  [\"This movie is SOOOO funny!!!\",\n",
        "  \"What a movie! I never\",\n",
        "  \"best movie ever!!!!! this movie\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grDA_sveyWbF"
      },
      "source": [
        "1.1.1（解答） 1-gram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kq7yuBF0yhj_"
      },
      "source": [
        "# リスト内の文字を全て小文字に変換\n",
        "data = list(map(str.lower,mini_dataset))\n",
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgj83CWiFuS4"
      },
      "source": [
        "word_list = [word.split(\" \") for word in data]\n",
        "word_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVAegKB9CHpP"
      },
      "source": [
        "import string\n",
        "kigo = string.punctuation # >>!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
        "table = str.maketrans( \"\", \"\", kigo)\n",
        "kigo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWA-Q0gmp5uB"
      },
      "source": [
        "# 文字の型を標準化するためのステイミング \n",
        "# 注意:機会にとっての処理しやすい文字列に変える技術のため、\n",
        "# 人が認識できる言語としては意味を成さない場合がある。\n",
        "\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "ps = PorterStemmer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THECJo1WrWle"
      },
      "source": [
        "words = []\n",
        "split_words = []\n",
        "for word in word_list:\n",
        "  a = []\n",
        "  for moji in word:\n",
        "    moji = ps.stem(moji)\n",
        "    split_words.append(moji.translate(table))\n",
        "    a.append(moji.translate(table))\n",
        "  words.append(a)\n",
        "print(words)\n",
        "print(split_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "faIlSscuzo84"
      },
      "source": [
        "result_dict1 = {}\n",
        "for i in split_words:\n",
        "    one_list = []\n",
        "    for j in words:\n",
        "      one_list.append(j.count(i))\n",
        "      result_dict1[i] = one_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UonsYNZSzo2Q"
      },
      "source": [
        "result_dict1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjl5K6HyzoAJ"
      },
      "source": [
        "result_df = pd.DataFrame(result_dict1, index=data)\n",
        "result_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1TeHO9kshYD"
      },
      "source": [
        "### 1.2.1（解答） 2-gram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQ9H8AgNAh0T"
      },
      "source": [
        "print(words)\n",
        "print(split_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8sZesi6sjp8"
      },
      "source": [
        "words2 = []\n",
        "split_words2 = []\n",
        "\n",
        "for l in words:\n",
        "  b = []\n",
        "  for a in range(len(l) -2 +1):    \n",
        "    ab = l[a : a + 2]\n",
        "    d = ab[0] +\" \" + ab[1]\n",
        "    split_words2.append(d)\n",
        "    b.append(d)\n",
        "  words2.append(b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPOdZRVuGWmr"
      },
      "source": [
        "words2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GidTyZT6Gb0L"
      },
      "source": [
        "split_words2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeW0YGYaLRAI"
      },
      "source": [
        "result_dict2 = {}\n",
        "for i in split_words2:\n",
        "    one_list = []\n",
        "    for j in words2:\n",
        "      one_list.append(j.count(i))\n",
        "      result_dict2[i] = one_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGkHO4UbLaeM"
      },
      "source": [
        "result_df = pd.DataFrame(result_dict2, index=data)\n",
        "result_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Du8Kx3rspYS"
      },
      "source": [
        "## 6.TF-IDF\n",
        "\n",
        "BoWの発展的手法として TF-IDF もよく使われます。これは Term Frequency (TF) と Inverse Document Frequency (IDF) という2つの指標の組み合わせです。\n",
        "\n",
        "\n",
        "**《標準的なTF-IDFの式》**\n",
        "\n",
        "\n",
        "Term Frequency:\n",
        "\n",
        "\n",
        "$$tf(t,d) = \\frac{n_{t,d}}{\\sum_{s \\in d}n_{s,d}}$$\n",
        "\n",
        "$n_{t,d}$ : サンプルd内のトークンtの出現回数（BoWと同じ）\n",
        "\n",
        "\n",
        "$\\sum_{s \\in d}n_{s,d}$ : サンプルdの全トークンの出現回数の和\n",
        "\n",
        "\n",
        "Inverse Document Frequency:\n",
        "\n",
        "\n",
        "$$idf(t) = \\log{\\frac{N}{df(t)}}$$\n",
        "$N$ : サンプル数\n",
        "\n",
        "\n",
        "$df(t)$ : トークンtが出現するサンプル数\n",
        "\n",
        "\n",
        "＊logの底は任意の値\n",
        "\n",
        "\n",
        "TF-IDF:\n",
        "\n",
        "\n",
        "$$tfidf(t, d) = tf(t, d) \\times idf(t)$$\n",
        "\n",
        "\n",
        "### IDF\n",
        "IDFはそのトークンがデータセット内で珍しいほど値が大きくなる指標です。\n",
        "\n",
        "\n",
        "サンプル数 $N$ をIMDB映画レビューデータセットの訓練データに合わせ25000として、トークンが出現するサンプル数 $df(t)$ を変化させたグラフを確認してみると、次のようになります。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZOuNcVotTcI"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "n_samples = 25000\n",
        "idf = np.log(n_samples/np.arange(1,n_samples))\n",
        "plt.title(\"IDF\")\n",
        "plt.xlabel(\"df(t)\")\n",
        "plt.ylabel(\"IDF\")\n",
        "plt.plot(idf)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mXbiF57spV0"
      },
      "source": [
        "TF-IDFではこの数を出現回数に掛け合わせるので、珍しいトークンの登場に重み付けを行なっていることになります。\n",
        "\n",
        "\n",
        "### ストップワード\n",
        "あまりにも頻繁に登場するトークンは、値を小さくするだけでなく、取り除くという前処理を加えることもあります。取り除くもののことを ストップワード と呼びます。既存のストップワード一覧を利用したり、しきい値によって求めたりします。\n",
        "\n",
        "\n",
        "scikit-learnのCountVectorizerでは引数stop_wordsにリストで指定することで処理を行なってくれます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXk9ePYLtjUZ"
      },
      "source": [
        "vectorizer = CountVectorizer(stop_words=[\"is\"], token_pattern=r'\\b\\w+\\b')\n",
        "bow_train = (vectorizer.fit_transform(mini_dataset)).toarray()\n",
        "df = pd.DataFrame(bow_train, columns=vectorizer.get_feature_names())\n",
        "display(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZrSeOVqspTr"
      },
      "source": [
        "代表的な既存のストップワード一覧としては、NLTK という自然言語処理のライブラリのものがあげられます。あるデータセットにおいては特別重要な意味を持つ単語が一覧に含まれている可能性もあるため、使用する際は中身を確認することが望ましいです。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaYTAPKEtnYE"
      },
      "source": [
        "# はじめて使う場合はストップワードをダウンロード\n",
        "import nltk\n",
        "stop_words = nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "print(\"stop word : {}\".format(stop_words)) # 'i', 'me', 'my', ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfcVx1ZmspRi"
      },
      "source": [
        "逆に、登場回数が特に少ないトークンも取り除くことが多いです。全てのトークンを用いるとベクトルの次元数が著しく大きくなってしまい計算コストが高まるためです。\n",
        "\n",
        "\n",
        "scikit-learnのCountVectorizerでは引数max_featuresに最大の語彙数を指定することで処理を行なってくれます。以下の例では出現数が多い順に5個でベクトル化しています。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agG0BxAetrvi"
      },
      "source": [
        "vectorizer = CountVectorizer(token_pattern=r'\\b\\w+\\b', max_features = 5)\n",
        "bow_train = (vectorizer.fit_transform(mini_dataset)).toarray()\n",
        "df = pd.DataFrame(bow_train, columns=vectorizer.get_feature_names())\n",
        "display(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JswOgKe7spPQ"
      },
      "source": [
        "## 【問題2】TF-IDFの計算\n",
        "IMDB映画レビューデータセットをTF-IDFによりベクトル化してください。NLTKのストップワードを利用し、最大の語彙数は5000程度に設定してください。テキストクリーニングやステミングなどの前処理はこの問題では要求しません。\n",
        "\n",
        "\n",
        "TF-IDFの計算にはscikit-learnの以下のどちらかのクラスを使用してください。\n",
        "\n",
        "\n",
        "sklearn.feature_extraction.text.TfidfVectorizer — scikit-learn 0.21.3 documentation  \n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
        "\n",
        "sklearn.feature_extraction.text.TfidfTransformer — scikit-learn 0.21.3 documentation  \n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html\n",
        "\n",
        "\n",
        "なお、scikit-learnでは標準的な式とは異なる式が採用されています。\n",
        "\n",
        "\n",
        "また、デフォルトではnorm=\"l2\"の引数が設定されており、各サンプルにL2正規化が行われます。norm=Noneとすることで正規化は行われなくなります。\n",
        "\n",
        "\n",
        "Term Frequency:\n",
        "\n",
        "$$tf(t,d) = n_{t,d}$$\n",
        "\n",
        "\n",
        "$ntd$ : サンプルd内のトークンtの出現回数\n",
        "\n",
        "\n",
        "scikit-learnのTFは分母がなくなりBoWと同じ計算になります。\n",
        "\n",
        "\n",
        "Inverse Document Frequency:\n",
        "\n",
        "$$idf(t) = \\log{\\frac{1+N}{1+df(t)}}+1$$\n",
        "\n",
        "$N$ : サンプル数\n",
        "\n",
        "\n",
        "$df(t)$ : トークンtが出現するサンプル数\n",
        "\n",
        "\n",
        "＊logの底はネイピア数e\n",
        "\n",
        "\n",
        "詳細は以下のドキュメントを確認してください。\n",
        "\n",
        "\n",
        "5.2.3.4. Tf–idf term weighting — scikit-learn 0.21.3 documentation  \n",
        "https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUhQw3z1uqQY"
      },
      "source": [
        "### 2.1.1（解答）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANF9PXAkX2ts"
      },
      "source": [
        "# 生のドキュメントのコレクションをTF-IDF機能のマトリックスに変換します。 \n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(stop_words=stop_words, max_features = 5000)\n",
        "X_vec = vectorizer.fit_transform(x_train).toarray()\n",
        "\n",
        "print(f\"X_vec.shape:{X_vec.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5v3LUpUoyrB"
      },
      "source": [
        "y_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KC6t2Wo62swy"
      },
      "source": [
        "## 【問題3】TF-IDFを用いた学習\n",
        "問題2で求めたベクトルを用いてIMDB映画レビューデータセットの学習・推定を行なってください。モデルは2値分類が行える任意のものを利用してください。\n",
        "\n",
        "\n",
        "ここでは精度の高さは求めませんが、最大の語彙数やストップワード、n-gramの数を変化させて影響を検証してみてください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYvyd3Js2ksq"
      },
      "source": [
        "### 3.1.1(解答)ストップワード使用、最大語彙数5000 (問題2の設定)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4ACsQ35T6NE"
      },
      "source": [
        "# 訓練データとテストデータに分割\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(X_vec, y_train, test_size=0.2)\n",
        "print(X_train.shape, X_val.shape, Y_train.shape, Y_val.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2IgGXjezY_s"
      },
      "source": [
        "# ロジスティック回帰\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "clf = LogisticRegression()\n",
        "clf.fit(X_train, Y_train)\n",
        "y_predict = clf.predict(X_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sk9voujBqnGD"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "print(\"正解率：\",accuracy_score(y_predict, Y_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLe-7kD7q3Uc"
      },
      "source": [
        "### 3.2.1(解答) ストップワードなし、最大語彙数5000"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8UzRAVzsQIS"
      },
      "source": [
        "vectorizer = TfidfVectorizer(max_features = 5000)\n",
        "X_vec1 = vectorizer.fit_transform(x_train).toarray()\n",
        "\n",
        "print(f\"X_vec1.shape:{X_vec1.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIownFiJul1b"
      },
      "source": [
        "X_train1, X_val1, Y_train, Y_val = train_test_split(X_vec1, y_train, test_size=0.2)\n",
        "print(X_train1.shape, X_val1.shape, Y_train.shape, Y_val.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foj33ANQnzda"
      },
      "source": [
        "clf1 = LogisticRegression()\n",
        "clf1.fit(X_train1, Y_train)\n",
        "y_predict1 = clf1.predict(X_val1)\n",
        "print(\"正解率：\",accuracy_score(y_predict1, Y_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxO-q7jNvFyS"
      },
      "source": [
        "### 3.3.1(解答) ストップワードなし、最大語彙数2500"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBbN5YOPvM5K"
      },
      "source": [
        "vectorizer = TfidfVectorizer(max_features = 2500)\n",
        "X_vec2 = vectorizer.fit_transform(x_train).toarray()\n",
        "\n",
        "X_train2, X_val2, Y_train, Y_val = train_test_split(X_vec2, y_train, test_size=0.2)\n",
        "print(X_train2.shape, X_val2.shape, Y_train.shape, Y_val.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eb8EU6l7vekJ"
      },
      "source": [
        "clf2 = LogisticRegression()\n",
        "clf2.fit(X_train2, Y_train)\n",
        "y_predict2 = clf2.predict(X_val2)\n",
        "print(\"正解率：\",accuracy_score(y_predict2, Y_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TSb5AIvvqBj"
      },
      "source": [
        "### 3.4.1(解答) ストップワード使用、最大語彙数5000 2-gram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIsldlyUvnla"
      },
      "source": [
        "vectorizer = TfidfVectorizer(stop_words=stop_words, ngram_range=(2, 2), max_features=5000)\n",
        "X_vec3 = vectorizer.fit_transform(x_train).toarray()\n",
        "\n",
        "X_train3, X_val3, Y_train, Y_val = train_test_split(X_vec3, y_train, test_size=0.2)\n",
        "print(X_train3.shape, X_val3.shape, Y_train.shape, Y_val.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2j3v4WyCwsbc"
      },
      "source": [
        "clf3 = LogisticRegression()\n",
        "clf3.fit(X_train3, Y_train)\n",
        "y_predict3 = clf3.predict(X_val3)\n",
        "print(\"正解率：\",accuracy_score(y_predict3, Y_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GKreCtcxPdg"
      },
      "source": [
        "## 7.Word2Vec\n",
        "\n",
        "ニューラルネットワークを用いてベクトル化を行う手法が Word2Vec です。\n",
        "\n",
        "\n",
        "BoWやTF-IDFはone-hot表現であったため、得られるベクトルの次元は語彙数分になります。そのため、語彙数を増やしにくいという問題があります。一方で、Word2Vecでは単語を任意の次元のベクトルに変換します。これをを Word Embedding（単語埋め込み） や 分散表現 と呼びます。変換操作を「ベクトル空間に埋め込む」と言うことが多いです。\n",
        "\n",
        "\n",
        "Word2VecにはCBoWとSkip-gramという2種類の仕組みがあるため順番に見ていきます。\n",
        "\n",
        "\n",
        "### CBoW\n",
        "CBoW (Continuous Bag-of-Words) によるWord2Vecではある単語とある単語の間に来る単語を推定できるように全結合層2層のニューラルネットワークを学習します。\n",
        "\n",
        "\n",
        "単語はコーパスの語彙数次元のone-hot表現を行なっておきます。そのため、入力と出力の次元は語彙数と同じになります。一方で、中間のノード数をWord2Vecにより得たい任意の次元数とします。これにより全結合層の重みは「得たい次元のノード数×語彙数」になります。このネットワークにより学習を行なった後、出力側の重みを取り出すことで、各語彙を表すベクトルを手に入れることができます。\n",
        "\n",
        "\n",
        "間の単語の推定を行なっているため、同じ箇所で代替可能な言葉は似たベクトルになるというメリットもあります。これはBoWやTF-IDFでは得られない情報です。\n",
        "\n",
        "\n",
        "あるテキストは「そのテキストの長さ（単語数）×Word2Vecで得た分散表現の次元数」の配列になりますが、各入力の配列を揃える必要があるモデルに入力するためには、短いテキストは空白を表す単語を加える パディング を行なったり、長いテキストは単語を消したりします。テキストを 固定長 にすると呼びます。\n",
        "\n",
        "\n",
        "### ウィンドウサイズ\n",
        "入力する単語は推定する前後1つずつだけでなく、複数個とする場合もあります。前後いくつを見るかの大きさを ウィンドウサイズ と呼びます。\n",
        "\n",
        "\n",
        "### Skip-gram\n",
        "CBoWとは逆にある単語の前後の単語を推定できるように全結合層2層のニューラルネットワークを学習する方法が Skip-gram です。学習を行なった後は入力側の重みを取り出し各語彙を表すベクトルとします。現在一般的に使われているのはCBoWよりもSki-gramです。\n",
        "\n",
        "\n",
        "### 利用方法\n",
        "Pythonでは Gensim ライブラリを用いて扱うことができます。\n",
        "\n",
        "\n",
        "gensim: models.word2vec – Word2vec embeddings  \n",
        "https://radimrehurek.com/gensim/models/word2vec.html\n",
        "\n",
        "\n",
        "BoWの例と同じ文章で学習してみます。CountVectorizerと異なり前処理を自動的に行なってはくれないため、単語（トークン）はリストで分割しておきます。また、大文字は小文字に揃え、記号は取り除きます。\n",
        "\n",
        "\n",
        "デフォルトのパラメータではCBoWで計算されます。また、ウィンドウサイズはwindow=5に設定されています。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYfRj_tSxcrj"
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "sentences = [['this', 'movie', 'is', 'very', 'good'], ['this', 'film', 'is', 'a', 'good'], ['very', 'bad', 'very', 'very', 'bad']]\n",
        "model = Word2Vec(min_count=1, size=10) # 次元数を10に設定\n",
        "model.build_vocab(sentences) # 準備\n",
        "model.train(sentences, total_examples=model.corpus_count, epochs=model.iter) # 学習\n",
        "print(\"語彙の一覧 : {}\".format(model.wv.vocab.keys()))\n",
        "for vocab in model.wv.vocab.keys():\n",
        "  print(\"{}のベクトル : \\n{}\".format(vocab, model.wv[vocab]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nM1ur6Iky_nH"
      },
      "source": [
        "このようにしてベクトルが得られます。\n",
        "\n",
        "\n",
        "### 単語の距離\n",
        "ベクトル間で計算を行うことで、ある単語に似たベクトルを持つ単語を見つけることができます。例えばgoodに似たベクトルの単語を3つ探します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFyijDtKxiL1"
      },
      "source": [
        "model.wv.most_similar(positive=\"good\", topn=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tt8X18R3zKo6"
      },
      "source": [
        "今の例では3文しか学習していませんので効果を発揮しませんが、大きなコーパスで学習することで、並列関係のものが近くに来たりなど面白い結果が得られます。\n",
        "\n",
        "\n",
        "### 可視化\n",
        "2次元に圧縮することで単語ごとの位置関係を可視化することができます。以下はt-SNEを用いた例です。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRdBdV4gxkwt"
      },
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "vocabs = model.wv.vocab.keys()\n",
        "tsne_model = TSNE(perplexity=40, n_components=2, init=\"pca\", n_iter=5000, random_state=23)\n",
        "vectors_tsne = tsne_model.fit_transform(model[vocabs])\n",
        "fig, ax = plt.subplots(figsize=(5,5))\n",
        "ax.scatter(vectors_tsne[:, 0], vectors_tsne[:, 1])\n",
        "for i, word in enumerate(list(vocabs)):\n",
        "    plt.annotate(word, xy=(vectors_tsne[i, 0], vectors_tsne[i, 1]))\n",
        "ax.set_yticklabels([])\n",
        "ax.set_xticklabels([])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdNuWJqzzWGo"
      },
      "source": [
        "## 8.IMDB映画レビューデータセットの分散表現\n",
        "\n",
        "IMDB映画レビューデータセットの訓練データをコーパスとしてWord2Vecを学習させ分散表現を獲得しましょう。\n",
        "\n",
        "\n",
        "### 【問題5】コーパスの前処理\n",
        "コーパスの前処理として、特殊文字（!など）やURLの除去、大文字の小文字化といったことを行なってください。また、単語（トークン）はリストで分割してください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSz59QYOza_l"
      },
      "source": [
        "### 5.1.1（解答）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLcVmnsm2n07"
      },
      "source": [
        "import re\n",
        "\n",
        "train_list = []\n",
        "text_list = []\n",
        "X_train = x_train.copy()\n",
        "X_test = x_test.copy()\n",
        "\n",
        "for i in range(len(X_train)):\n",
        "    X_train[i] = X_train[i].lower()\n",
        "    X_train[i] = X_train[i].rstrip()\n",
        "    X_train[i] = X_train[i].translate(table)\n",
        "    token = re.split('[, /]',X_train[i])\n",
        "    train_list.append(token)\n",
        "\n",
        "    X_test[i] = X_test[i].lower()\n",
        "    X_test[i] = X_test[i].rstrip()\n",
        "    X_test[i] = X_test[i].translate(table)\n",
        "    token1 = re.split('[, /]',X_test[i])\n",
        "    text_list.append(token1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65TbDPKcKGR9"
      },
      "source": [
        "x_train[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOUmDqBs8b4Q"
      },
      "source": [
        "train_list[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJaacZ5CKDoB"
      },
      "source": [
        "### 【問題6】Word2Vecの学習\n",
        "Word2Vecの学習を行なってください。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlHRQIuYMTFd"
      },
      "source": [
        "model = Word2Vec(min_count=1, size=10) # 次元数を10に設定\n",
        "model.build_vocab(train_list) # 準備\n",
        "model.train(train_list, total_examples=model.corpus_count, epochs=model.iter) # 学習"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOanhOCvN52M"
      },
      "source": [
        "model.wv.most_similar(positive=\"good\", topn=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LG8di09rOF2k"
      },
      "source": [
        "vocabs = model.wv.vocab.keys()\n",
        "tsne_model = TSNE(perplexity=40, n_components=2, init=\"pca\", n_iter=5000, random_state=23)\n",
        "vectors_tsne = tsne_model.fit_transform(model[vocabs])\n",
        "fig, ax = plt.subplots(figsize=(5,5))\n",
        "ax.scatter(vectors_tsne[:, 0], vectors_tsne[:, 1])\n",
        "for i, word in enumerate(list(vocabs)):\n",
        "    plt.annotate(word, xy=(vectors_tsne[i, 0], vectors_tsne[i, 1]))\n",
        "ax.set_yticklabels([])\n",
        "ax.set_xticklabels([])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}