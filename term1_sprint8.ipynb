{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# term1_sprint8 アンサンブル学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .アンサンブル学習\n",
    "\n",
    "3種類のアンサンブル学習をスクラッチ実装していきます。そして、それぞれの効果を小さめのデータセットで確認します。\n",
    "\n",
    "\n",
    "ブレンディング\n",
    "バギング\n",
    "スタッキング\n",
    "\n",
    "小さなデータセットの用意\n",
    "以前も利用した回帰のデータセットを用意します。\n",
    "\n",
    "\n",
    "House Prices: Advanced Regression Techniques  \n",
    "https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data\n",
    "\n",
    "\n",
    "\n",
    "この中のtrain.csvをダウンロードし、目的変数としてSalePrice、説明変数として、GrLivAreaとYearBuiltを使います。\n",
    "\n",
    "\n",
    "train.csvを学習用（train）8割、検証用（val）2割に分割してください。\n",
    "\n",
    "\n",
    "scikit-learn\n",
    "単一のモデルはスクラッチ実装ではなく、scikit-learnなどのライブラリの使用を推奨します。\n",
    "\n",
    "\n",
    "sklearn.linear_model.LinearRegression — scikit-learn 0.21.3 documentation  \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html  \n",
    "\n",
    "\n",
    "sklearn.svm.SVR — scikit-learn 0.21.3 documentation  \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html  \n",
    "\n",
    "\n",
    "sklearn.tree.DecisionTreeRegressor — scikit-learn 0.21.3 documentation  \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.0.0（ベースデータ準備)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "from sklearn .linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"sample_dataset/house-prices-advanced-regression-techniques/train.csv\")\n",
    "\n",
    "X = data[[\"GrLivArea\",\"YearBuilt\"]]\n",
    "Y = data[[\"SalePrice\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1460 entries, 0 to 1459\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype\n",
      "---  ------     --------------  -----\n",
      " 0   GrLivArea  1460 non-null   int64\n",
      " 1   YearBuilt  1460 non-null   int64\n",
      "dtypes: int64(2)\n",
      "memory usage: 22.9 KB\n"
     ]
    }
   ],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題1】ブレンディングのスクラッチ実装\n",
    "ブレンディング をスクラッチ実装し、単一モデルより精度があがる例を 最低3つ 示してください。精度があがるとは、検証用データに対する平均二乗誤差（MSE）が小さくなることを指します。\n",
    "\n",
    "\n",
    "ブレンディングとは\n",
    "ブレンディングとは、N個の多様なモデルを独立して学習させ、推定結果を重み付けした上で足し合わせる方法です。最も単純には平均をとります。多様なモデルとは、以下のような条件を変化させることで作り出すものです。\n",
    "\n",
    "\n",
    "手法（例：線形回帰、SVM、決定木、ニューラルネットワークなど）\n",
    "ハイパーパラメータ（例：SVMのカーネルの種類、重みの初期値など）\n",
    "入力データの前処理の仕方（例：標準化、対数変換、PCAなど）\n",
    "\n",
    "重要なのはそれぞれのモデルが大きく異なることです。\n",
    "\n",
    "\n",
    "回帰問題でのブレンディングは非常に単純であるため、scikit-learnには用意されていません。\n",
    "\n",
    "\n",
    "《補足》\n",
    "\n",
    "\n",
    "分類問題の場合は、多数決を行います。回帰問題に比べると複雑なため、scikit-learnにはVotingClassifierが用意されています。\n",
    "\n",
    "\n",
    "sklearn.ensemble.VotingClassifier — scikit-learn 0.21.3 documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1（予備知識）単体scoreの検証"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_mean(clf, X, Y, X_val=None, Y_val=None, num=100):\n",
    "    clf_score_list = []\n",
    "    clf_error_list = []\n",
    "    \n",
    "    for i in range(num):\n",
    "        x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)\n",
    "        \n",
    "        clf = clf.fit(x_train, y_train)\n",
    "        y_pred = clf.predict(x_test)\n",
    "        \n",
    "        clf_score_list.append(clf.score(x_test, y_test))\n",
    "        clf_error_list.append(mean_squared_error(y_test, y_pred))\n",
    "    \n",
    "    mean = np.mean(clf_score_list)\n",
    "    error = np.mean(clf_error_list)\n",
    "    \n",
    "    return mean, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "line mean: 0.6400433248652061 error 2234579950.0787716\n"
     ]
    }
   ],
   "source": [
    "# 線形回帰\n",
    "line = LinearRegression()\n",
    "line_data = score_mean(line, X, Y)\n",
    "print(\"line mean:\", line_data[0], \"error\", line_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM mean: 0.6340332587038313 error 2266042039.8533535\n"
     ]
    }
   ],
   "source": [
    "# SVM\n",
    "SVM = SVR(kernel='linear')\n",
    "SVM_data = score_mean(SVM, X, Y)\n",
    "print(\"SVM mean:\", SVM_data[0], \"error\", SVM_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree mean: 0.5475747468401644 error 2803662846.468687\n"
     ]
    }
   ],
   "source": [
    "# 決定木\n",
    "tree = DecisionTreeRegressor()\n",
    "tree_data = score_mean(tree, X, Y)\n",
    "print(\"tree mean:\", tree_data[0], \"error\", tree_data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1（解答)ブレンディング用関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class blending():\n",
    "    def __init__(self, clf1, clf2, ratio1, ratio2):\n",
    "        self.clf1 = clf1\n",
    "        self.clf2 = clf2\n",
    "        x = ratio1\n",
    "        y = ratio2\n",
    "        self.ratio1 = x/(x+y)\n",
    "        self.ratio2 = y/(x+y)\n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        self.clf1.fit(X, Y)\n",
    "        self.clf2.fit(X, Y)\n",
    "\n",
    "    def predict(self, X_var):\n",
    "        pred_1 = (self.clf1.predict(X_var) * self.ratio1).reshape(-1,1)\n",
    "        pred_2 = (self.clf2.predict(X_var) * self.ratio2).reshape(-1,1)\n",
    "        return pred_1+pred_2\n",
    "        \n",
    "    def score(self, X_var, Y_var):\n",
    "        score1 = self.clf1.score(X_var, Y_var) * self.ratio1\n",
    "        score2 = self.clf2.score(X_var, Y_var) * self.ratio2\n",
    "        return score1+score2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1（解答）blend_scoreの平均算出用関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blend_score_mean(clf1, clf2, ratio1, ratio2, X, Y, num=100):\n",
    "    \n",
    "    score_list = []\n",
    "    error_list = []\n",
    "    \n",
    "    for i in range(num):\n",
    "        x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)\n",
    "        \n",
    "        model = blending(clf1, clf2, ratio1, ratio2)\n",
    "        model.fit(x_train, y_train)\n",
    "        y_pred = model.predict(x_test)\n",
    "        \n",
    "        score_list.append(model.score(x_test, y_test))\n",
    "        error_list.append(mean_squared_error(y_test, y_pred))\n",
    "        \n",
    "    mean = np.mean(score_list)\n",
    "    error = np.mean(error_list)\n",
    "    \n",
    "    return mean, error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.1（解答）blend検証"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "line_SVM mean: 0.6419621450836465 error 2229186093.2301583\n"
     ]
    }
   ],
   "source": [
    "# 線形回帰 ＋ SVM\n",
    "ls1 = blend_score_mean(line, SVM, line_data[0], SVM_data[0], X, Y)\n",
    "print(\"line_SVM mean:\", ls1[0], \"error\", ls1[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM_tree mean: 0.5920884264300557 error 2016288598.308999\n"
     ]
    }
   ],
   "source": [
    "# SVM + tree\n",
    "st2 = blend_score_mean(SVM, tree, SVM_data[0], tree_data[0], X, Y)\n",
    "print(\"SVM_tree mean:\", st2[0], \"error\", st2[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree_line mean: 0.6069852466071737 error 1907580092.3443573\n"
     ]
    }
   ],
   "source": [
    "# tree + line\n",
    "tl3 = blend_score_mean(tree, line, tree_data[0], line_data[0], X, Y)\n",
    "print(\"tree_line mean:\", tl3[0], \"error\", tl3[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.バギング\n",
    "\n",
    "## 【問題2】バギングのスクラッチ実装\n",
    "バギング をスクラッチ実装し、単一モデルより精度があがる例を 最低1つ 示してください。\n",
    "\n",
    "\n",
    "バギングとは\n",
    "バギングは入力データの選び方を多様化する方法です。学習データから重複を許した上でランダムに抜き出すことで、N種類のサブセット（ ブートストラップサンプル ）を作り出します。それらによってモデルをN個学習し、推定結果の平均をとります。ブレンディングと異なり、それぞれの重み付けを変えることはありません。\n",
    "\n",
    "\n",
    "sklearn.model_selection.train_test_split — scikit-learn 0.21.3 documentation  \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "\n",
    "\n",
    "scikit-learnのtrain_test_splitを、shuffleパラメータをTrueにして使うことで、ランダムにデータを分割することができます。これによりブートストラップサンプルが手に入ります。\n",
    "\n",
    "\n",
    "推定結果の平均をとる部分はブースティングと同様の実装になります。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1（解答）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bagging():\n",
    "    def fit(self, X, Y): \n",
    "        x_1a, x_1b, y_1a, y_1b = train_test_split(X, Y, test_size=0.2, shuffle = True)\n",
    "        x_2a, x_2b, y_2a, y_2b = train_test_split(X, Y, test_size=0.2, shuffle = True)\n",
    "        x_3a, x_3b, y_3a, y_3b = train_test_split(X, Y, test_size=0.2, shuffle = True)\n",
    "        x_4a, x_4b, y_4a, y_4b = train_test_split(X, Y, test_size=0.2, shuffle = True)\n",
    "        \n",
    "        self.tree1 = DecisionTreeRegressor()\n",
    "        self.tree2 = DecisionTreeRegressor()\n",
    "        self.tree3 = DecisionTreeRegressor()\n",
    "        self.tree4 = DecisionTreeRegressor()\n",
    "        \n",
    "        self.tree1 = self.tree1.fit(x_1a, y_1a)\n",
    "        self.tree2 = self.tree2.fit(x_2a, y_2a)\n",
    "        self.tree3 = self.tree3.fit(x_3a, y_3a)\n",
    "        self.tree4 = self.tree4.fit(x_4a, y_4a)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        pred_1 = self.tree1.predict(X).reshape(-1,1)\n",
    "        pred_2 = self.tree2.predict(X).reshape(-1,1)\n",
    "        pred_3 = self.tree3.predict(X).reshape(-1,1)\n",
    "        pred_4 = self.tree4.predict(X).reshape(-1,1)\n",
    "\n",
    "        return (pred_1+pred_2+pred_3+pred_4)/4\n",
    "    \n",
    "    def score(self, X, Y):\n",
    "        score_1 = self.tree1.score(X, Y)\n",
    "        score_2 = self.tree2.score(X, Y)\n",
    "        score_3 = self.tree3.score(X, Y)\n",
    "        score_4 = self.tree4.score(X, Y)\n",
    "        score_list = [score_1, score_2, score_3, score_4]\n",
    "        \n",
    "        return np.mean(score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagging_score_mean(X, Y, num=100):\n",
    "    \n",
    "    score_list = []\n",
    "    error_list = []\n",
    "    \n",
    "    for i in range(num):\n",
    "        x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)\n",
    "        \n",
    "        bagg = bagging()\n",
    "        bagg.fit(x_train, y_train)\n",
    "        y_pred = bagg.predict(x_test)\n",
    "        \n",
    "        score_list.append(bagg.score(x_test, y_test))\n",
    "        error_list.append(mean_squared_error(y_test, y_pred))\n",
    "        \n",
    "    mean = np.mean(score_list)\n",
    "    error = np.mean(error_list)\n",
    "    \n",
    "    return mean, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bagging mean: 0.5254202346183174 error 2291607696.6303396\n"
     ]
    }
   ],
   "source": [
    "# 4tree_bagging\n",
    "bagg = bagging_score_mean(X, Y)\n",
    "print(\"bagging mean:\", bagg[0], \"error\", bagg[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.スタッキング\n",
    "\n",
    "## 【問題3】スタッキングのスクラッチ実装\n",
    "スタッキング をスクラッチ実装し、単一モデルより精度があがる例を 最低1つ 示してください。\n",
    "\n",
    "\n",
    "スタッキングとは\n",
    "スタッキングの手順は以下の通りです。最低限ステージ0とステージ1があればスタッキングは成立するため、それを実装してください。まずは \n",
    "K\n",
    "0\n",
    "=\n",
    "3\n",
    ",\n",
    "M\n",
    "0\n",
    "=\n",
    "2\n",
    " 程度にします。\n",
    "\n",
    "\n",
    "《学習時》\n",
    "\n",
    "\n",
    "（ステージ \n",
    "0\n",
    " ）\n",
    "\n",
    "\n",
    "学習データを \n",
    "K\n",
    "0\n",
    " 個に分割する。\n",
    "分割した内の \n",
    "(\n",
    "K\n",
    "0\n",
    "−\n",
    "1\n",
    ")\n",
    " 個をまとめて学習用データ、残り \n",
    "1\n",
    " 個を推定用データとする組み合わせが \n",
    "K\n",
    "0\n",
    " 個作れる。\n",
    "あるモデルのインスタンスを \n",
    "K\n",
    "0\n",
    " 個用意し、異なる学習用データを使い学習する。\n",
    "それぞれの学習済みモデルに対して、使っていない残り \n",
    "1\n",
    " 個の推定用データを入力し、推定値を得る。（これをブレンドデータと呼ぶ）\n",
    "さらに、異なるモデルのインスタンスも \n",
    "K\n",
    "0\n",
    " 個用意し、同様のことを行う。モデルが \n",
    "M\n",
    "0\n",
    " 個あれば、 \n",
    "M\n",
    "0\n",
    " 個のブレンドデータが得られる。\n",
    "\n",
    "（ステージ \n",
    "n\n",
    " ）\n",
    "\n",
    "\n",
    "ステージ \n",
    "n\n",
    "−\n",
    "1\n",
    " のブレンドデータを\n",
    "M\n",
    "n\n",
    "−\n",
    "1\n",
    " 次元の特徴量を持つ学習用データと考え、 \n",
    "K\n",
    "n\n",
    " 個に分割する。以下同様である。\n",
    "\n",
    "（ステージ \n",
    "N\n",
    " ）＊最後のステージ\n",
    "\n",
    "\n",
    "ステージ \n",
    "N\n",
    "−\n",
    "1\n",
    " の \n",
    "M\n",
    "N\n",
    "−\n",
    "1\n",
    " 個のブレンドデータを\n",
    "M\n",
    "N\n",
    "−\n",
    "1\n",
    " 次元の特徴量の入力として、1種類のモデルの学習を行う。これが最終的な推定を行うモデルとなる。\n",
    "\n",
    "《推定時》\n",
    "\n",
    "\n",
    "（ステージ \n",
    "0\n",
    " ）\n",
    "\n",
    "\n",
    "テストデータを \n",
    "K\n",
    "0\n",
    "×\n",
    "M\n",
    "0\n",
    " 個の学習済みモデルに入力し、\n",
    "K\n",
    "0\n",
    "×\n",
    "M\n",
    "0\n",
    " 個の推定値を得る。これを \n",
    "K\n",
    "0\n",
    " の軸で平均値を求め \n",
    "M\n",
    "0\n",
    " 次元の特徴量を持つデータを得る。（ブレンドテストと呼ぶ）\n",
    "\n",
    "（ステージ \n",
    "n\n",
    " ）\n",
    "\n",
    "\n",
    "ステージ \n",
    "n\n",
    "−\n",
    "1\n",
    " で得たブレンドテストを \n",
    "K\n",
    "n\n",
    "×\n",
    "M\n",
    "n\n",
    " 個の学習済みモデルに入力し、\n",
    "K\n",
    "n\n",
    "×\n",
    "M\n",
    "n\n",
    " 個の推定値を得る。これを \n",
    "K\n",
    "n\n",
    " の軸で平均値を求め \n",
    "M\n",
    "0\n",
    " 次元の特徴量を持つデータを得る。（ブレンドテストと呼ぶ）\n",
    "\n",
    "（ステージ \n",
    "N\n",
    " ）＊最後のステージ\n",
    "\n",
    "\n",
    "ステージ \n",
    "N\n",
    "−\n",
    "1\n",
    " で得たブレンドテストを学習済みモデルに入力し、推定値を得る。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1（解答） stage0の学習用関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "class staking():\n",
    "    def __init__(self, model, splits=3):\n",
    "        # 分類機を設定\n",
    "        self.model = model\n",
    "        self.splits = splits\n",
    "        # K-Folds(クロスバリデーション)の設定\n",
    "        self.kf = KFold(n_splits=self.splits, random_state=None, shuffle=False)\n",
    "        \n",
    "    def fit(self, X, Y):\n",
    "        self.model.fit(X, Y)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "    \n",
    "    # stage0の学習→分類器毎のoof（out of fold)を返す----------------------\n",
    "    \n",
    "    def get_oof(self, x_train, y_train, x_test):\n",
    "        # oofの受け皿を作成\n",
    "        oof_train = np.zeros((x_train.shape[0],))\n",
    "        oof_test = np.zeros((x_test.shape[0],))\n",
    "        oof_test_skf = np.empty((self.splits, x_test.shape[0]))\n",
    "        \n",
    "        # K-Foldsでデータを分ける\n",
    "        for i, (train_index, test_index) in enumerate(self.kf.split(x_train, y_train)):\n",
    "            x_tr = x_train[train_index]\n",
    "            y_tr = y_train[train_index]\n",
    "            x_te = x_train[test_index]\n",
    "#             print(i, \"x_tr\", x_tr, \"y_tr\", y_tr, \"x_te\", x_te)\n",
    "            \n",
    "            self.model.fit(x_tr, y_tr)\n",
    "            \n",
    "            # 学習結果を元にtrainデータ(test)の推定\n",
    "            oof_train[test_index] = self.model.predict(x_te).ravel()\n",
    "            # 学習結果を元にtestデータの推定\n",
    "            oof_test_skf[i, :] = self.model.predict(x_test).ravel()\n",
    "            \n",
    "        oof_test[:] = oof_test_skf.mean(axis=0)\n",
    "        \n",
    "        return (oof_train.reshape(-1, 1), oof_test.reshape(-1,1))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2（解答） stage0の実行"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# データ型を整える\n",
    "x_train = x_train.values\n",
    "y_train = y_train.values\n",
    "x_test = x_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用する分類機の呼び出し\n",
    "dt = staking(model=DecisionTreeRegressor(), splits=4)\n",
    "sv = staking(model=SVR(kernel='linear'), splits=4)\n",
    "lr = staking(model=LinearRegression(), splits=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stage0の実行\n",
    "lr_oof_train, lr_oof_test = lr.get_oof(x_train, y_train, x_test)\n",
    "dt_oof_train, dt_oof_test = dt.get_oof(x_train, y_train, x_test)\n",
    "sv_oof_train, sv_oof_test = sv.get_oof(x_train, y_train, x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1（解答） stage1の実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_oof (1168, 3)\n",
      "x_test_oof  (292, 3)\n"
     ]
    }
   ],
   "source": [
    "# stage1用データの用意\n",
    "x_train_oof = np.concatenate((lr_oof_train, dt_oof_train, sv_oof_train),axis=1)\n",
    "x_test_oof = np.concatenate((lr_oof_test, dt_oof_test, sv_oof_test),axis=1)\n",
    "print(\"x_train_oof\",x_train_oof.shape)\n",
    "print(\"x_test_oof \",x_test_oof.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score 0.5103416273740732 error 3164153212.7260275\n"
     ]
    }
   ],
   "source": [
    "# stage1学習の実行\n",
    "dt2 = DecisionTreeRegressor()\n",
    "dt2.fit(x_train_oof, y_train)\n",
    "dt2_pred = dt2.predict(x_test_oof)\n",
    "dt2_score = dt2.score(x_test_oof, y_test)\n",
    "error = mean_squared_error(y_test, dt2_pred)\n",
    "print(\"score\",dt2_score, \"error\", error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1（解答）平均値"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dt2 score 0.4579333452079763 error 3535936527.7864633\n"
     ]
    }
   ],
   "source": [
    "dt2_score_list = []\n",
    "dt2_error_list = []\n",
    "\n",
    "for i in range(20):\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)\n",
    "    \n",
    "    dt = staking(model=DecisionTreeRegressor(), splits=4)\n",
    "    sv = staking(model=SVR(kernel='linear'), splits=4)\n",
    "    lr = staking(model=LinearRegression(), splits=4)\n",
    "    \n",
    "    lr_oof_train, lr_oof_test = lr.get_oof(x_train, y_train, x_test)\n",
    "    dt_oof_train, dt_oof_test = dt.get_oof(x_train, y_train, x_test)\n",
    "    sv_oof_train, sv_oof_test = sv.get_oof(x_train, y_train, x_test)\n",
    "    \n",
    "    x_train_oof = np.concatenate((lr_oof_train, dt_oof_train, sv_oof_train),axis=1)\n",
    "    x_test_oof = np.concatenate((lr_oof_test, dt_oof_test, sv_oof_test),axis=1)\n",
    "    \n",
    "    dt2 = DecisionTreeRegressor()\n",
    "    dt2.fit(x_train_oof, y_train)\n",
    "    dt2_pred = dt2.predict(x_test_oof)\n",
    "    dt2_score_list.append(dt2.score(x_test_oof, y_test))\n",
    "    dt2_error_list.append(mean_squared_error(y_test, dt2_pred))\n",
    "\n",
    "dt2_mean = np.mean(dt2_score_list)\n",
    "dt2_error = np.mean(dt2_error_list)\n",
    "dt2 = [dt2_mean, dt2_error]\n",
    "\n",
    "print(\"dt2 score\",dt2[0], \"error\", dt2[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM2 score 0.6256222525056039 error 2435809290.5139647\n"
     ]
    }
   ],
   "source": [
    "SVM2_score_list = []\n",
    "SVM2_error_list = []\n",
    "\n",
    "for i in range(1):\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)\n",
    "    \n",
    "    dt = staking(model=DecisionTreeRegressor(), splits=4)\n",
    "    sv = staking(model=SVR(kernel='linear'), splits=4)\n",
    "    lr = staking(model=LinearRegression(), splits=4)\n",
    "    \n",
    "    lr_oof_train, lr_oof_test = lr.get_oof(x_train, y_train, x_test)\n",
    "    dt_oof_train, dt_oof_test = dt.get_oof(x_train, y_train, x_test)\n",
    "    sv_oof_train, sv_oof_test = sv.get_oof(x_train, y_train, x_test)\n",
    "    \n",
    "    x_train_oof = np.concatenate((lr_oof_train, dt_oof_train, sv_oof_train),axis=1)\n",
    "    x_test_oof = np.concatenate((lr_oof_test, dt_oof_test, sv_oof_test),axis=1)\n",
    "    \n",
    "    SVM2 = SVR(kernel='linear')\n",
    "    SVM2.fit(x_train_oof, y_train)\n",
    "    SVM2_pred = SVM2.predict(x_test_oof)\n",
    "    SVM2_score_list.append(SVM2.score(x_test_oof, y_test))\n",
    "    SVM2_error_list.append(mean_squared_error(y_test, SVM2_pred))\n",
    "\n",
    "SVM2_mean = np.mean(SVM2_score_list)\n",
    "SVM2_error = np.mean(SVM2_error_list)\n",
    "SVM2 = [SVM2_mean, SVM2_error]\n",
    "\n",
    "print(\"SVM2 score\",SVM2[0], \"error\", SVM2[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1（予備知識）結果確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>MSE(平均乗誤差)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>決定木＋線形回帰</th>\n",
       "      <td>0.606985</td>\n",
       "      <td>1.907580e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM＋決定木</th>\n",
       "      <td>0.592088</td>\n",
       "      <td>2.016289e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>線形回帰+SVM</th>\n",
       "      <td>0.641962</td>\n",
       "      <td>2.229186e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>線形回帰</th>\n",
       "      <td>0.640043</td>\n",
       "      <td>2.234580e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>0.634033</td>\n",
       "      <td>2.266042e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bagging</th>\n",
       "      <td>0.525420</td>\n",
       "      <td>2.291608e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Staking_SVM</th>\n",
       "      <td>0.625622</td>\n",
       "      <td>2.435809e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>決定木</th>\n",
       "      <td>0.547575</td>\n",
       "      <td>2.803663e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Staking_決定木</th>\n",
       "      <td>0.457933</td>\n",
       "      <td>3.535937e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                score    MSE(平均乗誤差)\n",
       "決定木＋線形回帰     0.606985  1.907580e+09\n",
       "SVM＋決定木      0.592088  2.016289e+09\n",
       "線形回帰+SVM     0.641962  2.229186e+09\n",
       "線形回帰         0.640043  2.234580e+09\n",
       "SVM          0.634033  2.266042e+09\n",
       "Bagging      0.525420  2.291608e+09\n",
       "Staking_SVM  0.625622  2.435809e+09\n",
       "決定木          0.547575  2.803663e+09\n",
       "Staking_決定木  0.457933  3.535937e+09"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.array([line_data, SVM_data, tree_data, ls1, st2, tl3, bagg, dt2, SVM2])\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.columns = [\"score\", \"MSE(平均乗誤差)\"]\n",
    "df.index = [\"線形回帰\", \"SVM\", \"決定木\", \"線形回帰+SVM\", \"SVM＋決定木\", \"決定木＋線形回帰\", \"Bagging\",\"Staking_決定木\",\"Staking_SVM\"]\n",
    "df.sort_values(\"MSE(平均乗誤差)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 検証保管"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "x_train1= x_train[:10]\n",
    "y_train1= y_train[:10]\n",
    "x_test1= x_test[:5]\n",
    "x_test1= x_test[:5]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "y_train1.ravel().shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def get_oof(clf, x_train, y_train, x_test):\n",
    "    kf = KFold(n_splits=4, random_state=None, shuffle=False)\n",
    "    oof_train = np.zeros((x_train.shape[0],))\n",
    "    oof_test = np.zeros((x_test.shape[0],))\n",
    "    oof_test_skf = np.empty((4, x_test.shape[0]))\n",
    "\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(x_train, y_train)):\n",
    "        x_tr = x_train[train_index]\n",
    "        y_tr = y_train[train_index]\n",
    "        x_te = x_train[test_index]\n",
    "        \n",
    "        # print(\"x_tr\", x_tr, \"y_tr\", y_tr, \"x_te\", x_te)\n",
    "\n",
    "        clf.fit(x_tr, y_tr)\n",
    "\n",
    "        oof_train[test_index] = clf.predict(x_te)\n",
    "        print(\"oof_train[test_index]\", oof_train[test_index])\n",
    "\n",
    "        oof_test_skf[i, :] = clf.predict(x_test)\n",
    "        print(\"oof_test_skf[i,:]\", oof_test_skf[i, :])\n",
    "\n",
    "    oof_test[:] = oof_test_skf.mean(axis=0)\n",
    "    print(\"oof_test[:]\", oof_test[:])\n",
    "\n",
    "    print(oof_train.reshape(-1, 1), oof_test.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "lr = LinearRegression()\n",
    "lr_oof_train, lr_oof_test = get_oof(lr, x_train1, y_train1, x_test1)\n",
    "lr_oof_train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
